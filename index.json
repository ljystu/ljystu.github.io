[{"categories":["Note"],"content":"接口幂等性的保证方法： 幂等性指多次调用同一接口获得的结果相同，此处考虑如何在高并发场景下限制其重复操作。 分布式锁，获取锁进行业务操作，可以使用redisson等框架。在业务执行完毕后释放锁 数据库乐观锁，根据version限制其多次更新 系统生成唯一token，在分布式系统中对于一个id生成唯一token并直接写入redis，第一次操作完成后直接删除redis中的token，后续操作发现redis中没有对应的token可以直接返回错误信息。 ","date":"2022-12-26","objectID":"/%E4%B8%80%E4%BA%9B%E9%A1%B9%E7%9B%AE%E8%BF%87%E7%A8%8B%E4%B8%AD%E9%97%AE%E9%A2%98%E7%9A%84%E6%80%BB%E7%BB%93/:1:0","series":null,"tags":null,"title":"Notes","uri":"/%E4%B8%80%E4%BA%9B%E9%A1%B9%E7%9B%AE%E8%BF%87%E7%A8%8B%E4%B8%AD%E9%97%AE%E9%A2%98%E7%9A%84%E6%80%BB%E7%BB%93/#接口幂等性的保证方法"},{"categories":["Note"],"content":"缓存穿透 解决:将null结果进行缓存，并加入短暂过期时间 ","date":"2022-12-26","objectID":"/%E4%B8%80%E4%BA%9B%E9%A1%B9%E7%9B%AE%E8%BF%87%E7%A8%8B%E4%B8%AD%E9%97%AE%E9%A2%98%E7%9A%84%E6%80%BB%E7%BB%93/:2:0","series":null,"tags":null,"title":"Notes","uri":"/%E4%B8%80%E4%BA%9B%E9%A1%B9%E7%9B%AE%E8%BF%87%E7%A8%8B%E4%B8%AD%E9%97%AE%E9%A2%98%E7%9A%84%E6%80%BB%E7%BB%93/#缓存穿透"},{"categories":["Note"],"content":"缓存雪崩 缓存雪崩是指缓存同一时间大面积的失效，所以，后面的请求都会落到数据库上，造成数据库短时间内承受大量请求而崩掉。 解决： 1.缓存数据的过期时间设置随机，防止同一时间大量数据过期现象发生 2.一般并发量不是特别多的时候，使用最多的解决方案是加锁排队 ","date":"2022-12-26","objectID":"/%E4%B8%80%E4%BA%9B%E9%A1%B9%E7%9B%AE%E8%BF%87%E7%A8%8B%E4%B8%AD%E9%97%AE%E9%A2%98%E7%9A%84%E6%80%BB%E7%BB%93/:3:0","series":null,"tags":null,"title":"Notes","uri":"/%E4%B8%80%E4%BA%9B%E9%A1%B9%E7%9B%AE%E8%BF%87%E7%A8%8B%E4%B8%AD%E9%97%AE%E9%A2%98%E7%9A%84%E6%80%BB%E7%BB%93/#缓存雪崩"},{"categories":["Note"],"content":"缓存击穿 单体应用，如果本地不加同步锁的时候，在高并发的情况下，会发生线程安全问题。 单体应用，如果加上本地锁的时候，在高并发情况下，可以解决安全问题，但是效率会变低，所以就有分布式锁 ","date":"2022-12-26","objectID":"/%E4%B8%80%E4%BA%9B%E9%A1%B9%E7%9B%AE%E8%BF%87%E7%A8%8B%E4%B8%AD%E9%97%AE%E9%A2%98%E7%9A%84%E6%80%BB%E7%BB%93/:4:0","series":null,"tags":null,"title":"Notes","uri":"/%E4%B8%80%E4%BA%9B%E9%A1%B9%E7%9B%AE%E8%BF%87%E7%A8%8B%E4%B8%AD%E9%97%AE%E9%A2%98%E7%9A%84%E6%80%BB%E7%BB%93/#缓存击穿"},{"categories":["Note"],"content":"读写锁 在实际的业务场景中，其实会有很多读多写少的场景，那么对于这种场景来说，使用独占锁来加锁，在高并发场景下会导致大量的线程加锁失败，阻塞，对系统的吞吐量有一定的影响，为了适配这种读多写少的场景，Redisson也实现了读写锁的功能。 读写锁的特点： 1）读与读是共享的，不互斥 2）读与写互斥 3）写与写互斥 分布式可重入读写锁允许同时有多个读锁和一个写锁处于加锁状态。 ","date":"2022-12-26","objectID":"/%E4%B8%80%E4%BA%9B%E9%A1%B9%E7%9B%AE%E8%BF%87%E7%A8%8B%E4%B8%AD%E9%97%AE%E9%A2%98%E7%9A%84%E6%80%BB%E7%BB%93/:5:0","series":null,"tags":null,"title":"Notes","uri":"/%E4%B8%80%E4%BA%9B%E9%A1%B9%E7%9B%AE%E8%BF%87%E7%A8%8B%E4%B8%AD%E9%97%AE%E9%A2%98%E7%9A%84%E6%80%BB%E7%BB%93/#读写锁"},{"categories":["Note"],"content":"缓存一致性问题的解决方案 缓存一致性一般使用双写方案，更新数据库之后删除缓存。 先操作数据库，再删除缓存（线程不安全的可能性较低）使用这种方案的同时用超时剔除作为兜底 分布式缓存中使用分布式锁对单个缓存进行操作。 总结： 3、更新数据库 + 更新缓存方案，在「并发」场景下无法保证缓存和数据一致性，解决方案是加「分布锁」，但这种方案存在「缓存资源浪费」和「机器性能浪费」的情况 4、采用「先删除缓存，再更新数据库」方案，在「并发」场景下依旧有不一致问题，解决方案是「延迟双删」，但这个延迟时间很难评估 5、采用「先更新数据库，再删除缓存」方案，为了保证两步都成功执行，需配合「消息队列」或「订阅变更日志」的方案来做，本质是通过「重试」的方式保证数据最终一致 6、采用「先更新数据库，再删除缓存」方案，「读写分离 + 主从库延迟」也会导致缓存和数据库不一致，缓解此问题的方案是「延迟双删」，凭借经验发送「延迟消息」到队列中，延迟删除缓存，同时也要控制主从库延迟，尽可能降低不一致发生的概率 ","date":"2022-12-26","objectID":"/%E4%B8%80%E4%BA%9B%E9%A1%B9%E7%9B%AE%E8%BF%87%E7%A8%8B%E4%B8%AD%E9%97%AE%E9%A2%98%E7%9A%84%E6%80%BB%E7%BB%93/:6:0","series":null,"tags":null,"title":"Notes","uri":"/%E4%B8%80%E4%BA%9B%E9%A1%B9%E7%9B%AE%E8%BF%87%E7%A8%8B%E4%B8%AD%E9%97%AE%E9%A2%98%E7%9A%84%E6%80%BB%E7%BB%93/#缓存一致性问题的解决方案"},{"categories":["Note"],"content":"Elasticsearch ","date":"2022-12-26","objectID":"/%E4%B8%80%E4%BA%9B%E9%A1%B9%E7%9B%AE%E8%BF%87%E7%A8%8B%E4%B8%AD%E9%97%AE%E9%A2%98%E7%9A%84%E6%80%BB%E7%BB%93/:7:0","series":null,"tags":null,"title":"Notes","uri":"/%E4%B8%80%E4%BA%9B%E9%A1%B9%E7%9B%AE%E8%BF%87%E7%A8%8B%E4%B8%AD%E9%97%AE%E9%A2%98%E7%9A%84%E6%80%BB%E7%BB%93/#elasticsearch"},{"categories":["Note"],"content":"数据类型问题 使用过程中index数据类型注意要和项目中entity一致 ","date":"2022-12-26","objectID":"/%E4%B8%80%E4%BA%9B%E9%A1%B9%E7%9B%AE%E8%BF%87%E7%A8%8B%E4%B8%AD%E9%97%AE%E9%A2%98%E7%9A%84%E6%80%BB%E7%BB%93/:7:1","series":null,"tags":null,"title":"Notes","uri":"/%E4%B8%80%E4%BA%9B%E9%A1%B9%E7%9B%AE%E8%BF%87%E7%A8%8B%E4%B8%AD%E9%97%AE%E9%A2%98%E7%9A%84%E6%80%BB%E7%BB%93/#数据类型问题"},{"categories":["Note"],"content":"连接 连接时需要初始化client并且在使用完毕后关闭client ","date":"2022-12-26","objectID":"/%E4%B8%80%E4%BA%9B%E9%A1%B9%E7%9B%AE%E8%BF%87%E7%A8%8B%E4%B8%AD%E9%97%AE%E9%A2%98%E7%9A%84%E6%80%BB%E7%BB%93/:7:2","series":null,"tags":null,"title":"Notes","uri":"/%E4%B8%80%E4%BA%9B%E9%A1%B9%E7%9B%AE%E8%BF%87%E7%A8%8B%E4%B8%AD%E9%97%AE%E9%A2%98%E7%9A%84%E6%80%BB%E7%BB%93/#连接"},{"categories":["Note"],"content":"Java8 ","date":"2022-12-26","objectID":"/%E4%B8%80%E4%BA%9B%E9%A1%B9%E7%9B%AE%E8%BF%87%E7%A8%8B%E4%B8%AD%E9%97%AE%E9%A2%98%E7%9A%84%E6%80%BB%E7%BB%93/:8:0","series":null,"tags":null,"title":"Notes","uri":"/%E4%B8%80%E4%BA%9B%E9%A1%B9%E7%9B%AE%E8%BF%87%E7%A8%8B%E4%B8%AD%E9%97%AE%E9%A2%98%E7%9A%84%E6%80%BB%E7%BB%93/#java8"},{"categories":["Note"],"content":":: 双冒号运算符一般用在lambda类运算的情况下，多数时候创建一个匿名类只是为了调用其内部的一个方法，在此时可以直接使用::进行调用 使用lambda表达式会创建匿名方法， 但有时候需要使用一个lambda表达式只调用一个已经存在的方法（不做其它）， 所以这才有了方法引用！ 以下是Java 8中方法引用的一些语法： 静态方法引用（static method）语法：classname::methodname 例如：Person::getAge 对象的实例方法引用语法：instancename::methodname 例如：System.out::println 对象的超类方法引用语法： super::methodname 类构造器引用语法： classname::new 例如：ArrayList::new 数组构造器引用语法： typename[]::new 例如： String[]:new ","date":"2022-12-26","objectID":"/%E4%B8%80%E4%BA%9B%E9%A1%B9%E7%9B%AE%E8%BF%87%E7%A8%8B%E4%B8%AD%E9%97%AE%E9%A2%98%E7%9A%84%E6%80%BB%E7%BB%93/:8:1","series":null,"tags":null,"title":"Notes","uri":"/%E4%B8%80%E4%BA%9B%E9%A1%B9%E7%9B%AE%E8%BF%87%E7%A8%8B%E4%B8%AD%E9%97%AE%E9%A2%98%E7%9A%84%E6%80%BB%E7%BB%93/#heading"},{"categories":["Note"],"content":"Stream 6.1、filter filter方法用于通过设置的条件过滤出元素。以下代码片段使用filter方法过滤出空字符串。 复制 List\u003cString\u003e strings = Arrays.asList(\"abc\", \"\", \"bc\", \"efg\", \"abcd\",\"\", \"jkl\"); List\u003cString\u003e filtered = strings.stream().filter(string -\u003e !string.isEmpty()).collect(Collectors.toList());  6.2、limit limit方法用于获取指定数量的流。以下代码片段使用limit方法打印出 10 条数据： 复制 Random random = new Random(); random.ints().limit(10).forEach(System.out::println);  6.3、sorted sorted方法用于对流进行排序。以下代码片段使用sorted方法对集合中的数字进行排序： 复制 List\u003cInteger\u003e numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5); numbers.stream().sorted().forEach(System.out::println);  6.4、map map方法用于映射每个元素到对应的结果，以下代码片段使用map输出了元素对应的平方数： 复制 List\u003cInteger\u003e numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5); // 获取对应的平方数 List\u003cInteger\u003e squaresList = numbers.stream().map( i -\u003e i*i) .distinct().collect(Collectors.toList());  6.5、forEach forEach方法用于迭代流中的每个数据。以下代码片段使用forEach输出集合中的数字： 复制 List\u003cInteger\u003e numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5); numbers.stream().forEach(System.out::println);  6.6、Collectors Collectors类实现了很多归约操作，例如将流转换成集合和聚合元素。Collectors可用于返回列表或字符串： 复制 List\u003cString\u003estrings = Arrays.asList(\"abc\", \"\", \"bc\", \"efg\", \"abcd\",\"\", \"jkl\"); List\u003cString\u003e filtered = strings.stream().filter(string -\u003e !string.isEmpty()).collect(Collectors.toList()); System.out.println(\"筛选列表: \" + filtered); String mergedString = strings.stream().filter(string -\u003e !string.isEmpty()).collect(Collectors.joining(\", \")); System.out.println(\"合并字符串: \" + mergedString);  6.7、统计 一些产生统计结果的收集器也非常有用。它们主要用于int、double、long等基本类型上，它们可以用来产生类似如下的统计结果： 复制 List\u003cInteger\u003e numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5); IntSummaryStatistics stats = numbers.stream().mapToInt((x) -\u003e x).summaryStatistics(); System.out.println(\"列表中最大的数 : \" + stats.getMax()); System.out.println(\"列表中最小的数 : \" + stats.getMin()); System.out.println(\"所有数之和 : \" + stats.getSum()); System.out.println(\"平均数 : \" + stats.getAverage());  6.8、并行(parallel)程序 parallelStream是流并行处理程序的代替方法。以下实例我们使用 parallelStream来输出空字符串的数量： 复制List strings = Arrays.asList(“abc”, “\", “bc”, “efg”, “abcd”,”\", “jkl”); // 获取空字符串的数量 long count = strings.parallelStream().filter(string -\u003e string.isEmpty()).count(); Optional 类 Java应用中最常见的bug就是空值异常。在 Java 8 之前，Google Guava 引入了 Optionals 类来解决 NullPointerException，从而避免源码被各种 null 检查污染，以便开发者写出更加整洁的代码。Java 8 也将 Optional 加入了官方库。 ","date":"2022-12-26","objectID":"/%E4%B8%80%E4%BA%9B%E9%A1%B9%E7%9B%AE%E8%BF%87%E7%A8%8B%E4%B8%AD%E9%97%AE%E9%A2%98%E7%9A%84%E6%80%BB%E7%BB%93/:8:2","series":null,"tags":null,"title":"Notes","uri":"/%E4%B8%80%E4%BA%9B%E9%A1%B9%E7%9B%AE%E8%BF%87%E7%A8%8B%E4%B8%AD%E9%97%AE%E9%A2%98%E7%9A%84%E6%80%BB%E7%BB%93/#stream"},{"categories":["Note"],"content":"泛型 关于 和 https://blog.csdn.net/csdn_aiyang/article/details/107154081 总结：一般来讲用于泛型类的定义，因为定义一般需要类型较为确定，泛型方法的使用可以使用 因为类型不一定确定 ","date":"2022-12-26","objectID":"/%E4%B8%80%E4%BA%9B%E9%A1%B9%E7%9B%AE%E8%BF%87%E7%A8%8B%E4%B8%AD%E9%97%AE%E9%A2%98%E7%9A%84%E6%80%BB%E7%BB%93/:9:0","series":null,"tags":null,"title":"Notes","uri":"/%E4%B8%80%E4%BA%9B%E9%A1%B9%E7%9B%AE%E8%BF%87%E7%A8%8B%E4%B8%AD%E9%97%AE%E9%A2%98%E7%9A%84%E6%80%BB%E7%BB%93/#泛型"},{"categories":["Note"],"content":"序列化 ","date":"2022-12-26","objectID":"/%E4%B8%80%E4%BA%9B%E9%A1%B9%E7%9B%AE%E8%BF%87%E7%A8%8B%E4%B8%AD%E9%97%AE%E9%A2%98%E7%9A%84%E6%80%BB%E7%BB%93/:11:0","series":null,"tags":null,"title":"Notes","uri":"/%E4%B8%80%E4%BA%9B%E9%A1%B9%E7%9B%AE%E8%BF%87%E7%A8%8B%E4%B8%AD%E9%97%AE%E9%A2%98%E7%9A%84%E6%80%BB%E7%BB%93/#序列化"},{"categories":["Tech"],"content":"DB操作 主要来自菜鸟教程 A inner join B 取交集。 A left join B 取 A 全部，B 没有对应的值为 null。 A right join B 取 B 全部 A 没有对应的值为 null。 A full outer join B 取并集，彼此没有对应的值为 null。 数据库范式是设计数据库时，需要遵循的一些规范。各种范式是条件递增的联系，越高的范式数据库冗余越小。常用的数据库三大范式为： 第一范式（1NF）：每个列都不可以再拆分，强调的是列的原子性。第一范式要求数据库中的表都是二维表。 例子：一列中有两种信息，应当拆分 第二范式（2NF）：在第一范式的基础上，一个表必须有一个主键，非主键列 完全依赖 于主键，而不能是依赖于主键的一部分。 例子：订单号 产品数量 产品金额 产品折扣 … 订单金额 订单金额只和订单号相关，所以应该拆分到另一张表中 第三范式（3NF）：在第二范式的基础上，非主键列只依赖（直接依赖）于主键，不依赖于其他非主键。 类似于第二范式，3NF要求非主键直接依赖于主键 例如：学生学号 姓名 年龄 班主任年龄 性别 班主任性别和学生学号间接相关，应当拆分 注：况实际分析设计，兼顾性能和需求方面的考虑，无需一昧追求满足三范式。 ","date":"2022-10-27","objectID":"/db/:0:1","series":null,"tags":null,"title":"Mysql","uri":"/db/#db操作"},{"categories":["Tech"],"content":"连接池 普通DB连接需要在连接前建立连接 确认连接 执行SQL 关闭连接 TCP握手等开销过大 但是实现简单。 连接池可以提供一定数量的数据库连接，第一次访问的时候，需要建立连接。 但是之后的访问，均会复用之前创建的连接，直接执行SQL语句。 优点： 较少了网络开销 系统的性能会有一个实质的提升 没了麻烦的TIME_WAIT状态 连接池主要参数 使用连接池时，要配置一下参数 最小连接数：是连接池一直保持的数据库连接,所以如果应用程序对数据库连接的使用量不大,将会有大量的数据库连接资源被浪费. 最大连接数：是连接池能申请的最大连接数,如果数据库连接请求超过次数,后面的数据库连接请求将被加入到等待队列中,这会影响以后的数据库操作 最大空闲时间 获取连接超时时间 超时重试连接次数 连接池使用过程中的主要问题： 事务问题：必须保证两个线程中的事务原子性，为此我们可以给每一个线程（事务）单独分配一个连接对象，开销大但是安全 连接池的配置与维护：最小连接数设置过大则启动配置慢但是使用过程中很快，反之则使用过程中较慢。根据需求进行具体配置。 最流行连接池Hikari Druid 数据库连接泄露： 如果在某次使用或者某段程序中没有正确地关闭 Connection、Statement 和 ResultSet 资源，那么每次执行都会留下一些没有关闭的连接，这些连接失去了引用而不能得到重新使用，因此就造成了数据库连接的泄漏。 需要进行连接的配置服务降级等操作 ","date":"2022-10-27","objectID":"/db/:0:2","series":null,"tags":null,"title":"Mysql","uri":"/db/#连接池"},{"categories":["Tech"],"content":"连接池 普通DB连接需要在连接前建立连接 确认连接 执行SQL 关闭连接 TCP握手等开销过大 但是实现简单。 连接池可以提供一定数量的数据库连接，第一次访问的时候，需要建立连接。 但是之后的访问，均会复用之前创建的连接，直接执行SQL语句。 优点： 较少了网络开销 系统的性能会有一个实质的提升 没了麻烦的TIME_WAIT状态 连接池主要参数 使用连接池时，要配置一下参数 最小连接数：是连接池一直保持的数据库连接,所以如果应用程序对数据库连接的使用量不大,将会有大量的数据库连接资源被浪费. 最大连接数：是连接池能申请的最大连接数,如果数据库连接请求超过次数,后面的数据库连接请求将被加入到等待队列中,这会影响以后的数据库操作 最大空闲时间 获取连接超时时间 超时重试连接次数 连接池使用过程中的主要问题： 事务问题：必须保证两个线程中的事务原子性，为此我们可以给每一个线程（事务）单独分配一个连接对象，开销大但是安全 连接池的配置与维护：最小连接数设置过大则启动配置慢但是使用过程中很快，反之则使用过程中较慢。根据需求进行具体配置。 最流行连接池Hikari Druid 数据库连接泄露： 如果在某次使用或者某段程序中没有正确地关闭 Connection、Statement 和 ResultSet 资源，那么每次执行都会留下一些没有关闭的连接，这些连接失去了引用而不能得到重新使用，因此就造成了数据库连接的泄漏。 需要进行连接的配置服务降级等操作 ","date":"2022-10-27","objectID":"/db/:0:2","series":null,"tags":null,"title":"Mysql","uri":"/db/#优点"},{"categories":["Tech"],"content":"连接池 普通DB连接需要在连接前建立连接 确认连接 执行SQL 关闭连接 TCP握手等开销过大 但是实现简单。 连接池可以提供一定数量的数据库连接，第一次访问的时候，需要建立连接。 但是之后的访问，均会复用之前创建的连接，直接执行SQL语句。 优点： 较少了网络开销 系统的性能会有一个实质的提升 没了麻烦的TIME_WAIT状态 连接池主要参数 使用连接池时，要配置一下参数 最小连接数：是连接池一直保持的数据库连接,所以如果应用程序对数据库连接的使用量不大,将会有大量的数据库连接资源被浪费. 最大连接数：是连接池能申请的最大连接数,如果数据库连接请求超过次数,后面的数据库连接请求将被加入到等待队列中,这会影响以后的数据库操作 最大空闲时间 获取连接超时时间 超时重试连接次数 连接池使用过程中的主要问题： 事务问题：必须保证两个线程中的事务原子性，为此我们可以给每一个线程（事务）单独分配一个连接对象，开销大但是安全 连接池的配置与维护：最小连接数设置过大则启动配置慢但是使用过程中很快，反之则使用过程中较慢。根据需求进行具体配置。 最流行连接池Hikari Druid 数据库连接泄露： 如果在某次使用或者某段程序中没有正确地关闭 Connection、Statement 和 ResultSet 资源，那么每次执行都会留下一些没有关闭的连接，这些连接失去了引用而不能得到重新使用，因此就造成了数据库连接的泄漏。 需要进行连接的配置服务降级等操作 ","date":"2022-10-27","objectID":"/db/:0:2","series":null,"tags":null,"title":"Mysql","uri":"/db/#连接池主要参数"},{"categories":["Tech"],"content":"连接池 普通DB连接需要在连接前建立连接 确认连接 执行SQL 关闭连接 TCP握手等开销过大 但是实现简单。 连接池可以提供一定数量的数据库连接，第一次访问的时候，需要建立连接。 但是之后的访问，均会复用之前创建的连接，直接执行SQL语句。 优点： 较少了网络开销 系统的性能会有一个实质的提升 没了麻烦的TIME_WAIT状态 连接池主要参数 使用连接池时，要配置一下参数 最小连接数：是连接池一直保持的数据库连接,所以如果应用程序对数据库连接的使用量不大,将会有大量的数据库连接资源被浪费. 最大连接数：是连接池能申请的最大连接数,如果数据库连接请求超过次数,后面的数据库连接请求将被加入到等待队列中,这会影响以后的数据库操作 最大空闲时间 获取连接超时时间 超时重试连接次数 连接池使用过程中的主要问题： 事务问题：必须保证两个线程中的事务原子性，为此我们可以给每一个线程（事务）单独分配一个连接对象，开销大但是安全 连接池的配置与维护：最小连接数设置过大则启动配置慢但是使用过程中很快，反之则使用过程中较慢。根据需求进行具体配置。 最流行连接池Hikari Druid 数据库连接泄露： 如果在某次使用或者某段程序中没有正确地关闭 Connection、Statement 和 ResultSet 资源，那么每次执行都会留下一些没有关闭的连接，这些连接失去了引用而不能得到重新使用，因此就造成了数据库连接的泄漏。 需要进行连接的配置服务降级等操作 ","date":"2022-10-27","objectID":"/db/:0:2","series":null,"tags":null,"title":"Mysql","uri":"/db/#连接池使用过程中的主要问题"},{"categories":["Tech"],"content":"连接池 普通DB连接需要在连接前建立连接 确认连接 执行SQL 关闭连接 TCP握手等开销过大 但是实现简单。 连接池可以提供一定数量的数据库连接，第一次访问的时候，需要建立连接。 但是之后的访问，均会复用之前创建的连接，直接执行SQL语句。 优点： 较少了网络开销 系统的性能会有一个实质的提升 没了麻烦的TIME_WAIT状态 连接池主要参数 使用连接池时，要配置一下参数 最小连接数：是连接池一直保持的数据库连接,所以如果应用程序对数据库连接的使用量不大,将会有大量的数据库连接资源被浪费. 最大连接数：是连接池能申请的最大连接数,如果数据库连接请求超过次数,后面的数据库连接请求将被加入到等待队列中,这会影响以后的数据库操作 最大空闲时间 获取连接超时时间 超时重试连接次数 连接池使用过程中的主要问题： 事务问题：必须保证两个线程中的事务原子性，为此我们可以给每一个线程（事务）单独分配一个连接对象，开销大但是安全 连接池的配置与维护：最小连接数设置过大则启动配置慢但是使用过程中很快，反之则使用过程中较慢。根据需求进行具体配置。 最流行连接池Hikari Druid 数据库连接泄露： 如果在某次使用或者某段程序中没有正确地关闭 Connection、Statement 和 ResultSet 资源，那么每次执行都会留下一些没有关闭的连接，这些连接失去了引用而不能得到重新使用，因此就造成了数据库连接的泄漏。 需要进行连接的配置服务降级等操作 ","date":"2022-10-27","objectID":"/db/:0:2","series":null,"tags":null,"title":"Mysql","uri":"/db/#数据库连接泄露"},{"categories":["Tech"],"content":"索引 索引的优点 通过创建** 唯一性索引**，可以保证数据库表中每一行数据的唯一性； 可以加快数据的 检索速度，这也是创建索引的主要原因； 可以加速表和表之间的连接，特别是在实现 数据的参考完整性 方面特别有意义； 通过使用索引，可以在查询的过程中，使用 优化隐藏器，提高系统性能。 索引的缺点 时间上，创建和维护索引都要耗费时间，这种时间随着数据量的增加而增加，具体地，当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，这样就降低了数据的维护速度； 空间上，索引需要占 物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立聚簇索引，那么需要的空间就会更大。 索引为什么可以加快查询速度？ 因为使用索引后可以不用扫描全表来定位某行的数据，而是先通过索引表找到该行数据对应的物理地址然后访问相应的数据 同时，由于索引底层实现的有序性，使得在进行数据查询时，能够避免在磁盘不同扇区的随机寻址 使用索引后能够通过磁盘预读使得在磁盘上对数据的访问大致呈顺序的寻址。这本质上是依据局部性原理所实现的 总结： 索引大大减少了服务器需要扫描的数据量 索引可以帮助服务器避免排序和临时表 索引可以将随机I/O变成顺序I/O（局部性原理） 索引的数据结构 B Tree 层级结构中用页存储一定量的信息（16KB），所有数据存储在叶子结点中，非叶子结点只负责存储目录（关键字信息） 记录头信息里的record_type属性，它的各个取值代表的意思如下： 0：普通的用户记录 1：目录项记录 2：最小记录 3：最大记录 可以看到底层存储数据的页面中使用 2 3作为最小值和最大值的标记，中间的0为数据项。 层级中record_type为1的项中存储目录，其中分为key 和 page info，分别存储数据启示key和对应的页面number。 下图为B+树，和B树的唯一区别是叶子结点之间的连接 注：两个页面之间有连接方便范围查询。新分配的数据页编号可能并不是连续的，也就是说我们使用的这些页在存储空间里可能并不挨着 使用B树存储的上限取决于单个页面能够存储的数据量和目录量 按照层级相乘则是总共能存储的数据量 使用B+树的优点： 非叶子结点只存储key不放value，在同一页中可以读取到很多key更快缩小范围 叶子结点之间有一条链相连，加快全查找和范围查找的速度 稳定O(logn) Hash 索引 哈希索引采用一定的 哈希算法（常见哈希算法有 直接定址法、平方取中法、折叠法、除数取余法、随机数法），将数据库字段数据转换成定长的 Hash 值，与这条数据的行指针一并存入 Hash 表的对应位置，如果发生 Hash 碰撞（两个不同关键字的 Hash 值相同），则在对应 Hash 键下以 链表形式 存储。 检索时不需要类似 B+ 树那样从根节点到叶子节点逐级查找，只需一次哈希算法即可立刻定位到相应的位置，速度非常快，平均检索时间为 O(1)。 区别： hash索引查询快但是无法进行范围查询 hash索引无法进行索引排序 Hash 索引虽然在等值查询上较快，但是不稳定，性能不可预测，当某个键值存在大量重复的时候，发生 Hash 碰撞，此时效率可能极差；而 B+ 树的查询效率比较稳定，对于所有的查询都是从根结点到叶子结点，且树的高度较低。 hash无法进行模糊查询和前缀查询 前缀索引： 使用辨识度高的前几位作为前缀可以大大提升查询效率，但是如果辨识度低可能性能会很差。 最左前缀匹配规则： 在 MySQL 建立 联合索引（多列索引） 时会遵守最左前缀匹配原则，即 最左优先，在检索数据时从联合索引的最左边开始匹配。例如有一个 3 列索引（a,b,c），则已经对（a）、（a,b）、（a,b,c）上建立了索引。所以在创建 多列索引时，要根据业务需求，where 子句中 使用最频繁 的一列放在最左边。 根据最左前缀匹配原则，MySQL 会一直向右匹配直到遇到 范围查询（\u003e、\u003c、between、like）就停止匹配，比如采用查询条件 where a = 1 and b = 2 and c \u003e 3 and d = 4 时，如果建立（a,b,c,d）顺序的索引，d 是用不到索引的，如果建立（a,b,d,c）的索引则都可以用到，并且 where 子句中 a、b、d 的顺序可以任意调整。 如果建立的索引顺序是 （a,b） ，那么根据最左前缀匹配原则，直接采用查询条件 where b = 1 是无法利用到索引的。 如何添加索引 查询很少使用的烈不需要索引（增加维护成本 减缓查询速度） 只有很少数据的列不需要索引 修改操作远大于查询时不使用索引 有外键的列一定要索引 聚簇索引 聚簇索引指将 数据存储和 索引 放到一起，找到索引也就找到了数据。 一般而言使用在频繁使用、排序的字段上 ","date":"2022-10-27","objectID":"/db/:0:3","series":null,"tags":null,"title":"Mysql","uri":"/db/#索引"},{"categories":["Tech"],"content":"索引 索引的优点 通过创建** 唯一性索引**，可以保证数据库表中每一行数据的唯一性； 可以加快数据的 检索速度，这也是创建索引的主要原因； 可以加速表和表之间的连接，特别是在实现 数据的参考完整性 方面特别有意义； 通过使用索引，可以在查询的过程中，使用 优化隐藏器，提高系统性能。 索引的缺点 时间上，创建和维护索引都要耗费时间，这种时间随着数据量的增加而增加，具体地，当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，这样就降低了数据的维护速度； 空间上，索引需要占 物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立聚簇索引，那么需要的空间就会更大。 索引为什么可以加快查询速度？ 因为使用索引后可以不用扫描全表来定位某行的数据，而是先通过索引表找到该行数据对应的物理地址然后访问相应的数据 同时，由于索引底层实现的有序性，使得在进行数据查询时，能够避免在磁盘不同扇区的随机寻址 使用索引后能够通过磁盘预读使得在磁盘上对数据的访问大致呈顺序的寻址。这本质上是依据局部性原理所实现的 总结： 索引大大减少了服务器需要扫描的数据量 索引可以帮助服务器避免排序和临时表 索引可以将随机I/O变成顺序I/O（局部性原理） 索引的数据结构 B Tree 层级结构中用页存储一定量的信息（16KB），所有数据存储在叶子结点中，非叶子结点只负责存储目录（关键字信息） 记录头信息里的record_type属性，它的各个取值代表的意思如下： 0：普通的用户记录 1：目录项记录 2：最小记录 3：最大记录 可以看到底层存储数据的页面中使用 2 3作为最小值和最大值的标记，中间的0为数据项。 层级中record_type为1的项中存储目录，其中分为key 和 page info，分别存储数据启示key和对应的页面number。 下图为B+树，和B树的唯一区别是叶子结点之间的连接 注：两个页面之间有连接方便范围查询。新分配的数据页编号可能并不是连续的，也就是说我们使用的这些页在存储空间里可能并不挨着 使用B树存储的上限取决于单个页面能够存储的数据量和目录量 按照层级相乘则是总共能存储的数据量 使用B+树的优点： 非叶子结点只存储key不放value，在同一页中可以读取到很多key更快缩小范围 叶子结点之间有一条链相连，加快全查找和范围查找的速度 稳定O(logn) Hash 索引 哈希索引采用一定的 哈希算法（常见哈希算法有 直接定址法、平方取中法、折叠法、除数取余法、随机数法），将数据库字段数据转换成定长的 Hash 值，与这条数据的行指针一并存入 Hash 表的对应位置，如果发生 Hash 碰撞（两个不同关键字的 Hash 值相同），则在对应 Hash 键下以 链表形式 存储。 检索时不需要类似 B+ 树那样从根节点到叶子节点逐级查找，只需一次哈希算法即可立刻定位到相应的位置，速度非常快，平均检索时间为 O(1)。 区别： hash索引查询快但是无法进行范围查询 hash索引无法进行索引排序 Hash 索引虽然在等值查询上较快，但是不稳定，性能不可预测，当某个键值存在大量重复的时候，发生 Hash 碰撞，此时效率可能极差；而 B+ 树的查询效率比较稳定，对于所有的查询都是从根结点到叶子结点，且树的高度较低。 hash无法进行模糊查询和前缀查询 前缀索引： 使用辨识度高的前几位作为前缀可以大大提升查询效率，但是如果辨识度低可能性能会很差。 最左前缀匹配规则： 在 MySQL 建立 联合索引（多列索引） 时会遵守最左前缀匹配原则，即 最左优先，在检索数据时从联合索引的最左边开始匹配。例如有一个 3 列索引（a,b,c），则已经对（a）、（a,b）、（a,b,c）上建立了索引。所以在创建 多列索引时，要根据业务需求，where 子句中 使用最频繁 的一列放在最左边。 根据最左前缀匹配原则，MySQL 会一直向右匹配直到遇到 范围查询（、3 and d = 4 时，如果建立（a,b,c,d）顺序的索引，d 是用不到索引的，如果建立（a,b,d,c）的索引则都可以用到，并且 where 子句中 a、b、d 的顺序可以任意调整。 如果建立的索引顺序是 （a,b） ，那么根据最左前缀匹配原则，直接采用查询条件 where b = 1 是无法利用到索引的。 如何添加索引 查询很少使用的烈不需要索引（增加维护成本 减缓查询速度） 只有很少数据的列不需要索引 修改操作远大于查询时不使用索引 有外键的列一定要索引 聚簇索引 聚簇索引指将 数据存储和 索引 放到一起，找到索引也就找到了数据。 一般而言使用在频繁使用、排序的字段上 ","date":"2022-10-27","objectID":"/db/:0:3","series":null,"tags":null,"title":"Mysql","uri":"/db/#索引的优点"},{"categories":["Tech"],"content":"索引 索引的优点 通过创建** 唯一性索引**，可以保证数据库表中每一行数据的唯一性； 可以加快数据的 检索速度，这也是创建索引的主要原因； 可以加速表和表之间的连接，特别是在实现 数据的参考完整性 方面特别有意义； 通过使用索引，可以在查询的过程中，使用 优化隐藏器，提高系统性能。 索引的缺点 时间上，创建和维护索引都要耗费时间，这种时间随着数据量的增加而增加，具体地，当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，这样就降低了数据的维护速度； 空间上，索引需要占 物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立聚簇索引，那么需要的空间就会更大。 索引为什么可以加快查询速度？ 因为使用索引后可以不用扫描全表来定位某行的数据，而是先通过索引表找到该行数据对应的物理地址然后访问相应的数据 同时，由于索引底层实现的有序性，使得在进行数据查询时，能够避免在磁盘不同扇区的随机寻址 使用索引后能够通过磁盘预读使得在磁盘上对数据的访问大致呈顺序的寻址。这本质上是依据局部性原理所实现的 总结： 索引大大减少了服务器需要扫描的数据量 索引可以帮助服务器避免排序和临时表 索引可以将随机I/O变成顺序I/O（局部性原理） 索引的数据结构 B Tree 层级结构中用页存储一定量的信息（16KB），所有数据存储在叶子结点中，非叶子结点只负责存储目录（关键字信息） 记录头信息里的record_type属性，它的各个取值代表的意思如下： 0：普通的用户记录 1：目录项记录 2：最小记录 3：最大记录 可以看到底层存储数据的页面中使用 2 3作为最小值和最大值的标记，中间的0为数据项。 层级中record_type为1的项中存储目录，其中分为key 和 page info，分别存储数据启示key和对应的页面number。 下图为B+树，和B树的唯一区别是叶子结点之间的连接 注：两个页面之间有连接方便范围查询。新分配的数据页编号可能并不是连续的，也就是说我们使用的这些页在存储空间里可能并不挨着 使用B树存储的上限取决于单个页面能够存储的数据量和目录量 按照层级相乘则是总共能存储的数据量 使用B+树的优点： 非叶子结点只存储key不放value，在同一页中可以读取到很多key更快缩小范围 叶子结点之间有一条链相连，加快全查找和范围查找的速度 稳定O(logn) Hash 索引 哈希索引采用一定的 哈希算法（常见哈希算法有 直接定址法、平方取中法、折叠法、除数取余法、随机数法），将数据库字段数据转换成定长的 Hash 值，与这条数据的行指针一并存入 Hash 表的对应位置，如果发生 Hash 碰撞（两个不同关键字的 Hash 值相同），则在对应 Hash 键下以 链表形式 存储。 检索时不需要类似 B+ 树那样从根节点到叶子节点逐级查找，只需一次哈希算法即可立刻定位到相应的位置，速度非常快，平均检索时间为 O(1)。 区别： hash索引查询快但是无法进行范围查询 hash索引无法进行索引排序 Hash 索引虽然在等值查询上较快，但是不稳定，性能不可预测，当某个键值存在大量重复的时候，发生 Hash 碰撞，此时效率可能极差；而 B+ 树的查询效率比较稳定，对于所有的查询都是从根结点到叶子结点，且树的高度较低。 hash无法进行模糊查询和前缀查询 前缀索引： 使用辨识度高的前几位作为前缀可以大大提升查询效率，但是如果辨识度低可能性能会很差。 最左前缀匹配规则： 在 MySQL 建立 联合索引（多列索引） 时会遵守最左前缀匹配原则，即 最左优先，在检索数据时从联合索引的最左边开始匹配。例如有一个 3 列索引（a,b,c），则已经对（a）、（a,b）、（a,b,c）上建立了索引。所以在创建 多列索引时，要根据业务需求，where 子句中 使用最频繁 的一列放在最左边。 根据最左前缀匹配原则，MySQL 会一直向右匹配直到遇到 范围查询（、3 and d = 4 时，如果建立（a,b,c,d）顺序的索引，d 是用不到索引的，如果建立（a,b,d,c）的索引则都可以用到，并且 where 子句中 a、b、d 的顺序可以任意调整。 如果建立的索引顺序是 （a,b） ，那么根据最左前缀匹配原则，直接采用查询条件 where b = 1 是无法利用到索引的。 如何添加索引 查询很少使用的烈不需要索引（增加维护成本 减缓查询速度） 只有很少数据的列不需要索引 修改操作远大于查询时不使用索引 有外键的列一定要索引 聚簇索引 聚簇索引指将 数据存储和 索引 放到一起，找到索引也就找到了数据。 一般而言使用在频繁使用、排序的字段上 ","date":"2022-10-27","objectID":"/db/:0:3","series":null,"tags":null,"title":"Mysql","uri":"/db/#索引的缺点"},{"categories":["Tech"],"content":"索引 索引的优点 通过创建** 唯一性索引**，可以保证数据库表中每一行数据的唯一性； 可以加快数据的 检索速度，这也是创建索引的主要原因； 可以加速表和表之间的连接，特别是在实现 数据的参考完整性 方面特别有意义； 通过使用索引，可以在查询的过程中，使用 优化隐藏器，提高系统性能。 索引的缺点 时间上，创建和维护索引都要耗费时间，这种时间随着数据量的增加而增加，具体地，当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，这样就降低了数据的维护速度； 空间上，索引需要占 物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立聚簇索引，那么需要的空间就会更大。 索引为什么可以加快查询速度？ 因为使用索引后可以不用扫描全表来定位某行的数据，而是先通过索引表找到该行数据对应的物理地址然后访问相应的数据 同时，由于索引底层实现的有序性，使得在进行数据查询时，能够避免在磁盘不同扇区的随机寻址 使用索引后能够通过磁盘预读使得在磁盘上对数据的访问大致呈顺序的寻址。这本质上是依据局部性原理所实现的 总结： 索引大大减少了服务器需要扫描的数据量 索引可以帮助服务器避免排序和临时表 索引可以将随机I/O变成顺序I/O（局部性原理） 索引的数据结构 B Tree 层级结构中用页存储一定量的信息（16KB），所有数据存储在叶子结点中，非叶子结点只负责存储目录（关键字信息） 记录头信息里的record_type属性，它的各个取值代表的意思如下： 0：普通的用户记录 1：目录项记录 2：最小记录 3：最大记录 可以看到底层存储数据的页面中使用 2 3作为最小值和最大值的标记，中间的0为数据项。 层级中record_type为1的项中存储目录，其中分为key 和 page info，分别存储数据启示key和对应的页面number。 下图为B+树，和B树的唯一区别是叶子结点之间的连接 注：两个页面之间有连接方便范围查询。新分配的数据页编号可能并不是连续的，也就是说我们使用的这些页在存储空间里可能并不挨着 使用B树存储的上限取决于单个页面能够存储的数据量和目录量 按照层级相乘则是总共能存储的数据量 使用B+树的优点： 非叶子结点只存储key不放value，在同一页中可以读取到很多key更快缩小范围 叶子结点之间有一条链相连，加快全查找和范围查找的速度 稳定O(logn) Hash 索引 哈希索引采用一定的 哈希算法（常见哈希算法有 直接定址法、平方取中法、折叠法、除数取余法、随机数法），将数据库字段数据转换成定长的 Hash 值，与这条数据的行指针一并存入 Hash 表的对应位置，如果发生 Hash 碰撞（两个不同关键字的 Hash 值相同），则在对应 Hash 键下以 链表形式 存储。 检索时不需要类似 B+ 树那样从根节点到叶子节点逐级查找，只需一次哈希算法即可立刻定位到相应的位置，速度非常快，平均检索时间为 O(1)。 区别： hash索引查询快但是无法进行范围查询 hash索引无法进行索引排序 Hash 索引虽然在等值查询上较快，但是不稳定，性能不可预测，当某个键值存在大量重复的时候，发生 Hash 碰撞，此时效率可能极差；而 B+ 树的查询效率比较稳定，对于所有的查询都是从根结点到叶子结点，且树的高度较低。 hash无法进行模糊查询和前缀查询 前缀索引： 使用辨识度高的前几位作为前缀可以大大提升查询效率，但是如果辨识度低可能性能会很差。 最左前缀匹配规则： 在 MySQL 建立 联合索引（多列索引） 时会遵守最左前缀匹配原则，即 最左优先，在检索数据时从联合索引的最左边开始匹配。例如有一个 3 列索引（a,b,c），则已经对（a）、（a,b）、（a,b,c）上建立了索引。所以在创建 多列索引时，要根据业务需求，where 子句中 使用最频繁 的一列放在最左边。 根据最左前缀匹配原则，MySQL 会一直向右匹配直到遇到 范围查询（、3 and d = 4 时，如果建立（a,b,c,d）顺序的索引，d 是用不到索引的，如果建立（a,b,d,c）的索引则都可以用到，并且 where 子句中 a、b、d 的顺序可以任意调整。 如果建立的索引顺序是 （a,b） ，那么根据最左前缀匹配原则，直接采用查询条件 where b = 1 是无法利用到索引的。 如何添加索引 查询很少使用的烈不需要索引（增加维护成本 减缓查询速度） 只有很少数据的列不需要索引 修改操作远大于查询时不使用索引 有外键的列一定要索引 聚簇索引 聚簇索引指将 数据存储和 索引 放到一起，找到索引也就找到了数据。 一般而言使用在频繁使用、排序的字段上 ","date":"2022-10-27","objectID":"/db/:0:3","series":null,"tags":null,"title":"Mysql","uri":"/db/#索引的数据结构"},{"categories":["Tech"],"content":"索引 索引的优点 通过创建** 唯一性索引**，可以保证数据库表中每一行数据的唯一性； 可以加快数据的 检索速度，这也是创建索引的主要原因； 可以加速表和表之间的连接，特别是在实现 数据的参考完整性 方面特别有意义； 通过使用索引，可以在查询的过程中，使用 优化隐藏器，提高系统性能。 索引的缺点 时间上，创建和维护索引都要耗费时间，这种时间随着数据量的增加而增加，具体地，当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，这样就降低了数据的维护速度； 空间上，索引需要占 物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立聚簇索引，那么需要的空间就会更大。 索引为什么可以加快查询速度？ 因为使用索引后可以不用扫描全表来定位某行的数据，而是先通过索引表找到该行数据对应的物理地址然后访问相应的数据 同时，由于索引底层实现的有序性，使得在进行数据查询时，能够避免在磁盘不同扇区的随机寻址 使用索引后能够通过磁盘预读使得在磁盘上对数据的访问大致呈顺序的寻址。这本质上是依据局部性原理所实现的 总结： 索引大大减少了服务器需要扫描的数据量 索引可以帮助服务器避免排序和临时表 索引可以将随机I/O变成顺序I/O（局部性原理） 索引的数据结构 B Tree 层级结构中用页存储一定量的信息（16KB），所有数据存储在叶子结点中，非叶子结点只负责存储目录（关键字信息） 记录头信息里的record_type属性，它的各个取值代表的意思如下： 0：普通的用户记录 1：目录项记录 2：最小记录 3：最大记录 可以看到底层存储数据的页面中使用 2 3作为最小值和最大值的标记，中间的0为数据项。 层级中record_type为1的项中存储目录，其中分为key 和 page info，分别存储数据启示key和对应的页面number。 下图为B+树，和B树的唯一区别是叶子结点之间的连接 注：两个页面之间有连接方便范围查询。新分配的数据页编号可能并不是连续的，也就是说我们使用的这些页在存储空间里可能并不挨着 使用B树存储的上限取决于单个页面能够存储的数据量和目录量 按照层级相乘则是总共能存储的数据量 使用B+树的优点： 非叶子结点只存储key不放value，在同一页中可以读取到很多key更快缩小范围 叶子结点之间有一条链相连，加快全查找和范围查找的速度 稳定O(logn) Hash 索引 哈希索引采用一定的 哈希算法（常见哈希算法有 直接定址法、平方取中法、折叠法、除数取余法、随机数法），将数据库字段数据转换成定长的 Hash 值，与这条数据的行指针一并存入 Hash 表的对应位置，如果发生 Hash 碰撞（两个不同关键字的 Hash 值相同），则在对应 Hash 键下以 链表形式 存储。 检索时不需要类似 B+ 树那样从根节点到叶子节点逐级查找，只需一次哈希算法即可立刻定位到相应的位置，速度非常快，平均检索时间为 O(1)。 区别： hash索引查询快但是无法进行范围查询 hash索引无法进行索引排序 Hash 索引虽然在等值查询上较快，但是不稳定，性能不可预测，当某个键值存在大量重复的时候，发生 Hash 碰撞，此时效率可能极差；而 B+ 树的查询效率比较稳定，对于所有的查询都是从根结点到叶子结点，且树的高度较低。 hash无法进行模糊查询和前缀查询 前缀索引： 使用辨识度高的前几位作为前缀可以大大提升查询效率，但是如果辨识度低可能性能会很差。 最左前缀匹配规则： 在 MySQL 建立 联合索引（多列索引） 时会遵守最左前缀匹配原则，即 最左优先，在检索数据时从联合索引的最左边开始匹配。例如有一个 3 列索引（a,b,c），则已经对（a）、（a,b）、（a,b,c）上建立了索引。所以在创建 多列索引时，要根据业务需求，where 子句中 使用最频繁 的一列放在最左边。 根据最左前缀匹配原则，MySQL 会一直向右匹配直到遇到 范围查询（、3 and d = 4 时，如果建立（a,b,c,d）顺序的索引，d 是用不到索引的，如果建立（a,b,d,c）的索引则都可以用到，并且 where 子句中 a、b、d 的顺序可以任意调整。 如果建立的索引顺序是 （a,b） ，那么根据最左前缀匹配原则，直接采用查询条件 where b = 1 是无法利用到索引的。 如何添加索引 查询很少使用的烈不需要索引（增加维护成本 减缓查询速度） 只有很少数据的列不需要索引 修改操作远大于查询时不使用索引 有外键的列一定要索引 聚簇索引 聚簇索引指将 数据存储和 索引 放到一起，找到索引也就找到了数据。 一般而言使用在频繁使用、排序的字段上 ","date":"2022-10-27","objectID":"/db/:0:3","series":null,"tags":null,"title":"Mysql","uri":"/db/#b-tree"},{"categories":["Tech"],"content":"索引 索引的优点 通过创建** 唯一性索引**，可以保证数据库表中每一行数据的唯一性； 可以加快数据的 检索速度，这也是创建索引的主要原因； 可以加速表和表之间的连接，特别是在实现 数据的参考完整性 方面特别有意义； 通过使用索引，可以在查询的过程中，使用 优化隐藏器，提高系统性能。 索引的缺点 时间上，创建和维护索引都要耗费时间，这种时间随着数据量的增加而增加，具体地，当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，这样就降低了数据的维护速度； 空间上，索引需要占 物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立聚簇索引，那么需要的空间就会更大。 索引为什么可以加快查询速度？ 因为使用索引后可以不用扫描全表来定位某行的数据，而是先通过索引表找到该行数据对应的物理地址然后访问相应的数据 同时，由于索引底层实现的有序性，使得在进行数据查询时，能够避免在磁盘不同扇区的随机寻址 使用索引后能够通过磁盘预读使得在磁盘上对数据的访问大致呈顺序的寻址。这本质上是依据局部性原理所实现的 总结： 索引大大减少了服务器需要扫描的数据量 索引可以帮助服务器避免排序和临时表 索引可以将随机I/O变成顺序I/O（局部性原理） 索引的数据结构 B Tree 层级结构中用页存储一定量的信息（16KB），所有数据存储在叶子结点中，非叶子结点只负责存储目录（关键字信息） 记录头信息里的record_type属性，它的各个取值代表的意思如下： 0：普通的用户记录 1：目录项记录 2：最小记录 3：最大记录 可以看到底层存储数据的页面中使用 2 3作为最小值和最大值的标记，中间的0为数据项。 层级中record_type为1的项中存储目录，其中分为key 和 page info，分别存储数据启示key和对应的页面number。 下图为B+树，和B树的唯一区别是叶子结点之间的连接 注：两个页面之间有连接方便范围查询。新分配的数据页编号可能并不是连续的，也就是说我们使用的这些页在存储空间里可能并不挨着 使用B树存储的上限取决于单个页面能够存储的数据量和目录量 按照层级相乘则是总共能存储的数据量 使用B+树的优点： 非叶子结点只存储key不放value，在同一页中可以读取到很多key更快缩小范围 叶子结点之间有一条链相连，加快全查找和范围查找的速度 稳定O(logn) Hash 索引 哈希索引采用一定的 哈希算法（常见哈希算法有 直接定址法、平方取中法、折叠法、除数取余法、随机数法），将数据库字段数据转换成定长的 Hash 值，与这条数据的行指针一并存入 Hash 表的对应位置，如果发生 Hash 碰撞（两个不同关键字的 Hash 值相同），则在对应 Hash 键下以 链表形式 存储。 检索时不需要类似 B+ 树那样从根节点到叶子节点逐级查找，只需一次哈希算法即可立刻定位到相应的位置，速度非常快，平均检索时间为 O(1)。 区别： hash索引查询快但是无法进行范围查询 hash索引无法进行索引排序 Hash 索引虽然在等值查询上较快，但是不稳定，性能不可预测，当某个键值存在大量重复的时候，发生 Hash 碰撞，此时效率可能极差；而 B+ 树的查询效率比较稳定，对于所有的查询都是从根结点到叶子结点，且树的高度较低。 hash无法进行模糊查询和前缀查询 前缀索引： 使用辨识度高的前几位作为前缀可以大大提升查询效率，但是如果辨识度低可能性能会很差。 最左前缀匹配规则： 在 MySQL 建立 联合索引（多列索引） 时会遵守最左前缀匹配原则，即 最左优先，在检索数据时从联合索引的最左边开始匹配。例如有一个 3 列索引（a,b,c），则已经对（a）、（a,b）、（a,b,c）上建立了索引。所以在创建 多列索引时，要根据业务需求，where 子句中 使用最频繁 的一列放在最左边。 根据最左前缀匹配原则，MySQL 会一直向右匹配直到遇到 范围查询（、3 and d = 4 时，如果建立（a,b,c,d）顺序的索引，d 是用不到索引的，如果建立（a,b,d,c）的索引则都可以用到，并且 where 子句中 a、b、d 的顺序可以任意调整。 如果建立的索引顺序是 （a,b） ，那么根据最左前缀匹配原则，直接采用查询条件 where b = 1 是无法利用到索引的。 如何添加索引 查询很少使用的烈不需要索引（增加维护成本 减缓查询速度） 只有很少数据的列不需要索引 修改操作远大于查询时不使用索引 有外键的列一定要索引 聚簇索引 聚簇索引指将 数据存储和 索引 放到一起，找到索引也就找到了数据。 一般而言使用在频繁使用、排序的字段上 ","date":"2022-10-27","objectID":"/db/:0:3","series":null,"tags":null,"title":"Mysql","uri":"/db/#hash-索引"},{"categories":["Tech"],"content":"索引 索引的优点 通过创建** 唯一性索引**，可以保证数据库表中每一行数据的唯一性； 可以加快数据的 检索速度，这也是创建索引的主要原因； 可以加速表和表之间的连接，特别是在实现 数据的参考完整性 方面特别有意义； 通过使用索引，可以在查询的过程中，使用 优化隐藏器，提高系统性能。 索引的缺点 时间上，创建和维护索引都要耗费时间，这种时间随着数据量的增加而增加，具体地，当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，这样就降低了数据的维护速度； 空间上，索引需要占 物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立聚簇索引，那么需要的空间就会更大。 索引为什么可以加快查询速度？ 因为使用索引后可以不用扫描全表来定位某行的数据，而是先通过索引表找到该行数据对应的物理地址然后访问相应的数据 同时，由于索引底层实现的有序性，使得在进行数据查询时，能够避免在磁盘不同扇区的随机寻址 使用索引后能够通过磁盘预读使得在磁盘上对数据的访问大致呈顺序的寻址。这本质上是依据局部性原理所实现的 总结： 索引大大减少了服务器需要扫描的数据量 索引可以帮助服务器避免排序和临时表 索引可以将随机I/O变成顺序I/O（局部性原理） 索引的数据结构 B Tree 层级结构中用页存储一定量的信息（16KB），所有数据存储在叶子结点中，非叶子结点只负责存储目录（关键字信息） 记录头信息里的record_type属性，它的各个取值代表的意思如下： 0：普通的用户记录 1：目录项记录 2：最小记录 3：最大记录 可以看到底层存储数据的页面中使用 2 3作为最小值和最大值的标记，中间的0为数据项。 层级中record_type为1的项中存储目录，其中分为key 和 page info，分别存储数据启示key和对应的页面number。 下图为B+树，和B树的唯一区别是叶子结点之间的连接 注：两个页面之间有连接方便范围查询。新分配的数据页编号可能并不是连续的，也就是说我们使用的这些页在存储空间里可能并不挨着 使用B树存储的上限取决于单个页面能够存储的数据量和目录量 按照层级相乘则是总共能存储的数据量 使用B+树的优点： 非叶子结点只存储key不放value，在同一页中可以读取到很多key更快缩小范围 叶子结点之间有一条链相连，加快全查找和范围查找的速度 稳定O(logn) Hash 索引 哈希索引采用一定的 哈希算法（常见哈希算法有 直接定址法、平方取中法、折叠法、除数取余法、随机数法），将数据库字段数据转换成定长的 Hash 值，与这条数据的行指针一并存入 Hash 表的对应位置，如果发生 Hash 碰撞（两个不同关键字的 Hash 值相同），则在对应 Hash 键下以 链表形式 存储。 检索时不需要类似 B+ 树那样从根节点到叶子节点逐级查找，只需一次哈希算法即可立刻定位到相应的位置，速度非常快，平均检索时间为 O(1)。 区别： hash索引查询快但是无法进行范围查询 hash索引无法进行索引排序 Hash 索引虽然在等值查询上较快，但是不稳定，性能不可预测，当某个键值存在大量重复的时候，发生 Hash 碰撞，此时效率可能极差；而 B+ 树的查询效率比较稳定，对于所有的查询都是从根结点到叶子结点，且树的高度较低。 hash无法进行模糊查询和前缀查询 前缀索引： 使用辨识度高的前几位作为前缀可以大大提升查询效率，但是如果辨识度低可能性能会很差。 最左前缀匹配规则： 在 MySQL 建立 联合索引（多列索引） 时会遵守最左前缀匹配原则，即 最左优先，在检索数据时从联合索引的最左边开始匹配。例如有一个 3 列索引（a,b,c），则已经对（a）、（a,b）、（a,b,c）上建立了索引。所以在创建 多列索引时，要根据业务需求，where 子句中 使用最频繁 的一列放在最左边。 根据最左前缀匹配原则，MySQL 会一直向右匹配直到遇到 范围查询（、3 and d = 4 时，如果建立（a,b,c,d）顺序的索引，d 是用不到索引的，如果建立（a,b,d,c）的索引则都可以用到，并且 where 子句中 a、b、d 的顺序可以任意调整。 如果建立的索引顺序是 （a,b） ，那么根据最左前缀匹配原则，直接采用查询条件 where b = 1 是无法利用到索引的。 如何添加索引 查询很少使用的烈不需要索引（增加维护成本 减缓查询速度） 只有很少数据的列不需要索引 修改操作远大于查询时不使用索引 有外键的列一定要索引 聚簇索引 聚簇索引指将 数据存储和 索引 放到一起，找到索引也就找到了数据。 一般而言使用在频繁使用、排序的字段上 ","date":"2022-10-27","objectID":"/db/:0:3","series":null,"tags":null,"title":"Mysql","uri":"/db/#前缀索引"},{"categories":["Tech"],"content":"索引 索引的优点 通过创建** 唯一性索引**，可以保证数据库表中每一行数据的唯一性； 可以加快数据的 检索速度，这也是创建索引的主要原因； 可以加速表和表之间的连接，特别是在实现 数据的参考完整性 方面特别有意义； 通过使用索引，可以在查询的过程中，使用 优化隐藏器，提高系统性能。 索引的缺点 时间上，创建和维护索引都要耗费时间，这种时间随着数据量的增加而增加，具体地，当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，这样就降低了数据的维护速度； 空间上，索引需要占 物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立聚簇索引，那么需要的空间就会更大。 索引为什么可以加快查询速度？ 因为使用索引后可以不用扫描全表来定位某行的数据，而是先通过索引表找到该行数据对应的物理地址然后访问相应的数据 同时，由于索引底层实现的有序性，使得在进行数据查询时，能够避免在磁盘不同扇区的随机寻址 使用索引后能够通过磁盘预读使得在磁盘上对数据的访问大致呈顺序的寻址。这本质上是依据局部性原理所实现的 总结： 索引大大减少了服务器需要扫描的数据量 索引可以帮助服务器避免排序和临时表 索引可以将随机I/O变成顺序I/O（局部性原理） 索引的数据结构 B Tree 层级结构中用页存储一定量的信息（16KB），所有数据存储在叶子结点中，非叶子结点只负责存储目录（关键字信息） 记录头信息里的record_type属性，它的各个取值代表的意思如下： 0：普通的用户记录 1：目录项记录 2：最小记录 3：最大记录 可以看到底层存储数据的页面中使用 2 3作为最小值和最大值的标记，中间的0为数据项。 层级中record_type为1的项中存储目录，其中分为key 和 page info，分别存储数据启示key和对应的页面number。 下图为B+树，和B树的唯一区别是叶子结点之间的连接 注：两个页面之间有连接方便范围查询。新分配的数据页编号可能并不是连续的，也就是说我们使用的这些页在存储空间里可能并不挨着 使用B树存储的上限取决于单个页面能够存储的数据量和目录量 按照层级相乘则是总共能存储的数据量 使用B+树的优点： 非叶子结点只存储key不放value，在同一页中可以读取到很多key更快缩小范围 叶子结点之间有一条链相连，加快全查找和范围查找的速度 稳定O(logn) Hash 索引 哈希索引采用一定的 哈希算法（常见哈希算法有 直接定址法、平方取中法、折叠法、除数取余法、随机数法），将数据库字段数据转换成定长的 Hash 值，与这条数据的行指针一并存入 Hash 表的对应位置，如果发生 Hash 碰撞（两个不同关键字的 Hash 值相同），则在对应 Hash 键下以 链表形式 存储。 检索时不需要类似 B+ 树那样从根节点到叶子节点逐级查找，只需一次哈希算法即可立刻定位到相应的位置，速度非常快，平均检索时间为 O(1)。 区别： hash索引查询快但是无法进行范围查询 hash索引无法进行索引排序 Hash 索引虽然在等值查询上较快，但是不稳定，性能不可预测，当某个键值存在大量重复的时候，发生 Hash 碰撞，此时效率可能极差；而 B+ 树的查询效率比较稳定，对于所有的查询都是从根结点到叶子结点，且树的高度较低。 hash无法进行模糊查询和前缀查询 前缀索引： 使用辨识度高的前几位作为前缀可以大大提升查询效率，但是如果辨识度低可能性能会很差。 最左前缀匹配规则： 在 MySQL 建立 联合索引（多列索引） 时会遵守最左前缀匹配原则，即 最左优先，在检索数据时从联合索引的最左边开始匹配。例如有一个 3 列索引（a,b,c），则已经对（a）、（a,b）、（a,b,c）上建立了索引。所以在创建 多列索引时，要根据业务需求，where 子句中 使用最频繁 的一列放在最左边。 根据最左前缀匹配原则，MySQL 会一直向右匹配直到遇到 范围查询（、3 and d = 4 时，如果建立（a,b,c,d）顺序的索引，d 是用不到索引的，如果建立（a,b,d,c）的索引则都可以用到，并且 where 子句中 a、b、d 的顺序可以任意调整。 如果建立的索引顺序是 （a,b） ，那么根据最左前缀匹配原则，直接采用查询条件 where b = 1 是无法利用到索引的。 如何添加索引 查询很少使用的烈不需要索引（增加维护成本 减缓查询速度） 只有很少数据的列不需要索引 修改操作远大于查询时不使用索引 有外键的列一定要索引 聚簇索引 聚簇索引指将 数据存储和 索引 放到一起，找到索引也就找到了数据。 一般而言使用在频繁使用、排序的字段上 ","date":"2022-10-27","objectID":"/db/:0:3","series":null,"tags":null,"title":"Mysql","uri":"/db/#最左前缀匹配规则"},{"categories":["Tech"],"content":"索引 索引的优点 通过创建** 唯一性索引**，可以保证数据库表中每一行数据的唯一性； 可以加快数据的 检索速度，这也是创建索引的主要原因； 可以加速表和表之间的连接，特别是在实现 数据的参考完整性 方面特别有意义； 通过使用索引，可以在查询的过程中，使用 优化隐藏器，提高系统性能。 索引的缺点 时间上，创建和维护索引都要耗费时间，这种时间随着数据量的增加而增加，具体地，当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，这样就降低了数据的维护速度； 空间上，索引需要占 物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立聚簇索引，那么需要的空间就会更大。 索引为什么可以加快查询速度？ 因为使用索引后可以不用扫描全表来定位某行的数据，而是先通过索引表找到该行数据对应的物理地址然后访问相应的数据 同时，由于索引底层实现的有序性，使得在进行数据查询时，能够避免在磁盘不同扇区的随机寻址 使用索引后能够通过磁盘预读使得在磁盘上对数据的访问大致呈顺序的寻址。这本质上是依据局部性原理所实现的 总结： 索引大大减少了服务器需要扫描的数据量 索引可以帮助服务器避免排序和临时表 索引可以将随机I/O变成顺序I/O（局部性原理） 索引的数据结构 B Tree 层级结构中用页存储一定量的信息（16KB），所有数据存储在叶子结点中，非叶子结点只负责存储目录（关键字信息） 记录头信息里的record_type属性，它的各个取值代表的意思如下： 0：普通的用户记录 1：目录项记录 2：最小记录 3：最大记录 可以看到底层存储数据的页面中使用 2 3作为最小值和最大值的标记，中间的0为数据项。 层级中record_type为1的项中存储目录，其中分为key 和 page info，分别存储数据启示key和对应的页面number。 下图为B+树，和B树的唯一区别是叶子结点之间的连接 注：两个页面之间有连接方便范围查询。新分配的数据页编号可能并不是连续的，也就是说我们使用的这些页在存储空间里可能并不挨着 使用B树存储的上限取决于单个页面能够存储的数据量和目录量 按照层级相乘则是总共能存储的数据量 使用B+树的优点： 非叶子结点只存储key不放value，在同一页中可以读取到很多key更快缩小范围 叶子结点之间有一条链相连，加快全查找和范围查找的速度 稳定O(logn) Hash 索引 哈希索引采用一定的 哈希算法（常见哈希算法有 直接定址法、平方取中法、折叠法、除数取余法、随机数法），将数据库字段数据转换成定长的 Hash 值，与这条数据的行指针一并存入 Hash 表的对应位置，如果发生 Hash 碰撞（两个不同关键字的 Hash 值相同），则在对应 Hash 键下以 链表形式 存储。 检索时不需要类似 B+ 树那样从根节点到叶子节点逐级查找，只需一次哈希算法即可立刻定位到相应的位置，速度非常快，平均检索时间为 O(1)。 区别： hash索引查询快但是无法进行范围查询 hash索引无法进行索引排序 Hash 索引虽然在等值查询上较快，但是不稳定，性能不可预测，当某个键值存在大量重复的时候，发生 Hash 碰撞，此时效率可能极差；而 B+ 树的查询效率比较稳定，对于所有的查询都是从根结点到叶子结点，且树的高度较低。 hash无法进行模糊查询和前缀查询 前缀索引： 使用辨识度高的前几位作为前缀可以大大提升查询效率，但是如果辨识度低可能性能会很差。 最左前缀匹配规则： 在 MySQL 建立 联合索引（多列索引） 时会遵守最左前缀匹配原则，即 最左优先，在检索数据时从联合索引的最左边开始匹配。例如有一个 3 列索引（a,b,c），则已经对（a）、（a,b）、（a,b,c）上建立了索引。所以在创建 多列索引时，要根据业务需求，where 子句中 使用最频繁 的一列放在最左边。 根据最左前缀匹配原则，MySQL 会一直向右匹配直到遇到 范围查询（、3 and d = 4 时，如果建立（a,b,c,d）顺序的索引，d 是用不到索引的，如果建立（a,b,d,c）的索引则都可以用到，并且 where 子句中 a、b、d 的顺序可以任意调整。 如果建立的索引顺序是 （a,b） ，那么根据最左前缀匹配原则，直接采用查询条件 where b = 1 是无法利用到索引的。 如何添加索引 查询很少使用的烈不需要索引（增加维护成本 减缓查询速度） 只有很少数据的列不需要索引 修改操作远大于查询时不使用索引 有外键的列一定要索引 聚簇索引 聚簇索引指将 数据存储和 索引 放到一起，找到索引也就找到了数据。 一般而言使用在频繁使用、排序的字段上 ","date":"2022-10-27","objectID":"/db/:0:3","series":null,"tags":null,"title":"Mysql","uri":"/db/#如何添加索引"},{"categories":["Tech"],"content":"索引 索引的优点 通过创建** 唯一性索引**，可以保证数据库表中每一行数据的唯一性； 可以加快数据的 检索速度，这也是创建索引的主要原因； 可以加速表和表之间的连接，特别是在实现 数据的参考完整性 方面特别有意义； 通过使用索引，可以在查询的过程中，使用 优化隐藏器，提高系统性能。 索引的缺点 时间上，创建和维护索引都要耗费时间，这种时间随着数据量的增加而增加，具体地，当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，这样就降低了数据的维护速度； 空间上，索引需要占 物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立聚簇索引，那么需要的空间就会更大。 索引为什么可以加快查询速度？ 因为使用索引后可以不用扫描全表来定位某行的数据，而是先通过索引表找到该行数据对应的物理地址然后访问相应的数据 同时，由于索引底层实现的有序性，使得在进行数据查询时，能够避免在磁盘不同扇区的随机寻址 使用索引后能够通过磁盘预读使得在磁盘上对数据的访问大致呈顺序的寻址。这本质上是依据局部性原理所实现的 总结： 索引大大减少了服务器需要扫描的数据量 索引可以帮助服务器避免排序和临时表 索引可以将随机I/O变成顺序I/O（局部性原理） 索引的数据结构 B Tree 层级结构中用页存储一定量的信息（16KB），所有数据存储在叶子结点中，非叶子结点只负责存储目录（关键字信息） 记录头信息里的record_type属性，它的各个取值代表的意思如下： 0：普通的用户记录 1：目录项记录 2：最小记录 3：最大记录 可以看到底层存储数据的页面中使用 2 3作为最小值和最大值的标记，中间的0为数据项。 层级中record_type为1的项中存储目录，其中分为key 和 page info，分别存储数据启示key和对应的页面number。 下图为B+树，和B树的唯一区别是叶子结点之间的连接 注：两个页面之间有连接方便范围查询。新分配的数据页编号可能并不是连续的，也就是说我们使用的这些页在存储空间里可能并不挨着 使用B树存储的上限取决于单个页面能够存储的数据量和目录量 按照层级相乘则是总共能存储的数据量 使用B+树的优点： 非叶子结点只存储key不放value，在同一页中可以读取到很多key更快缩小范围 叶子结点之间有一条链相连，加快全查找和范围查找的速度 稳定O(logn) Hash 索引 哈希索引采用一定的 哈希算法（常见哈希算法有 直接定址法、平方取中法、折叠法、除数取余法、随机数法），将数据库字段数据转换成定长的 Hash 值，与这条数据的行指针一并存入 Hash 表的对应位置，如果发生 Hash 碰撞（两个不同关键字的 Hash 值相同），则在对应 Hash 键下以 链表形式 存储。 检索时不需要类似 B+ 树那样从根节点到叶子节点逐级查找，只需一次哈希算法即可立刻定位到相应的位置，速度非常快，平均检索时间为 O(1)。 区别： hash索引查询快但是无法进行范围查询 hash索引无法进行索引排序 Hash 索引虽然在等值查询上较快，但是不稳定，性能不可预测，当某个键值存在大量重复的时候，发生 Hash 碰撞，此时效率可能极差；而 B+ 树的查询效率比较稳定，对于所有的查询都是从根结点到叶子结点，且树的高度较低。 hash无法进行模糊查询和前缀查询 前缀索引： 使用辨识度高的前几位作为前缀可以大大提升查询效率，但是如果辨识度低可能性能会很差。 最左前缀匹配规则： 在 MySQL 建立 联合索引（多列索引） 时会遵守最左前缀匹配原则，即 最左优先，在检索数据时从联合索引的最左边开始匹配。例如有一个 3 列索引（a,b,c），则已经对（a）、（a,b）、（a,b,c）上建立了索引。所以在创建 多列索引时，要根据业务需求，where 子句中 使用最频繁 的一列放在最左边。 根据最左前缀匹配原则，MySQL 会一直向右匹配直到遇到 范围查询（、3 and d = 4 时，如果建立（a,b,c,d）顺序的索引，d 是用不到索引的，如果建立（a,b,d,c）的索引则都可以用到，并且 where 子句中 a、b、d 的顺序可以任意调整。 如果建立的索引顺序是 （a,b） ，那么根据最左前缀匹配原则，直接采用查询条件 where b = 1 是无法利用到索引的。 如何添加索引 查询很少使用的烈不需要索引（增加维护成本 减缓查询速度） 只有很少数据的列不需要索引 修改操作远大于查询时不使用索引 有外键的列一定要索引 聚簇索引 聚簇索引指将 数据存储和 索引 放到一起，找到索引也就找到了数据。 一般而言使用在频繁使用、排序的字段上 ","date":"2022-10-27","objectID":"/db/:0:3","series":null,"tags":null,"title":"Mysql","uri":"/db/#聚簇索引"},{"categories":["Tech"],"content":"事务 数据库的 事务（Transaction）是一种机制、一个操作序列，包含了一组数据库操作命令，其执行的结果必须使数据库从一种一致性状态变到另一种一致性状态。事务把所有的命令作为一个整体一起向系统提交或撤销操作请求，即这一组数据库命令要么都执行，要么都不执行，因此事务是一个不可分割的工作逻辑单元。如果任意一个操作失败，那么整组操作即为失败，会回到操作前状态或者是上一个节点。 因此，事务是保持 逻辑数据一致性 和 可恢复性 的重要利器。而锁是实现事务的关键，可以保证事务的完整性和并发性 事务状态 活跃状态：事务的第一个状态，任何正在执行的事务都处于此状态，所做的 更改 存储在 主内存的缓冲区 中。 部分提交状态：执行上次操作后，事务进入部分提交状态。之所以是部分提交，是因为所做的更改仍然在主内存的缓冲区中。 失败状态：如果某个检查在活动状态下失败，在活动状态或部分提交状态发生一些错误，并且事务无法进一步执行，则事务进入失败状态。 中止状态：如果任何事务已达到失败状态，则恢复管理器将数据库回滚到开始执行的原始状态。 提交状态：如果所有操作成功执行，则来自 部分提交状态 的事务进入提交状态。无法从此状态回滚，它是一个新的 一致状态。 ACID 原子性 事务是最小的执行单位，不可分割的（原子的）。事务的原子性确保动作要么全部执行，要么全部不执行。 以 银行转账 事务为例，如果该事务提交了，则这两个账户的数据将会更新；如果由于某种原因，事务在成功更新这两个账户之前终止了，则不会更新这两个账户的余额，并且会 **撤销 **对任何账户余额的修改，回到此操作前状态，即事务不能部分提交。 一致性 当事务完成时，数据必须处于一致状态，多个事务对同一个数据读取的结果是相同的。 以银行转账事务事务为例。在事务开始之前，所有 账户余额的总额处于一致状态。在事务进行的过程中，一个账户余额减少了，而另一个账户余额尚未修改。因此，所有账户余额的总额处于不一致状态。但是当事务完成以后，账户余额的总额再次恢复到一致状态。 隔离性 并发访问数据库 时，一个用户的事务不被其他事务所干扰，各个事务不干涉内部的数据。 修改数据的事务可以在另一个使用相同数据的事务开始之前访问这些数据，或者在另一个使用相同数据的事务结束之后访问这些数据。 持久性 一个事务被提交之后，它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。 A原子性:由undo log日志保证，它记录了需要回滚的日志信息，事务回滚时撤销已经执行成功的sql C一致性:一般由代码层面来保证 I隔离性:由MVCC来保证 D持久性:由内存+redo log来保证，mysql修改数据同时在内存和redo log记录这次操作，事务提交的时候通过redo log刷盘，宕机的时候可以从redo log恢复 事务之间互相影响的例子： 脏读（Dirty Read） 一个事务读取了另一个事务未提交的数据。 不可重复读（Non-repeatable Read） 就是在一个事务范围内，两次相同的查询会返回两个不同的数据，这是因为在此间隔内有其他事务对数据进行了修改。 幻读（Phantom Read） 幻读是指当事务 不是独立执行时 发生的一种现象，例如有一个事务对表中的数据进行了修改，这种修改涉及到表中的全部数据行，同时，第一个事务也修改这个表中的数据，这种修改是向表中 插入一行新数据。那么，第一个事务的用户发现表中还有没有修改的数据行，就好像发生了幻觉一样。 丢失更新（Lost Update） 两个事务同时读取同一条记录，事务 A 先修改记录，事务 B 也修改记录（B 是不知道 A 修改过），当 B 提交数据后， 其修改结果覆盖了 A 的修改结果，导致事务 A 更新丢失。 事务隔离级别 隔离级别 脏读 不可重复读 幻读 丢失更新 读取未提交 是 是 是 是 读取已提交 否 是 是 是 可重复读 否 否 是 否 可串行化 否 否 否 否 读取未提交 最低的隔离级别，一个事务可以读到另一个事务未提交的结果，所有的并发事务问题都会发生。 读取已提交 只有在事务提交后，其更新结果才会被其他事务看见，可以解决 脏读问题，但是不可重复读或幻读仍有可能发生。Oracle 默认采用的是该隔离级别。 可重复读 在一个事务中，对于同一份数据的读取结果总是相同的，无论是否有其他事务对这份数据进行操作，以及这个事务是否提交，除非数据是被本身事务自己所修改。可以解决 脏读、不可重复读。MySQL 默认采用可重复读隔离级别。 可串行化 事务 串行化执行，隔离级别最高，完全服从 ACID，牺牲了系统的并发性，也就是说，所有事务依次逐个执行，所以可以解决并发事务的所有问题。 ","date":"2022-10-27","objectID":"/db/:0:4","series":null,"tags":null,"title":"Mysql","uri":"/db/#事务"},{"categories":["Tech"],"content":"事务 数据库的 事务（Transaction）是一种机制、一个操作序列，包含了一组数据库操作命令，其执行的结果必须使数据库从一种一致性状态变到另一种一致性状态。事务把所有的命令作为一个整体一起向系统提交或撤销操作请求，即这一组数据库命令要么都执行，要么都不执行，因此事务是一个不可分割的工作逻辑单元。如果任意一个操作失败，那么整组操作即为失败，会回到操作前状态或者是上一个节点。 因此，事务是保持 逻辑数据一致性 和 可恢复性 的重要利器。而锁是实现事务的关键，可以保证事务的完整性和并发性 事务状态 活跃状态：事务的第一个状态，任何正在执行的事务都处于此状态，所做的 更改 存储在 主内存的缓冲区 中。 部分提交状态：执行上次操作后，事务进入部分提交状态。之所以是部分提交，是因为所做的更改仍然在主内存的缓冲区中。 失败状态：如果某个检查在活动状态下失败，在活动状态或部分提交状态发生一些错误，并且事务无法进一步执行，则事务进入失败状态。 中止状态：如果任何事务已达到失败状态，则恢复管理器将数据库回滚到开始执行的原始状态。 提交状态：如果所有操作成功执行，则来自 部分提交状态 的事务进入提交状态。无法从此状态回滚，它是一个新的 一致状态。 ACID 原子性 事务是最小的执行单位，不可分割的（原子的）。事务的原子性确保动作要么全部执行，要么全部不执行。 以 银行转账 事务为例，如果该事务提交了，则这两个账户的数据将会更新；如果由于某种原因，事务在成功更新这两个账户之前终止了，则不会更新这两个账户的余额，并且会 **撤销 **对任何账户余额的修改，回到此操作前状态，即事务不能部分提交。 一致性 当事务完成时，数据必须处于一致状态，多个事务对同一个数据读取的结果是相同的。 以银行转账事务事务为例。在事务开始之前，所有 账户余额的总额处于一致状态。在事务进行的过程中，一个账户余额减少了，而另一个账户余额尚未修改。因此，所有账户余额的总额处于不一致状态。但是当事务完成以后，账户余额的总额再次恢复到一致状态。 隔离性 并发访问数据库 时，一个用户的事务不被其他事务所干扰，各个事务不干涉内部的数据。 修改数据的事务可以在另一个使用相同数据的事务开始之前访问这些数据，或者在另一个使用相同数据的事务结束之后访问这些数据。 持久性 一个事务被提交之后，它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。 A原子性:由undo log日志保证，它记录了需要回滚的日志信息，事务回滚时撤销已经执行成功的sql C一致性:一般由代码层面来保证 I隔离性:由MVCC来保证 D持久性:由内存+redo log来保证，mysql修改数据同时在内存和redo log记录这次操作，事务提交的时候通过redo log刷盘，宕机的时候可以从redo log恢复 事务之间互相影响的例子： 脏读（Dirty Read） 一个事务读取了另一个事务未提交的数据。 不可重复读（Non-repeatable Read） 就是在一个事务范围内，两次相同的查询会返回两个不同的数据，这是因为在此间隔内有其他事务对数据进行了修改。 幻读（Phantom Read） 幻读是指当事务 不是独立执行时 发生的一种现象，例如有一个事务对表中的数据进行了修改，这种修改涉及到表中的全部数据行，同时，第一个事务也修改这个表中的数据，这种修改是向表中 插入一行新数据。那么，第一个事务的用户发现表中还有没有修改的数据行，就好像发生了幻觉一样。 丢失更新（Lost Update） 两个事务同时读取同一条记录，事务 A 先修改记录，事务 B 也修改记录（B 是不知道 A 修改过），当 B 提交数据后， 其修改结果覆盖了 A 的修改结果，导致事务 A 更新丢失。 事务隔离级别 隔离级别 脏读 不可重复读 幻读 丢失更新 读取未提交 是 是 是 是 读取已提交 否 是 是 是 可重复读 否 否 是 否 可串行化 否 否 否 否 读取未提交 最低的隔离级别，一个事务可以读到另一个事务未提交的结果，所有的并发事务问题都会发生。 读取已提交 只有在事务提交后，其更新结果才会被其他事务看见，可以解决 脏读问题，但是不可重复读或幻读仍有可能发生。Oracle 默认采用的是该隔离级别。 可重复读 在一个事务中，对于同一份数据的读取结果总是相同的，无论是否有其他事务对这份数据进行操作，以及这个事务是否提交，除非数据是被本身事务自己所修改。可以解决 脏读、不可重复读。MySQL 默认采用可重复读隔离级别。 可串行化 事务 串行化执行，隔离级别最高，完全服从 ACID，牺牲了系统的并发性，也就是说，所有事务依次逐个执行，所以可以解决并发事务的所有问题。 ","date":"2022-10-27","objectID":"/db/:0:4","series":null,"tags":null,"title":"Mysql","uri":"/db/#事务状态"},{"categories":["Tech"],"content":"事务 数据库的 事务（Transaction）是一种机制、一个操作序列，包含了一组数据库操作命令，其执行的结果必须使数据库从一种一致性状态变到另一种一致性状态。事务把所有的命令作为一个整体一起向系统提交或撤销操作请求，即这一组数据库命令要么都执行，要么都不执行，因此事务是一个不可分割的工作逻辑单元。如果任意一个操作失败，那么整组操作即为失败，会回到操作前状态或者是上一个节点。 因此，事务是保持 逻辑数据一致性 和 可恢复性 的重要利器。而锁是实现事务的关键，可以保证事务的完整性和并发性 事务状态 活跃状态：事务的第一个状态，任何正在执行的事务都处于此状态，所做的 更改 存储在 主内存的缓冲区 中。 部分提交状态：执行上次操作后，事务进入部分提交状态。之所以是部分提交，是因为所做的更改仍然在主内存的缓冲区中。 失败状态：如果某个检查在活动状态下失败，在活动状态或部分提交状态发生一些错误，并且事务无法进一步执行，则事务进入失败状态。 中止状态：如果任何事务已达到失败状态，则恢复管理器将数据库回滚到开始执行的原始状态。 提交状态：如果所有操作成功执行，则来自 部分提交状态 的事务进入提交状态。无法从此状态回滚，它是一个新的 一致状态。 ACID 原子性 事务是最小的执行单位，不可分割的（原子的）。事务的原子性确保动作要么全部执行，要么全部不执行。 以 银行转账 事务为例，如果该事务提交了，则这两个账户的数据将会更新；如果由于某种原因，事务在成功更新这两个账户之前终止了，则不会更新这两个账户的余额，并且会 **撤销 **对任何账户余额的修改，回到此操作前状态，即事务不能部分提交。 一致性 当事务完成时，数据必须处于一致状态，多个事务对同一个数据读取的结果是相同的。 以银行转账事务事务为例。在事务开始之前，所有 账户余额的总额处于一致状态。在事务进行的过程中，一个账户余额减少了，而另一个账户余额尚未修改。因此，所有账户余额的总额处于不一致状态。但是当事务完成以后，账户余额的总额再次恢复到一致状态。 隔离性 并发访问数据库 时，一个用户的事务不被其他事务所干扰，各个事务不干涉内部的数据。 修改数据的事务可以在另一个使用相同数据的事务开始之前访问这些数据，或者在另一个使用相同数据的事务结束之后访问这些数据。 持久性 一个事务被提交之后，它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。 A原子性:由undo log日志保证，它记录了需要回滚的日志信息，事务回滚时撤销已经执行成功的sql C一致性:一般由代码层面来保证 I隔离性:由MVCC来保证 D持久性:由内存+redo log来保证，mysql修改数据同时在内存和redo log记录这次操作，事务提交的时候通过redo log刷盘，宕机的时候可以从redo log恢复 事务之间互相影响的例子： 脏读（Dirty Read） 一个事务读取了另一个事务未提交的数据。 不可重复读（Non-repeatable Read） 就是在一个事务范围内，两次相同的查询会返回两个不同的数据，这是因为在此间隔内有其他事务对数据进行了修改。 幻读（Phantom Read） 幻读是指当事务 不是独立执行时 发生的一种现象，例如有一个事务对表中的数据进行了修改，这种修改涉及到表中的全部数据行，同时，第一个事务也修改这个表中的数据，这种修改是向表中 插入一行新数据。那么，第一个事务的用户发现表中还有没有修改的数据行，就好像发生了幻觉一样。 丢失更新（Lost Update） 两个事务同时读取同一条记录，事务 A 先修改记录，事务 B 也修改记录（B 是不知道 A 修改过），当 B 提交数据后， 其修改结果覆盖了 A 的修改结果，导致事务 A 更新丢失。 事务隔离级别 隔离级别 脏读 不可重复读 幻读 丢失更新 读取未提交 是 是 是 是 读取已提交 否 是 是 是 可重复读 否 否 是 否 可串行化 否 否 否 否 读取未提交 最低的隔离级别，一个事务可以读到另一个事务未提交的结果，所有的并发事务问题都会发生。 读取已提交 只有在事务提交后，其更新结果才会被其他事务看见，可以解决 脏读问题，但是不可重复读或幻读仍有可能发生。Oracle 默认采用的是该隔离级别。 可重复读 在一个事务中，对于同一份数据的读取结果总是相同的，无论是否有其他事务对这份数据进行操作，以及这个事务是否提交，除非数据是被本身事务自己所修改。可以解决 脏读、不可重复读。MySQL 默认采用可重复读隔离级别。 可串行化 事务 串行化执行，隔离级别最高，完全服从 ACID，牺牲了系统的并发性，也就是说，所有事务依次逐个执行，所以可以解决并发事务的所有问题。 ","date":"2022-10-27","objectID":"/db/:0:4","series":null,"tags":null,"title":"Mysql","uri":"/db/#acid"},{"categories":["Tech"],"content":"典型问题： 数据保存在内存的优点和缺点分别是什么？ 优点是存取速度快，缺点是数据无法永久保存。 触发器的使用场景有哪些？ • 可以通过数据库中的相关表实现 级联更改； • 实时监控某张表中的某个字段的更改，并需要做出相应的处理。 ","date":"2022-10-27","objectID":"/db/:0:5","series":null,"tags":null,"title":"Mysql","uri":"/db/#典型问题"},{"categories":["Tech"],"content":"锁 共享锁（S）：又叫 他读锁。可以并发读取数据，但不能修改数据。也就是说当数据资源上存在共享锁时，所有的事务都不能对该数据进行修改，直到数据读取完成，共享锁释放。 可以理解为读锁， 多个事务可以封锁同一个共享页 任何事务都不能修改此页但可以读 页被读取完后S锁自动释放 排它锁（X）：又叫 独占锁、写锁。对数据资源进行增删改操作时，不允许其它事务操作这块资源，直到排它锁被释放，从而防止同时对同一资源进行多重操作。 仅允许一个事务封锁此页 其他事务必须等待X锁被释放时才能对页进行访问 事务结束后X锁才会被释放 select for update 会锁住行以及任何关联的索引条目，其他事务的update会被阻塞。所有S和X锁在事务提交或者回滚后被释放。 更新锁（U）：防止出现 死锁 的锁模式，两个事务对一个数据资源进行先读取再修改的情况下，使用共享锁和排它锁有时会出现死锁现象，而使用更新锁就可以避免死锁的出现。 用来预定对此页施加X锁，允许其他事务读但不允许再施加U或者X锁 需要更新时升级为X锁 事务结束后U锁才会被释放 意向锁：表示 SQL Server 需要在 层次结构中的某些底层资源上 获取共享锁或排它锁。例如，放置在 表级 的 **共享意向锁 **表示事务打算在表中的页或行上放置共享锁。在表级设置意向锁可防止另一个事务随后在包含那一页的表上获取排它锁。 意向锁可以提高性能，因为 SQL Server 仅在 表级 检查意向锁来确定事务是否可以安全地获取该表上的锁，而无须检查表中的每行或每页上的锁以确定事务是否可以锁定整个表。 意向锁包括意向共享 (IS)、意向排它 (IX) 以及与意向排它共享 (SIX)。 乐观锁（Optimistic Locking）认为对同一数据的并发操作不会总发生，属于小概率事件，不用每次都对数据上锁，也就是不采用数据库自身的锁机制，而是通过程序来实现。在程序上，我们可以采用版本号机制或者时间戳机制实现。 每次访问时根据某些特殊字段（时间戳等）进行判断是否有人修改过当前值 时间戳和版本号机制一样，也是在更新提交的时候，将当前数据的时间戳和更新之前取得的时间戳进行比较，如果两者一致则更新成功，否则就是版本冲突。 **悲观锁（Pessimistic Locking）**也是指一种锁的思想，对数据被其他事务的修改持保守态度，会通过数据库自身的锁机制(表锁，行锁，读锁，写锁等）来实现，从而保证数据操作的排它性。 数据库能够确定那些行需要锁的情况下使用行锁，如果不知道会影响哪些行的时候就会使用表锁。 举个例子，一个用户表user，有主键id和用户生日birthday。 当你使用update … where id=?这样的语句时，数据库明确知道会影响哪一行，它就会使用行锁； 当你使用update … where birthday=?这样的的语句时，因为事先不知道会影响哪些行就可能会使用表锁。 从这两种锁的设计思想中，可以看出乐观锁和悲观锁的适用场景： 乐观锁适合读操作多的场景，相对来说写的操作比较少。它的优点在于程序实现，不存在死锁问题，不过适用场景也会相对乐观，因为它阻止不了除了程序以外的数据库操作。 悲观锁适合写操作多的场景，因为写的操作具有排它性。采用悲观锁的方式，可以在数据库层面阻止其他事务对该数据的操作权限，防止读 - 写和写 - 写的冲突。 死锁 如果一组进程中的每一个进程都在等待仅由该组进程中的其他进程才能引发的事件，那么该组进程是死锁的。 死锁有四个必要条件 互斥 请求和保持 不可抢占 循环等待 常见原因： 事务之间对资源的交替访问 并发修改同一个记录（使用乐观锁） 索引不当导致死锁（注意不要使用过多的关联多表的查询，优化索引） 预防死锁：设置某些限制条件破坏死锁的几个必要条件 避免死锁：在资源的动态分配中，用某种方法（如银行家算法）防止系统进入不安全状态 检测死锁 解除死锁 如何尽可能的避免死锁呢？ 1）以固定的顺序访问表和行。即按顺序申请锁，这样就不会造成互相等待的场面。 2）大事务拆小。大事务更倾向于死锁，如果业务允许，将大事务拆小。 3）在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁概率。 4）降低隔离级别。如果业务允许，将隔离级别调低也是较好的选择，比如将隔离级别从RR调整为RC，可以避免掉很多因为gap锁造成的死锁。 5）为表添加合理的索引。如果不走索引将会为表的每一行记录添加上锁，死锁的概率大大增大。 ","date":"2022-10-27","objectID":"/db/:0:6","series":null,"tags":null,"title":"Mysql","uri":"/db/#锁"},{"categories":["Tech"],"content":"锁 共享锁（S）：又叫 他读锁。可以并发读取数据，但不能修改数据。也就是说当数据资源上存在共享锁时，所有的事务都不能对该数据进行修改，直到数据读取完成，共享锁释放。 可以理解为读锁， 多个事务可以封锁同一个共享页 任何事务都不能修改此页但可以读 页被读取完后S锁自动释放 排它锁（X）：又叫 独占锁、写锁。对数据资源进行增删改操作时，不允许其它事务操作这块资源，直到排它锁被释放，从而防止同时对同一资源进行多重操作。 仅允许一个事务封锁此页 其他事务必须等待X锁被释放时才能对页进行访问 事务结束后X锁才会被释放 select for update 会锁住行以及任何关联的索引条目，其他事务的update会被阻塞。所有S和X锁在事务提交或者回滚后被释放。 更新锁（U）：防止出现 死锁 的锁模式，两个事务对一个数据资源进行先读取再修改的情况下，使用共享锁和排它锁有时会出现死锁现象，而使用更新锁就可以避免死锁的出现。 用来预定对此页施加X锁，允许其他事务读但不允许再施加U或者X锁 需要更新时升级为X锁 事务结束后U锁才会被释放 意向锁：表示 SQL Server 需要在 层次结构中的某些底层资源上 获取共享锁或排它锁。例如，放置在 表级 的 **共享意向锁 **表示事务打算在表中的页或行上放置共享锁。在表级设置意向锁可防止另一个事务随后在包含那一页的表上获取排它锁。 意向锁可以提高性能，因为 SQL Server 仅在 表级 检查意向锁来确定事务是否可以安全地获取该表上的锁，而无须检查表中的每行或每页上的锁以确定事务是否可以锁定整个表。 意向锁包括意向共享 (IS)、意向排它 (IX) 以及与意向排它共享 (SIX)。 乐观锁（Optimistic Locking）认为对同一数据的并发操作不会总发生，属于小概率事件，不用每次都对数据上锁，也就是不采用数据库自身的锁机制，而是通过程序来实现。在程序上，我们可以采用版本号机制或者时间戳机制实现。 每次访问时根据某些特殊字段（时间戳等）进行判断是否有人修改过当前值 时间戳和版本号机制一样，也是在更新提交的时候，将当前数据的时间戳和更新之前取得的时间戳进行比较，如果两者一致则更新成功，否则就是版本冲突。 **悲观锁（Pessimistic Locking）**也是指一种锁的思想，对数据被其他事务的修改持保守态度，会通过数据库自身的锁机制(表锁，行锁，读锁，写锁等）来实现，从而保证数据操作的排它性。 数据库能够确定那些行需要锁的情况下使用行锁，如果不知道会影响哪些行的时候就会使用表锁。 举个例子，一个用户表user，有主键id和用户生日birthday。 当你使用update … where id=?这样的语句时，数据库明确知道会影响哪一行，它就会使用行锁； 当你使用update … where birthday=?这样的的语句时，因为事先不知道会影响哪些行就可能会使用表锁。 从这两种锁的设计思想中，可以看出乐观锁和悲观锁的适用场景： 乐观锁适合读操作多的场景，相对来说写的操作比较少。它的优点在于程序实现，不存在死锁问题，不过适用场景也会相对乐观，因为它阻止不了除了程序以外的数据库操作。 悲观锁适合写操作多的场景，因为写的操作具有排它性。采用悲观锁的方式，可以在数据库层面阻止其他事务对该数据的操作权限，防止读 - 写和写 - 写的冲突。 死锁 如果一组进程中的每一个进程都在等待仅由该组进程中的其他进程才能引发的事件，那么该组进程是死锁的。 死锁有四个必要条件 互斥 请求和保持 不可抢占 循环等待 常见原因： 事务之间对资源的交替访问 并发修改同一个记录（使用乐观锁） 索引不当导致死锁（注意不要使用过多的关联多表的查询，优化索引） 预防死锁：设置某些限制条件破坏死锁的几个必要条件 避免死锁：在资源的动态分配中，用某种方法（如银行家算法）防止系统进入不安全状态 检测死锁 解除死锁 如何尽可能的避免死锁呢？ 1）以固定的顺序访问表和行。即按顺序申请锁，这样就不会造成互相等待的场面。 2）大事务拆小。大事务更倾向于死锁，如果业务允许，将大事务拆小。 3）在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁概率。 4）降低隔离级别。如果业务允许，将隔离级别调低也是较好的选择，比如将隔离级别从RR调整为RC，可以避免掉很多因为gap锁造成的死锁。 5）为表添加合理的索引。如果不走索引将会为表的每一行记录添加上锁，死锁的概率大大增大。 ","date":"2022-10-27","objectID":"/db/:0:6","series":null,"tags":null,"title":"Mysql","uri":"/db/#死锁"},{"categories":["Tech"],"content":"SQL 语句可分为以下几类： 数据定义语言 DDL（Data Definition Language）：例如 CREATE，DROP，ALTER 等，对逻辑结构等有操作的，其中包括表结构，视图和索引。 数据查询语言 DQL（Data Query Language）：即查询操作，以 SELECT 关键字为主，各种简单查询、连接查询等都属于 DQL。 数据操纵语言 DML（Data Manipulation Language）：例如 INSERT，UPDATE，DELETE 等，对数据进行操作的。DQL 与 DML共同构建了多数初级程序员常用的 增删改查 操作，而查询是较为特殊的一种，被划分到 DQL 中。 数据控制功能 DCL（Data Control Language）：例如 GRANT，REVOKE，COMMIT，ROLLBACK 等，对数据库安全性、完整性等有操作的，可以简单的理解为权限控制等。 keys 超 键：在关系中，能唯一标识元组的属性集称为关系模式的超键。一个属性可以作为一个超键，多个属性组合在一起也可以作为一个超键。超键包含候选键和主键。 候选键：是最小超键，即没有冗余元素的超键。 主 键：数据库表中对储存数据对象予以 唯一和完整标识的数据列或属性的组合。一个数据列只能有一个主键，且主键的取值不能缺失，即不能为空值（NULL）。 外 键：在一个表中存在的另一个表的主键称此表的外键，外键可以有重复的, 可以是空值。外键是用来和其他表建立联系用的。 举例子： 现在有个表（学号，身份证号，姓名，分数） 学号和身份证号都可以唯一标识，因此只要含有这两个属性的属性集合就可以是超键，如（学号，姓名）、（身份证号，分数）等等。 而将超键的属性集合中的冗余属性剔除，则变成了候选键，这里（学号）和（身份证号）为候选键。 主键则是表设计者从候选键中选取的属性或属性集合，构成主键，可以单独选学号或身份证号作为主键，也可以选（学号，身份证号）一起作为主键。 外键则很好理解了，就是某个属性，是其他表的主键，则在这个表中就是外键。 char 与 varchar 的区别 char 表示定长字符串，长度是固定的，最多能存放的字符个数为 255，和编码无关；而 varchar 表示可变长字符串，长度是可变的，最多能存放的字符个数为 65532； 使用 char 时，如果插入数据的长度小于 char 的固定长度时，则用空格填充； 因为固定长度，char 的存取速度比 varchar 快很多，同时缺点是会占用多余空间，属于空间换时间； DROP、DELETE 与 TRUNCATE 的区别 三种都可以表示删除，其中的细微区别之处如下： DROP DELETE TRUNCATE SQL 语句类型 DDL DML DDL 回滚 不可回滚 可回滚 不可回滚 删除内容 从数据库中 删除表， 表结构还在，删除表的 表结构还在，删除表中的所有数据 所有的数据行， 全部或者一部分数据行 索引和权限也会被删除 删除速度 删除速度最快 删除速度慢，需要逐行删除 删除速度快 因此，在不再需要一张表的时候，采用 DROP；在想删除部分数据行时候，用 DELETE；在保留表而删除所有数据的时候用 TRUNCATE。 [SQL] UNION [SQL] UNION 和 UNION ALL UNION 会去重 因此比 UNION ALL快很多 SQL优化 将字段很多的表分解成多个表 对于字段较多的表，如果有些字段的使用频率很低，可以将这些字段分离出来形成新表。 增加中间表 对于需要经常 联合查询 的表，通过建立中间表以提高查询效率，具体地，将需要通过联合查询的数据插入到中间表中，然后将原来的联合查询改为对中间表的查询。 增加冗余字段 众所皆知，设计数据表时应尽量遵循范式理论的规约，尽可能的减少冗余字段，让数据库设计看起来精致、优雅。但是，表的规范化程度越高，表和表之间的关系越多，需要连接查询的情况也就越多，性能也就越差，所以合理的加入冗余字段可以提高查询速度。 partition and sharding 分库的问题 事务问题 分库分表后，就成了分布式事务。如果依赖数据库本身的分布式事务管理功能去执行事务，将付出高昂的性能代价； 如果由应用程序去协助控制，形成程序逻辑上的事务，又会造成编程方面的负担。 跨库跨表的 JOIN 问题 在执行了分库分表之后，难以避免会将原本逻辑关联性很强的数据划分到不同的表、不同的库上，这时，表的关联操作将受到限制，我们无法 JOIN 位于不同分库的表，也无法 JOIN 分表粒度不同的表，结果原本一次查询能够完成的业务，可能需要多次查询才能完成。 额外的数据管理负担和数据运算压力 额外的数据管理负担，最为常见的是数据的 定位问题 和数据的 增删改查 的重复执行问题，这些都可以通过应用程序来解决，但必然会引起额外的逻辑运算。 ","date":"2022-10-27","objectID":"/db/:0:7","series":null,"tags":null,"title":"Mysql","uri":"/db/#sql"},{"categories":["Tech"],"content":"SQL 语句可分为以下几类： 数据定义语言 DDL（Data Definition Language）：例如 CREATE，DROP，ALTER 等，对逻辑结构等有操作的，其中包括表结构，视图和索引。 数据查询语言 DQL（Data Query Language）：即查询操作，以 SELECT 关键字为主，各种简单查询、连接查询等都属于 DQL。 数据操纵语言 DML（Data Manipulation Language）：例如 INSERT，UPDATE，DELETE 等，对数据进行操作的。DQL 与 DML共同构建了多数初级程序员常用的 增删改查 操作，而查询是较为特殊的一种，被划分到 DQL 中。 数据控制功能 DCL（Data Control Language）：例如 GRANT，REVOKE，COMMIT，ROLLBACK 等，对数据库安全性、完整性等有操作的，可以简单的理解为权限控制等。 keys 超 键：在关系中，能唯一标识元组的属性集称为关系模式的超键。一个属性可以作为一个超键，多个属性组合在一起也可以作为一个超键。超键包含候选键和主键。 候选键：是最小超键，即没有冗余元素的超键。 主 键：数据库表中对储存数据对象予以 唯一和完整标识的数据列或属性的组合。一个数据列只能有一个主键，且主键的取值不能缺失，即不能为空值（NULL）。 外 键：在一个表中存在的另一个表的主键称此表的外键，外键可以有重复的, 可以是空值。外键是用来和其他表建立联系用的。 举例子： 现在有个表（学号，身份证号，姓名，分数） 学号和身份证号都可以唯一标识，因此只要含有这两个属性的属性集合就可以是超键，如（学号，姓名）、（身份证号，分数）等等。 而将超键的属性集合中的冗余属性剔除，则变成了候选键，这里（学号）和（身份证号）为候选键。 主键则是表设计者从候选键中选取的属性或属性集合，构成主键，可以单独选学号或身份证号作为主键，也可以选（学号，身份证号）一起作为主键。 外键则很好理解了，就是某个属性，是其他表的主键，则在这个表中就是外键。 char 与 varchar 的区别 char 表示定长字符串，长度是固定的，最多能存放的字符个数为 255，和编码无关；而 varchar 表示可变长字符串，长度是可变的，最多能存放的字符个数为 65532； 使用 char 时，如果插入数据的长度小于 char 的固定长度时，则用空格填充； 因为固定长度，char 的存取速度比 varchar 快很多，同时缺点是会占用多余空间，属于空间换时间； DROP、DELETE 与 TRUNCATE 的区别 三种都可以表示删除，其中的细微区别之处如下： DROP DELETE TRUNCATE SQL 语句类型 DDL DML DDL 回滚 不可回滚 可回滚 不可回滚 删除内容 从数据库中 删除表， 表结构还在，删除表的 表结构还在，删除表中的所有数据 所有的数据行， 全部或者一部分数据行 索引和权限也会被删除 删除速度 删除速度最快 删除速度慢，需要逐行删除 删除速度快 因此，在不再需要一张表的时候，采用 DROP；在想删除部分数据行时候，用 DELETE；在保留表而删除所有数据的时候用 TRUNCATE。 [SQL] UNION [SQL] UNION 和 UNION ALL UNION 会去重 因此比 UNION ALL快很多 SQL优化 将字段很多的表分解成多个表 对于字段较多的表，如果有些字段的使用频率很低，可以将这些字段分离出来形成新表。 增加中间表 对于需要经常 联合查询 的表，通过建立中间表以提高查询效率，具体地，将需要通过联合查询的数据插入到中间表中，然后将原来的联合查询改为对中间表的查询。 增加冗余字段 众所皆知，设计数据表时应尽量遵循范式理论的规约，尽可能的减少冗余字段，让数据库设计看起来精致、优雅。但是，表的规范化程度越高，表和表之间的关系越多，需要连接查询的情况也就越多，性能也就越差，所以合理的加入冗余字段可以提高查询速度。 partition and sharding 分库的问题 事务问题 分库分表后，就成了分布式事务。如果依赖数据库本身的分布式事务管理功能去执行事务，将付出高昂的性能代价； 如果由应用程序去协助控制，形成程序逻辑上的事务，又会造成编程方面的负担。 跨库跨表的 JOIN 问题 在执行了分库分表之后，难以避免会将原本逻辑关联性很强的数据划分到不同的表、不同的库上，这时，表的关联操作将受到限制，我们无法 JOIN 位于不同分库的表，也无法 JOIN 分表粒度不同的表，结果原本一次查询能够完成的业务，可能需要多次查询才能完成。 额外的数据管理负担和数据运算压力 额外的数据管理负担，最为常见的是数据的 定位问题 和数据的 增删改查 的重复执行问题，这些都可以通过应用程序来解决，但必然会引起额外的逻辑运算。 ","date":"2022-10-27","objectID":"/db/:0:7","series":null,"tags":null,"title":"Mysql","uri":"/db/#sql优化"},{"categories":["Tech"],"content":"SQL 语句可分为以下几类： 数据定义语言 DDL（Data Definition Language）：例如 CREATE，DROP，ALTER 等，对逻辑结构等有操作的，其中包括表结构，视图和索引。 数据查询语言 DQL（Data Query Language）：即查询操作，以 SELECT 关键字为主，各种简单查询、连接查询等都属于 DQL。 数据操纵语言 DML（Data Manipulation Language）：例如 INSERT，UPDATE，DELETE 等，对数据进行操作的。DQL 与 DML共同构建了多数初级程序员常用的 增删改查 操作，而查询是较为特殊的一种，被划分到 DQL 中。 数据控制功能 DCL（Data Control Language）：例如 GRANT，REVOKE，COMMIT，ROLLBACK 等，对数据库安全性、完整性等有操作的，可以简单的理解为权限控制等。 keys 超 键：在关系中，能唯一标识元组的属性集称为关系模式的超键。一个属性可以作为一个超键，多个属性组合在一起也可以作为一个超键。超键包含候选键和主键。 候选键：是最小超键，即没有冗余元素的超键。 主 键：数据库表中对储存数据对象予以 唯一和完整标识的数据列或属性的组合。一个数据列只能有一个主键，且主键的取值不能缺失，即不能为空值（NULL）。 外 键：在一个表中存在的另一个表的主键称此表的外键，外键可以有重复的, 可以是空值。外键是用来和其他表建立联系用的。 举例子： 现在有个表（学号，身份证号，姓名，分数） 学号和身份证号都可以唯一标识，因此只要含有这两个属性的属性集合就可以是超键，如（学号，姓名）、（身份证号，分数）等等。 而将超键的属性集合中的冗余属性剔除，则变成了候选键，这里（学号）和（身份证号）为候选键。 主键则是表设计者从候选键中选取的属性或属性集合，构成主键，可以单独选学号或身份证号作为主键，也可以选（学号，身份证号）一起作为主键。 外键则很好理解了，就是某个属性，是其他表的主键，则在这个表中就是外键。 char 与 varchar 的区别 char 表示定长字符串，长度是固定的，最多能存放的字符个数为 255，和编码无关；而 varchar 表示可变长字符串，长度是可变的，最多能存放的字符个数为 65532； 使用 char 时，如果插入数据的长度小于 char 的固定长度时，则用空格填充； 因为固定长度，char 的存取速度比 varchar 快很多，同时缺点是会占用多余空间，属于空间换时间； DROP、DELETE 与 TRUNCATE 的区别 三种都可以表示删除，其中的细微区别之处如下： DROP DELETE TRUNCATE SQL 语句类型 DDL DML DDL 回滚 不可回滚 可回滚 不可回滚 删除内容 从数据库中 删除表， 表结构还在，删除表的 表结构还在，删除表中的所有数据 所有的数据行， 全部或者一部分数据行 索引和权限也会被删除 删除速度 删除速度最快 删除速度慢，需要逐行删除 删除速度快 因此，在不再需要一张表的时候，采用 DROP；在想删除部分数据行时候，用 DELETE；在保留表而删除所有数据的时候用 TRUNCATE。 [SQL] UNION [SQL] UNION 和 UNION ALL UNION 会去重 因此比 UNION ALL快很多 SQL优化 将字段很多的表分解成多个表 对于字段较多的表，如果有些字段的使用频率很低，可以将这些字段分离出来形成新表。 增加中间表 对于需要经常 联合查询 的表，通过建立中间表以提高查询效率，具体地，将需要通过联合查询的数据插入到中间表中，然后将原来的联合查询改为对中间表的查询。 增加冗余字段 众所皆知，设计数据表时应尽量遵循范式理论的规约，尽可能的减少冗余字段，让数据库设计看起来精致、优雅。但是，表的规范化程度越高，表和表之间的关系越多，需要连接查询的情况也就越多，性能也就越差，所以合理的加入冗余字段可以提高查询速度。 partition and sharding 分库的问题 事务问题 分库分表后，就成了分布式事务。如果依赖数据库本身的分布式事务管理功能去执行事务，将付出高昂的性能代价； 如果由应用程序去协助控制，形成程序逻辑上的事务，又会造成编程方面的负担。 跨库跨表的 JOIN 问题 在执行了分库分表之后，难以避免会将原本逻辑关联性很强的数据划分到不同的表、不同的库上，这时，表的关联操作将受到限制，我们无法 JOIN 位于不同分库的表，也无法 JOIN 分表粒度不同的表，结果原本一次查询能够完成的业务，可能需要多次查询才能完成。 额外的数据管理负担和数据运算压力 额外的数据管理负担，最为常见的是数据的 定位问题 和数据的 增删改查 的重复执行问题，这些都可以通过应用程序来解决，但必然会引起额外的逻辑运算。 ","date":"2022-10-27","objectID":"/db/:0:7","series":null,"tags":null,"title":"Mysql","uri":"/db/#partition-and-sharding"},{"categories":["Tech"],"content":"SQL 语句可分为以下几类： 数据定义语言 DDL（Data Definition Language）：例如 CREATE，DROP，ALTER 等，对逻辑结构等有操作的，其中包括表结构，视图和索引。 数据查询语言 DQL（Data Query Language）：即查询操作，以 SELECT 关键字为主，各种简单查询、连接查询等都属于 DQL。 数据操纵语言 DML（Data Manipulation Language）：例如 INSERT，UPDATE，DELETE 等，对数据进行操作的。DQL 与 DML共同构建了多数初级程序员常用的 增删改查 操作，而查询是较为特殊的一种，被划分到 DQL 中。 数据控制功能 DCL（Data Control Language）：例如 GRANT，REVOKE，COMMIT，ROLLBACK 等，对数据库安全性、完整性等有操作的，可以简单的理解为权限控制等。 keys 超 键：在关系中，能唯一标识元组的属性集称为关系模式的超键。一个属性可以作为一个超键，多个属性组合在一起也可以作为一个超键。超键包含候选键和主键。 候选键：是最小超键，即没有冗余元素的超键。 主 键：数据库表中对储存数据对象予以 唯一和完整标识的数据列或属性的组合。一个数据列只能有一个主键，且主键的取值不能缺失，即不能为空值（NULL）。 外 键：在一个表中存在的另一个表的主键称此表的外键，外键可以有重复的, 可以是空值。外键是用来和其他表建立联系用的。 举例子： 现在有个表（学号，身份证号，姓名，分数） 学号和身份证号都可以唯一标识，因此只要含有这两个属性的属性集合就可以是超键，如（学号，姓名）、（身份证号，分数）等等。 而将超键的属性集合中的冗余属性剔除，则变成了候选键，这里（学号）和（身份证号）为候选键。 主键则是表设计者从候选键中选取的属性或属性集合，构成主键，可以单独选学号或身份证号作为主键，也可以选（学号，身份证号）一起作为主键。 外键则很好理解了，就是某个属性，是其他表的主键，则在这个表中就是外键。 char 与 varchar 的区别 char 表示定长字符串，长度是固定的，最多能存放的字符个数为 255，和编码无关；而 varchar 表示可变长字符串，长度是可变的，最多能存放的字符个数为 65532； 使用 char 时，如果插入数据的长度小于 char 的固定长度时，则用空格填充； 因为固定长度，char 的存取速度比 varchar 快很多，同时缺点是会占用多余空间，属于空间换时间； DROP、DELETE 与 TRUNCATE 的区别 三种都可以表示删除，其中的细微区别之处如下： DROP DELETE TRUNCATE SQL 语句类型 DDL DML DDL 回滚 不可回滚 可回滚 不可回滚 删除内容 从数据库中 删除表， 表结构还在，删除表的 表结构还在，删除表中的所有数据 所有的数据行， 全部或者一部分数据行 索引和权限也会被删除 删除速度 删除速度最快 删除速度慢，需要逐行删除 删除速度快 因此，在不再需要一张表的时候，采用 DROP；在想删除部分数据行时候，用 DELETE；在保留表而删除所有数据的时候用 TRUNCATE。 [SQL] UNION [SQL] UNION 和 UNION ALL UNION 会去重 因此比 UNION ALL快很多 SQL优化 将字段很多的表分解成多个表 对于字段较多的表，如果有些字段的使用频率很低，可以将这些字段分离出来形成新表。 增加中间表 对于需要经常 联合查询 的表，通过建立中间表以提高查询效率，具体地，将需要通过联合查询的数据插入到中间表中，然后将原来的联合查询改为对中间表的查询。 增加冗余字段 众所皆知，设计数据表时应尽量遵循范式理论的规约，尽可能的减少冗余字段，让数据库设计看起来精致、优雅。但是，表的规范化程度越高，表和表之间的关系越多，需要连接查询的情况也就越多，性能也就越差，所以合理的加入冗余字段可以提高查询速度。 partition and sharding 分库的问题 事务问题 分库分表后，就成了分布式事务。如果依赖数据库本身的分布式事务管理功能去执行事务，将付出高昂的性能代价； 如果由应用程序去协助控制，形成程序逻辑上的事务，又会造成编程方面的负担。 跨库跨表的 JOIN 问题 在执行了分库分表之后，难以避免会将原本逻辑关联性很强的数据划分到不同的表、不同的库上，这时，表的关联操作将受到限制，我们无法 JOIN 位于不同分库的表，也无法 JOIN 分表粒度不同的表，结果原本一次查询能够完成的业务，可能需要多次查询才能完成。 额外的数据管理负担和数据运算压力 额外的数据管理负担，最为常见的是数据的 定位问题 和数据的 增删改查 的重复执行问题，这些都可以通过应用程序来解决，但必然会引起额外的逻辑运算。 ","date":"2022-10-27","objectID":"/db/:0:7","series":null,"tags":null,"title":"Mysql","uri":"/db/#分库的问题"},{"categories":["Tech"],"content":"总结 dense_rank() 不跳过值排序 https://leetcode.cn/problems/rank-scores/submissions/ group_concat() 可以拼接字段进行输出并且可以排序 https://leetcode.cn/problems/group-sold-products-by-the-date/ in 可以使用多个字段进行联合查找：(x,y) select Department.name Department, Employee.name Employee, Employee.salary Salary from Department, Employee where Employee.departmentId = Department.id and (Employee.salary , Employee.departmentId) in ( select MAX(salary), departmentId from Employee group by departmentId ) ","date":"2022-10-27","objectID":"/db/:1:0","series":null,"tags":null,"title":"Mysql","uri":"/db/#总结"},{"categories":["Tech"],"content":"大部分参考菜鸟教程 有意思的数据类型： tsvector，tsquery：方便的全文搜索特性 使用to构建全文 SELECT title FROM pgweb WHERE to_tsvector('english', body) @@ to_tsquery('english', 'friend'); 创建复合类型： CREATE TYPE complex AS ( r double precision, i double precision ); CREATE TYPE inventory_item AS ( name text, supplier_id integer, price numeric ); //用复合类型创建表并且插入数据 //注意使用括号对一个类型进行定义 CREATE TABLE on_hand ( item inventory_item, count integer ); INSERT INTO on_hand VALUES (ROW('fuzzy dice', 42, 1.99), 1000); //注意必须使用括号，不然会被解析成从表中访问数据 SELECT (item).name FROM on_hand WHERE (item).price \u003e 9.99; SELECT (on_hand.item).name FROM on_hand WHERE (on_hand.item).price \u003e 9.99; ","date":"2022-10-10","objectID":"/postgresql/:0:0","series":null,"tags":null,"title":"Postgresql","uri":"/postgresql/#"},{"categories":["Tech"],"content":"什么情况下要避免使用索引？ 虽然索引的目的在于提高数据库的性能，但这里有几个情况需要避免使用索引。 使用索引时，需要考虑下列准则： 索引不应该使用在较小的表上。 索引不应该使用在有频繁的大批量的更新或插入操作的表上。 索引不应该使用在含有大量的 NULL 值的列上。 索引不应该使用在频繁操作的列上。 ","date":"2022-10-10","objectID":"/postgresql/:0:1","series":null,"tags":null,"title":"Postgresql","uri":"/postgresql/#什么情况下要避免使用索引"},{"categories":["Tech"],"content":"AOP AOP（Aspect orietend programming）面向切面编程 AOP是对OOP的一种完善，OOP引入了封装多台和继承来建立对象层次结构，但是对于水平结构则显得无能为力，例如在同一个对象的核心功能中的代码重复性（日志，权限）。 AOP使用横切的技术将与业务无关但是被业务共同调用的逻辑封装起来，降低模块耦合度，有利于可操作性和可维护性。 Aspect 切面 Joint point 连接点 在应用执行过程中能够插入切面的一个点，可以是调用前后和方法抛出异常后。 Pointcut 切点 十分钟全面理解Spring AOP - 简书 AOP使用例子： 自定义annotation @Target(ElementType.METHOD) @Retention(RetentionPolicy.RUNTIME) @Documented public @interface RefreshEsMqSender { String sender(); String msg() default \"send refresh msg to ElasticSearch\"; } 使用@interface自定义注解，可以用这个注解来作为切点 https://www.cnblogs.com/liaojie970/p/7879917.html Interface用法 Aspect： 使用AOP特性发送rabbitmq消息： @Aspect @Component public class RefreshEsMqAspect { @Resource private RabbitMqUtils rabbitMqUtils; //对于使用注解RefreshMqSender的方法进行切点注册 @Pointcut(\"@annotation(cn.dblearn.blog.common.mq.annotation.RefreshEsMqSender)\") public void pointCut() { } @Around(\"pointCut()\") public Object around(ProceedingJoinPoint point) throws Throwable { //执行方法 Object result = point.proceed(); MethodSignature signature = (MethodSignature) point.getSignature(); Method method = signature.getMethod(); RefreshEsMqSender senderAnnotation = method.getAnnotation(RefreshEsMqSender.class); // 发送刷新信息 rabbitMqUtils.send(RabbitMqConstants.REFRESH_ES_INDEX_QUEUE, senderAnnotation.sender() + \" \" + senderAnnotation.msg()); return result; } } 使用了RefreshMqSender的方法例子： @PutMapping(\"/update\") @RequiresPermissions(\"article:update\") @CacheEvict(allEntries = true) @RefreshEsMqSender(sender = \"dbblog-manage-updateArticle\") public Result updateArticle(@RequestBody ArticleDTO article){ ValidatorUtils.validateEntity(article); articleService.updateArticle(article); return Result.ok(); } ","date":"2022-09-25","objectID":"/spring/:1:0","series":null,"tags":null,"title":"Spring","uri":"/spring/#aop"},{"categories":["Tech"],"content":"OSI 7层 ① 应用层 应用层位于 OSI 参考模型的第七层，其作用是通过应用程序间的交互来完成特定的网络应用。该层协议定义了应用进程之间的交互规则，通过不同的应用层协议为不同的网络应用提供服务。例如域名系统 DNS，支持万维网应用的 HTTP 协议，电子邮件系统采用的 SMTP 协议等。在应用层交互的数据单元我们称之为报文。 ② 表示层 表示层的作用是使通信的应用程序能够解释交换数据的含义，其位于 OSI 参考模型的第六层，向上为应用层提供服务，向下接收来自会话层的服务。该层提供的服务主要包括数据压缩，数据加密以及数据描述。这使得应用程序不必担心在各台计算机中表示和存储的内部格式差异。 ③ 会话层 会话层就是负责建立、管理和终止表示层实体之间的通信会话。该层提供了数据交换的定界和同步功能，包括了建立检查点和恢复方案的方法。 ④ 传输层 传输层的主要任务是为两台主机进程之间的通信提供服务。应用程序利用该服务传送应用层报文。该服务并不针对某一特定的应用，多种应用可以使用同一个传输层服务。由于一台主机可同时运行多个线程，因此传输层有复用和分用的功能。所谓复用就是指多个应用层进程可同时使用下面传输层的服务，分用和复用相反，是传输层把收到的信息分别交付上面应用层中的相应进程。 ⑤ 网络层 两台计算机之间传送数据时其通信链路往往不止一条，所传输的信息甚至可能经过很多通信子网。网络层的主要任务就是选择合适的网间路由和交换节点，确保数据按时成功传送。在发送数据时，网络层把传输层产生的报文或用户数据报封装成分组和包向下传输到数据链路层。在网络层使用的协议是无连接的网际协议（Internet Protocol）和许多路由协议，因此我们通常把该层简单地称为 IP 层。 ⑥ 数据链路层 数据链路层通常也叫做链路层，在物理层和网络层之间。两台主机之间的数据传输，总是在一段一段的链路上传送的，这就需要使用专门的链路层协议。在两个相邻节点之间传送数据时，数据链路层将网络层交下来的 IP 数据报组装成帧，在两个相邻节点间的链路上传送帧。每一帧包括数据和必要的控制信息。通过控制信息我们可以知道一个帧的起止比特位置，此外，也能使接收端检测出所收到的帧有无差错，如果发现差错，数据链路层能够简单的丢弃掉这个帧，以避免继续占用网络资源。 ⑦ 物理层 作为 OSI 参考模型中最低的一层，物理层的作用是实现计算机节点之间比特流的透明传送，尽可能屏蔽掉具体传输介质和物理设备的差异。使其上面的数据链路层不必考虑网络的具体传输介质是什么。该层的主要任务是确定与传输媒体的接口的一些特性（机械特性、电气特性、功能特性，过程特性）。 OSI 七层模型在提出时的出发点是基于标准化的考虑，而没有考虑到具体的市场需求，使得该模型结构复杂，部分功能冗余，因而完全实现 OSI 参考模型的系统不多。而 TCP/IP 参考模型直接面向市场需求，实现起来也比较容易，因此在一经提出便得到了广泛的应用。基于 TCP/IP 的参考模型将协议分成四个层次，如上图所示，它们分别是：网络访问层、网际互联层、传输层、和应用层。 ","date":"2022-09-18","objectID":"/computer-network/:0:1","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#osi-7层"},{"categories":["Tech"],"content":"OSI 7层 ① 应用层 应用层位于 OSI 参考模型的第七层，其作用是通过应用程序间的交互来完成特定的网络应用。该层协议定义了应用进程之间的交互规则，通过不同的应用层协议为不同的网络应用提供服务。例如域名系统 DNS，支持万维网应用的 HTTP 协议，电子邮件系统采用的 SMTP 协议等。在应用层交互的数据单元我们称之为报文。 ② 表示层 表示层的作用是使通信的应用程序能够解释交换数据的含义，其位于 OSI 参考模型的第六层，向上为应用层提供服务，向下接收来自会话层的服务。该层提供的服务主要包括数据压缩，数据加密以及数据描述。这使得应用程序不必担心在各台计算机中表示和存储的内部格式差异。 ③ 会话层 会话层就是负责建立、管理和终止表示层实体之间的通信会话。该层提供了数据交换的定界和同步功能，包括了建立检查点和恢复方案的方法。 ④ 传输层 传输层的主要任务是为两台主机进程之间的通信提供服务。应用程序利用该服务传送应用层报文。该服务并不针对某一特定的应用，多种应用可以使用同一个传输层服务。由于一台主机可同时运行多个线程，因此传输层有复用和分用的功能。所谓复用就是指多个应用层进程可同时使用下面传输层的服务，分用和复用相反，是传输层把收到的信息分别交付上面应用层中的相应进程。 ⑤ 网络层 两台计算机之间传送数据时其通信链路往往不止一条，所传输的信息甚至可能经过很多通信子网。网络层的主要任务就是选择合适的网间路由和交换节点，确保数据按时成功传送。在发送数据时，网络层把传输层产生的报文或用户数据报封装成分组和包向下传输到数据链路层。在网络层使用的协议是无连接的网际协议（Internet Protocol）和许多路由协议，因此我们通常把该层简单地称为 IP 层。 ⑥ 数据链路层 数据链路层通常也叫做链路层，在物理层和网络层之间。两台主机之间的数据传输，总是在一段一段的链路上传送的，这就需要使用专门的链路层协议。在两个相邻节点之间传送数据时，数据链路层将网络层交下来的 IP 数据报组装成帧，在两个相邻节点间的链路上传送帧。每一帧包括数据和必要的控制信息。通过控制信息我们可以知道一个帧的起止比特位置，此外，也能使接收端检测出所收到的帧有无差错，如果发现差错，数据链路层能够简单的丢弃掉这个帧，以避免继续占用网络资源。 ⑦ 物理层 作为 OSI 参考模型中最低的一层，物理层的作用是实现计算机节点之间比特流的透明传送，尽可能屏蔽掉具体传输介质和物理设备的差异。使其上面的数据链路层不必考虑网络的具体传输介质是什么。该层的主要任务是确定与传输媒体的接口的一些特性（机械特性、电气特性、功能特性，过程特性）。 OSI 七层模型在提出时的出发点是基于标准化的考虑，而没有考虑到具体的市场需求，使得该模型结构复杂，部分功能冗余，因而完全实现 OSI 参考模型的系统不多。而 TCP/IP 参考模型直接面向市场需求，实现起来也比较容易，因此在一经提出便得到了广泛的应用。基于 TCP/IP 的参考模型将协议分成四个层次，如上图所示，它们分别是：网络访问层、网际互联层、传输层、和应用层。 ","date":"2022-09-18","objectID":"/computer-network/:0:1","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#-应用层"},{"categories":["Tech"],"content":"OSI 7层 ① 应用层 应用层位于 OSI 参考模型的第七层，其作用是通过应用程序间的交互来完成特定的网络应用。该层协议定义了应用进程之间的交互规则，通过不同的应用层协议为不同的网络应用提供服务。例如域名系统 DNS，支持万维网应用的 HTTP 协议，电子邮件系统采用的 SMTP 协议等。在应用层交互的数据单元我们称之为报文。 ② 表示层 表示层的作用是使通信的应用程序能够解释交换数据的含义，其位于 OSI 参考模型的第六层，向上为应用层提供服务，向下接收来自会话层的服务。该层提供的服务主要包括数据压缩，数据加密以及数据描述。这使得应用程序不必担心在各台计算机中表示和存储的内部格式差异。 ③ 会话层 会话层就是负责建立、管理和终止表示层实体之间的通信会话。该层提供了数据交换的定界和同步功能，包括了建立检查点和恢复方案的方法。 ④ 传输层 传输层的主要任务是为两台主机进程之间的通信提供服务。应用程序利用该服务传送应用层报文。该服务并不针对某一特定的应用，多种应用可以使用同一个传输层服务。由于一台主机可同时运行多个线程，因此传输层有复用和分用的功能。所谓复用就是指多个应用层进程可同时使用下面传输层的服务，分用和复用相反，是传输层把收到的信息分别交付上面应用层中的相应进程。 ⑤ 网络层 两台计算机之间传送数据时其通信链路往往不止一条，所传输的信息甚至可能经过很多通信子网。网络层的主要任务就是选择合适的网间路由和交换节点，确保数据按时成功传送。在发送数据时，网络层把传输层产生的报文或用户数据报封装成分组和包向下传输到数据链路层。在网络层使用的协议是无连接的网际协议（Internet Protocol）和许多路由协议，因此我们通常把该层简单地称为 IP 层。 ⑥ 数据链路层 数据链路层通常也叫做链路层，在物理层和网络层之间。两台主机之间的数据传输，总是在一段一段的链路上传送的，这就需要使用专门的链路层协议。在两个相邻节点之间传送数据时，数据链路层将网络层交下来的 IP 数据报组装成帧，在两个相邻节点间的链路上传送帧。每一帧包括数据和必要的控制信息。通过控制信息我们可以知道一个帧的起止比特位置，此外，也能使接收端检测出所收到的帧有无差错，如果发现差错，数据链路层能够简单的丢弃掉这个帧，以避免继续占用网络资源。 ⑦ 物理层 作为 OSI 参考模型中最低的一层，物理层的作用是实现计算机节点之间比特流的透明传送，尽可能屏蔽掉具体传输介质和物理设备的差异。使其上面的数据链路层不必考虑网络的具体传输介质是什么。该层的主要任务是确定与传输媒体的接口的一些特性（机械特性、电气特性、功能特性，过程特性）。 OSI 七层模型在提出时的出发点是基于标准化的考虑，而没有考虑到具体的市场需求，使得该模型结构复杂，部分功能冗余，因而完全实现 OSI 参考模型的系统不多。而 TCP/IP 参考模型直接面向市场需求，实现起来也比较容易，因此在一经提出便得到了广泛的应用。基于 TCP/IP 的参考模型将协议分成四个层次，如上图所示，它们分别是：网络访问层、网际互联层、传输层、和应用层。 ","date":"2022-09-18","objectID":"/computer-network/:0:1","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#-表示层"},{"categories":["Tech"],"content":"OSI 7层 ① 应用层 应用层位于 OSI 参考模型的第七层，其作用是通过应用程序间的交互来完成特定的网络应用。该层协议定义了应用进程之间的交互规则，通过不同的应用层协议为不同的网络应用提供服务。例如域名系统 DNS，支持万维网应用的 HTTP 协议，电子邮件系统采用的 SMTP 协议等。在应用层交互的数据单元我们称之为报文。 ② 表示层 表示层的作用是使通信的应用程序能够解释交换数据的含义，其位于 OSI 参考模型的第六层，向上为应用层提供服务，向下接收来自会话层的服务。该层提供的服务主要包括数据压缩，数据加密以及数据描述。这使得应用程序不必担心在各台计算机中表示和存储的内部格式差异。 ③ 会话层 会话层就是负责建立、管理和终止表示层实体之间的通信会话。该层提供了数据交换的定界和同步功能，包括了建立检查点和恢复方案的方法。 ④ 传输层 传输层的主要任务是为两台主机进程之间的通信提供服务。应用程序利用该服务传送应用层报文。该服务并不针对某一特定的应用，多种应用可以使用同一个传输层服务。由于一台主机可同时运行多个线程，因此传输层有复用和分用的功能。所谓复用就是指多个应用层进程可同时使用下面传输层的服务，分用和复用相反，是传输层把收到的信息分别交付上面应用层中的相应进程。 ⑤ 网络层 两台计算机之间传送数据时其通信链路往往不止一条，所传输的信息甚至可能经过很多通信子网。网络层的主要任务就是选择合适的网间路由和交换节点，确保数据按时成功传送。在发送数据时，网络层把传输层产生的报文或用户数据报封装成分组和包向下传输到数据链路层。在网络层使用的协议是无连接的网际协议（Internet Protocol）和许多路由协议，因此我们通常把该层简单地称为 IP 层。 ⑥ 数据链路层 数据链路层通常也叫做链路层，在物理层和网络层之间。两台主机之间的数据传输，总是在一段一段的链路上传送的，这就需要使用专门的链路层协议。在两个相邻节点之间传送数据时，数据链路层将网络层交下来的 IP 数据报组装成帧，在两个相邻节点间的链路上传送帧。每一帧包括数据和必要的控制信息。通过控制信息我们可以知道一个帧的起止比特位置，此外，也能使接收端检测出所收到的帧有无差错，如果发现差错，数据链路层能够简单的丢弃掉这个帧，以避免继续占用网络资源。 ⑦ 物理层 作为 OSI 参考模型中最低的一层，物理层的作用是实现计算机节点之间比特流的透明传送，尽可能屏蔽掉具体传输介质和物理设备的差异。使其上面的数据链路层不必考虑网络的具体传输介质是什么。该层的主要任务是确定与传输媒体的接口的一些特性（机械特性、电气特性、功能特性，过程特性）。 OSI 七层模型在提出时的出发点是基于标准化的考虑，而没有考虑到具体的市场需求，使得该模型结构复杂，部分功能冗余，因而完全实现 OSI 参考模型的系统不多。而 TCP/IP 参考模型直接面向市场需求，实现起来也比较容易，因此在一经提出便得到了广泛的应用。基于 TCP/IP 的参考模型将协议分成四个层次，如上图所示，它们分别是：网络访问层、网际互联层、传输层、和应用层。 ","date":"2022-09-18","objectID":"/computer-network/:0:1","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#-会话层"},{"categories":["Tech"],"content":"OSI 7层 ① 应用层 应用层位于 OSI 参考模型的第七层，其作用是通过应用程序间的交互来完成特定的网络应用。该层协议定义了应用进程之间的交互规则，通过不同的应用层协议为不同的网络应用提供服务。例如域名系统 DNS，支持万维网应用的 HTTP 协议，电子邮件系统采用的 SMTP 协议等。在应用层交互的数据单元我们称之为报文。 ② 表示层 表示层的作用是使通信的应用程序能够解释交换数据的含义，其位于 OSI 参考模型的第六层，向上为应用层提供服务，向下接收来自会话层的服务。该层提供的服务主要包括数据压缩，数据加密以及数据描述。这使得应用程序不必担心在各台计算机中表示和存储的内部格式差异。 ③ 会话层 会话层就是负责建立、管理和终止表示层实体之间的通信会话。该层提供了数据交换的定界和同步功能，包括了建立检查点和恢复方案的方法。 ④ 传输层 传输层的主要任务是为两台主机进程之间的通信提供服务。应用程序利用该服务传送应用层报文。该服务并不针对某一特定的应用，多种应用可以使用同一个传输层服务。由于一台主机可同时运行多个线程，因此传输层有复用和分用的功能。所谓复用就是指多个应用层进程可同时使用下面传输层的服务，分用和复用相反，是传输层把收到的信息分别交付上面应用层中的相应进程。 ⑤ 网络层 两台计算机之间传送数据时其通信链路往往不止一条，所传输的信息甚至可能经过很多通信子网。网络层的主要任务就是选择合适的网间路由和交换节点，确保数据按时成功传送。在发送数据时，网络层把传输层产生的报文或用户数据报封装成分组和包向下传输到数据链路层。在网络层使用的协议是无连接的网际协议（Internet Protocol）和许多路由协议，因此我们通常把该层简单地称为 IP 层。 ⑥ 数据链路层 数据链路层通常也叫做链路层，在物理层和网络层之间。两台主机之间的数据传输，总是在一段一段的链路上传送的，这就需要使用专门的链路层协议。在两个相邻节点之间传送数据时，数据链路层将网络层交下来的 IP 数据报组装成帧，在两个相邻节点间的链路上传送帧。每一帧包括数据和必要的控制信息。通过控制信息我们可以知道一个帧的起止比特位置，此外，也能使接收端检测出所收到的帧有无差错，如果发现差错，数据链路层能够简单的丢弃掉这个帧，以避免继续占用网络资源。 ⑦ 物理层 作为 OSI 参考模型中最低的一层，物理层的作用是实现计算机节点之间比特流的透明传送，尽可能屏蔽掉具体传输介质和物理设备的差异。使其上面的数据链路层不必考虑网络的具体传输介质是什么。该层的主要任务是确定与传输媒体的接口的一些特性（机械特性、电气特性、功能特性，过程特性）。 OSI 七层模型在提出时的出发点是基于标准化的考虑，而没有考虑到具体的市场需求，使得该模型结构复杂，部分功能冗余，因而完全实现 OSI 参考模型的系统不多。而 TCP/IP 参考模型直接面向市场需求，实现起来也比较容易，因此在一经提出便得到了广泛的应用。基于 TCP/IP 的参考模型将协议分成四个层次，如上图所示，它们分别是：网络访问层、网际互联层、传输层、和应用层。 ","date":"2022-09-18","objectID":"/computer-network/:0:1","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#-传输层"},{"categories":["Tech"],"content":"OSI 7层 ① 应用层 应用层位于 OSI 参考模型的第七层，其作用是通过应用程序间的交互来完成特定的网络应用。该层协议定义了应用进程之间的交互规则，通过不同的应用层协议为不同的网络应用提供服务。例如域名系统 DNS，支持万维网应用的 HTTP 协议，电子邮件系统采用的 SMTP 协议等。在应用层交互的数据单元我们称之为报文。 ② 表示层 表示层的作用是使通信的应用程序能够解释交换数据的含义，其位于 OSI 参考模型的第六层，向上为应用层提供服务，向下接收来自会话层的服务。该层提供的服务主要包括数据压缩，数据加密以及数据描述。这使得应用程序不必担心在各台计算机中表示和存储的内部格式差异。 ③ 会话层 会话层就是负责建立、管理和终止表示层实体之间的通信会话。该层提供了数据交换的定界和同步功能，包括了建立检查点和恢复方案的方法。 ④ 传输层 传输层的主要任务是为两台主机进程之间的通信提供服务。应用程序利用该服务传送应用层报文。该服务并不针对某一特定的应用，多种应用可以使用同一个传输层服务。由于一台主机可同时运行多个线程，因此传输层有复用和分用的功能。所谓复用就是指多个应用层进程可同时使用下面传输层的服务，分用和复用相反，是传输层把收到的信息分别交付上面应用层中的相应进程。 ⑤ 网络层 两台计算机之间传送数据时其通信链路往往不止一条，所传输的信息甚至可能经过很多通信子网。网络层的主要任务就是选择合适的网间路由和交换节点，确保数据按时成功传送。在发送数据时，网络层把传输层产生的报文或用户数据报封装成分组和包向下传输到数据链路层。在网络层使用的协议是无连接的网际协议（Internet Protocol）和许多路由协议，因此我们通常把该层简单地称为 IP 层。 ⑥ 数据链路层 数据链路层通常也叫做链路层，在物理层和网络层之间。两台主机之间的数据传输，总是在一段一段的链路上传送的，这就需要使用专门的链路层协议。在两个相邻节点之间传送数据时，数据链路层将网络层交下来的 IP 数据报组装成帧，在两个相邻节点间的链路上传送帧。每一帧包括数据和必要的控制信息。通过控制信息我们可以知道一个帧的起止比特位置，此外，也能使接收端检测出所收到的帧有无差错，如果发现差错，数据链路层能够简单的丢弃掉这个帧，以避免继续占用网络资源。 ⑦ 物理层 作为 OSI 参考模型中最低的一层，物理层的作用是实现计算机节点之间比特流的透明传送，尽可能屏蔽掉具体传输介质和物理设备的差异。使其上面的数据链路层不必考虑网络的具体传输介质是什么。该层的主要任务是确定与传输媒体的接口的一些特性（机械特性、电气特性、功能特性，过程特性）。 OSI 七层模型在提出时的出发点是基于标准化的考虑，而没有考虑到具体的市场需求，使得该模型结构复杂，部分功能冗余，因而完全实现 OSI 参考模型的系统不多。而 TCP/IP 参考模型直接面向市场需求，实现起来也比较容易，因此在一经提出便得到了广泛的应用。基于 TCP/IP 的参考模型将协议分成四个层次，如上图所示，它们分别是：网络访问层、网际互联层、传输层、和应用层。 ","date":"2022-09-18","objectID":"/computer-network/:0:1","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#-网络层"},{"categories":["Tech"],"content":"OSI 7层 ① 应用层 应用层位于 OSI 参考模型的第七层，其作用是通过应用程序间的交互来完成特定的网络应用。该层协议定义了应用进程之间的交互规则，通过不同的应用层协议为不同的网络应用提供服务。例如域名系统 DNS，支持万维网应用的 HTTP 协议，电子邮件系统采用的 SMTP 协议等。在应用层交互的数据单元我们称之为报文。 ② 表示层 表示层的作用是使通信的应用程序能够解释交换数据的含义，其位于 OSI 参考模型的第六层，向上为应用层提供服务，向下接收来自会话层的服务。该层提供的服务主要包括数据压缩，数据加密以及数据描述。这使得应用程序不必担心在各台计算机中表示和存储的内部格式差异。 ③ 会话层 会话层就是负责建立、管理和终止表示层实体之间的通信会话。该层提供了数据交换的定界和同步功能，包括了建立检查点和恢复方案的方法。 ④ 传输层 传输层的主要任务是为两台主机进程之间的通信提供服务。应用程序利用该服务传送应用层报文。该服务并不针对某一特定的应用，多种应用可以使用同一个传输层服务。由于一台主机可同时运行多个线程，因此传输层有复用和分用的功能。所谓复用就是指多个应用层进程可同时使用下面传输层的服务，分用和复用相反，是传输层把收到的信息分别交付上面应用层中的相应进程。 ⑤ 网络层 两台计算机之间传送数据时其通信链路往往不止一条，所传输的信息甚至可能经过很多通信子网。网络层的主要任务就是选择合适的网间路由和交换节点，确保数据按时成功传送。在发送数据时，网络层把传输层产生的报文或用户数据报封装成分组和包向下传输到数据链路层。在网络层使用的协议是无连接的网际协议（Internet Protocol）和许多路由协议，因此我们通常把该层简单地称为 IP 层。 ⑥ 数据链路层 数据链路层通常也叫做链路层，在物理层和网络层之间。两台主机之间的数据传输，总是在一段一段的链路上传送的，这就需要使用专门的链路层协议。在两个相邻节点之间传送数据时，数据链路层将网络层交下来的 IP 数据报组装成帧，在两个相邻节点间的链路上传送帧。每一帧包括数据和必要的控制信息。通过控制信息我们可以知道一个帧的起止比特位置，此外，也能使接收端检测出所收到的帧有无差错，如果发现差错，数据链路层能够简单的丢弃掉这个帧，以避免继续占用网络资源。 ⑦ 物理层 作为 OSI 参考模型中最低的一层，物理层的作用是实现计算机节点之间比特流的透明传送，尽可能屏蔽掉具体传输介质和物理设备的差异。使其上面的数据链路层不必考虑网络的具体传输介质是什么。该层的主要任务是确定与传输媒体的接口的一些特性（机械特性、电气特性、功能特性，过程特性）。 OSI 七层模型在提出时的出发点是基于标准化的考虑，而没有考虑到具体的市场需求，使得该模型结构复杂，部分功能冗余，因而完全实现 OSI 参考模型的系统不多。而 TCP/IP 参考模型直接面向市场需求，实现起来也比较容易，因此在一经提出便得到了广泛的应用。基于 TCP/IP 的参考模型将协议分成四个层次，如上图所示，它们分别是：网络访问层、网际互联层、传输层、和应用层。 ","date":"2022-09-18","objectID":"/computer-network/:0:1","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#-数据链路层"},{"categories":["Tech"],"content":"OSI 7层 ① 应用层 应用层位于 OSI 参考模型的第七层，其作用是通过应用程序间的交互来完成特定的网络应用。该层协议定义了应用进程之间的交互规则，通过不同的应用层协议为不同的网络应用提供服务。例如域名系统 DNS，支持万维网应用的 HTTP 协议，电子邮件系统采用的 SMTP 协议等。在应用层交互的数据单元我们称之为报文。 ② 表示层 表示层的作用是使通信的应用程序能够解释交换数据的含义，其位于 OSI 参考模型的第六层，向上为应用层提供服务，向下接收来自会话层的服务。该层提供的服务主要包括数据压缩，数据加密以及数据描述。这使得应用程序不必担心在各台计算机中表示和存储的内部格式差异。 ③ 会话层 会话层就是负责建立、管理和终止表示层实体之间的通信会话。该层提供了数据交换的定界和同步功能，包括了建立检查点和恢复方案的方法。 ④ 传输层 传输层的主要任务是为两台主机进程之间的通信提供服务。应用程序利用该服务传送应用层报文。该服务并不针对某一特定的应用，多种应用可以使用同一个传输层服务。由于一台主机可同时运行多个线程，因此传输层有复用和分用的功能。所谓复用就是指多个应用层进程可同时使用下面传输层的服务，分用和复用相反，是传输层把收到的信息分别交付上面应用层中的相应进程。 ⑤ 网络层 两台计算机之间传送数据时其通信链路往往不止一条，所传输的信息甚至可能经过很多通信子网。网络层的主要任务就是选择合适的网间路由和交换节点，确保数据按时成功传送。在发送数据时，网络层把传输层产生的报文或用户数据报封装成分组和包向下传输到数据链路层。在网络层使用的协议是无连接的网际协议（Internet Protocol）和许多路由协议，因此我们通常把该层简单地称为 IP 层。 ⑥ 数据链路层 数据链路层通常也叫做链路层，在物理层和网络层之间。两台主机之间的数据传输，总是在一段一段的链路上传送的，这就需要使用专门的链路层协议。在两个相邻节点之间传送数据时，数据链路层将网络层交下来的 IP 数据报组装成帧，在两个相邻节点间的链路上传送帧。每一帧包括数据和必要的控制信息。通过控制信息我们可以知道一个帧的起止比特位置，此外，也能使接收端检测出所收到的帧有无差错，如果发现差错，数据链路层能够简单的丢弃掉这个帧，以避免继续占用网络资源。 ⑦ 物理层 作为 OSI 参考模型中最低的一层，物理层的作用是实现计算机节点之间比特流的透明传送，尽可能屏蔽掉具体传输介质和物理设备的差异。使其上面的数据链路层不必考虑网络的具体传输介质是什么。该层的主要任务是确定与传输媒体的接口的一些特性（机械特性、电气特性、功能特性，过程特性）。 OSI 七层模型在提出时的出发点是基于标准化的考虑，而没有考虑到具体的市场需求，使得该模型结构复杂，部分功能冗余，因而完全实现 OSI 参考模型的系统不多。而 TCP/IP 参考模型直接面向市场需求，实现起来也比较容易，因此在一经提出便得到了广泛的应用。基于 TCP/IP 的参考模型将协议分成四个层次，如上图所示，它们分别是：网络访问层、网际互联层、传输层、和应用层。 ","date":"2022-09-18","objectID":"/computer-network/:0:1","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#-物理层"},{"categories":["Tech"],"content":"TCP/IP 4层 ① 应用层 TCP/IP 模型将 OSI 参考模型中的会话层、表示层和应用层的功能合并到一个应用层实现，通过不同的应用层协议为不同的应用提供服务。例如：FTP、Telnet、DNS、SMTP 等。 ② 传输层 该层对应于 OSI 参考模型的传输层，为上层实体提供源端到对端主机的通信功能。传输层定义了两个主要协议：传输控制协议（TCP）和用户数据报协议（UDP）。其中面向连接的 TCP 协议保证了数据的传输可靠性，面向无连接的 UDP 协议能够实现数据包简单、快速地传输。 ③ 网际互联层 网际互联层对应 OSI 参考模型的网络层，主要负责相同或不同网络中计算机之间的通信。在网际互联层， IP 协议提供的是一个不可靠、无连接的数据报传递服务。该协议实现两个基本功能：寻址和分段。根据数据报报头中的目的地址将数据传送到目的地址，在这个过程中 IP 负责选择传送路线。除了 IP 协议外，该层另外两个主要协议是互联网组管理协议（IGMP）和互联网控制报文协议（ICMP）。 ④ 网络接入层 网络接入层的功能对应于 OSI 参考模型中的物理层和数据链路层，它负责监视数据在主机和网络之间的交换。事实上，TCP/IP 并未真正描述这一层的实现，而由参与互连的各网络使用自己的物理层和数据链路层协议，然后与 TCP/IP 的网络接入层进行连接，因此具体的实现方法将随着网络类型的不同而有所差异。 ","date":"2022-09-18","objectID":"/computer-network/:0:2","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#tcpip-4层"},{"categories":["Tech"],"content":"TCP/IP 4层 ① 应用层 TCP/IP 模型将 OSI 参考模型中的会话层、表示层和应用层的功能合并到一个应用层实现，通过不同的应用层协议为不同的应用提供服务。例如：FTP、Telnet、DNS、SMTP 等。 ② 传输层 该层对应于 OSI 参考模型的传输层，为上层实体提供源端到对端主机的通信功能。传输层定义了两个主要协议：传输控制协议（TCP）和用户数据报协议（UDP）。其中面向连接的 TCP 协议保证了数据的传输可靠性，面向无连接的 UDP 协议能够实现数据包简单、快速地传输。 ③ 网际互联层 网际互联层对应 OSI 参考模型的网络层，主要负责相同或不同网络中计算机之间的通信。在网际互联层， IP 协议提供的是一个不可靠、无连接的数据报传递服务。该协议实现两个基本功能：寻址和分段。根据数据报报头中的目的地址将数据传送到目的地址，在这个过程中 IP 负责选择传送路线。除了 IP 协议外，该层另外两个主要协议是互联网组管理协议（IGMP）和互联网控制报文协议（ICMP）。 ④ 网络接入层 网络接入层的功能对应于 OSI 参考模型中的物理层和数据链路层，它负责监视数据在主机和网络之间的交换。事实上，TCP/IP 并未真正描述这一层的实现，而由参与互连的各网络使用自己的物理层和数据链路层协议，然后与 TCP/IP 的网络接入层进行连接，因此具体的实现方法将随着网络类型的不同而有所差异。 ","date":"2022-09-18","objectID":"/computer-network/:0:2","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#-应用层-1"},{"categories":["Tech"],"content":"TCP/IP 4层 ① 应用层 TCP/IP 模型将 OSI 参考模型中的会话层、表示层和应用层的功能合并到一个应用层实现，通过不同的应用层协议为不同的应用提供服务。例如：FTP、Telnet、DNS、SMTP 等。 ② 传输层 该层对应于 OSI 参考模型的传输层，为上层实体提供源端到对端主机的通信功能。传输层定义了两个主要协议：传输控制协议（TCP）和用户数据报协议（UDP）。其中面向连接的 TCP 协议保证了数据的传输可靠性，面向无连接的 UDP 协议能够实现数据包简单、快速地传输。 ③ 网际互联层 网际互联层对应 OSI 参考模型的网络层，主要负责相同或不同网络中计算机之间的通信。在网际互联层， IP 协议提供的是一个不可靠、无连接的数据报传递服务。该协议实现两个基本功能：寻址和分段。根据数据报报头中的目的地址将数据传送到目的地址，在这个过程中 IP 负责选择传送路线。除了 IP 协议外，该层另外两个主要协议是互联网组管理协议（IGMP）和互联网控制报文协议（ICMP）。 ④ 网络接入层 网络接入层的功能对应于 OSI 参考模型中的物理层和数据链路层，它负责监视数据在主机和网络之间的交换。事实上，TCP/IP 并未真正描述这一层的实现，而由参与互连的各网络使用自己的物理层和数据链路层协议，然后与 TCP/IP 的网络接入层进行连接，因此具体的实现方法将随着网络类型的不同而有所差异。 ","date":"2022-09-18","objectID":"/computer-network/:0:2","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#-传输层-1"},{"categories":["Tech"],"content":"TCP/IP 4层 ① 应用层 TCP/IP 模型将 OSI 参考模型中的会话层、表示层和应用层的功能合并到一个应用层实现，通过不同的应用层协议为不同的应用提供服务。例如：FTP、Telnet、DNS、SMTP 等。 ② 传输层 该层对应于 OSI 参考模型的传输层，为上层实体提供源端到对端主机的通信功能。传输层定义了两个主要协议：传输控制协议（TCP）和用户数据报协议（UDP）。其中面向连接的 TCP 协议保证了数据的传输可靠性，面向无连接的 UDP 协议能够实现数据包简单、快速地传输。 ③ 网际互联层 网际互联层对应 OSI 参考模型的网络层，主要负责相同或不同网络中计算机之间的通信。在网际互联层， IP 协议提供的是一个不可靠、无连接的数据报传递服务。该协议实现两个基本功能：寻址和分段。根据数据报报头中的目的地址将数据传送到目的地址，在这个过程中 IP 负责选择传送路线。除了 IP 协议外，该层另外两个主要协议是互联网组管理协议（IGMP）和互联网控制报文协议（ICMP）。 ④ 网络接入层 网络接入层的功能对应于 OSI 参考模型中的物理层和数据链路层，它负责监视数据在主机和网络之间的交换。事实上，TCP/IP 并未真正描述这一层的实现，而由参与互连的各网络使用自己的物理层和数据链路层协议，然后与 TCP/IP 的网络接入层进行连接，因此具体的实现方法将随着网络类型的不同而有所差异。 ","date":"2022-09-18","objectID":"/computer-network/:0:2","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#-网际互联层"},{"categories":["Tech"],"content":"TCP/IP 4层 ① 应用层 TCP/IP 模型将 OSI 参考模型中的会话层、表示层和应用层的功能合并到一个应用层实现，通过不同的应用层协议为不同的应用提供服务。例如：FTP、Telnet、DNS、SMTP 等。 ② 传输层 该层对应于 OSI 参考模型的传输层，为上层实体提供源端到对端主机的通信功能。传输层定义了两个主要协议：传输控制协议（TCP）和用户数据报协议（UDP）。其中面向连接的 TCP 协议保证了数据的传输可靠性，面向无连接的 UDP 协议能够实现数据包简单、快速地传输。 ③ 网际互联层 网际互联层对应 OSI 参考模型的网络层，主要负责相同或不同网络中计算机之间的通信。在网际互联层， IP 协议提供的是一个不可靠、无连接的数据报传递服务。该协议实现两个基本功能：寻址和分段。根据数据报报头中的目的地址将数据传送到目的地址，在这个过程中 IP 负责选择传送路线。除了 IP 协议外，该层另外两个主要协议是互联网组管理协议（IGMP）和互联网控制报文协议（ICMP）。 ④ 网络接入层 网络接入层的功能对应于 OSI 参考模型中的物理层和数据链路层，它负责监视数据在主机和网络之间的交换。事实上，TCP/IP 并未真正描述这一层的实现，而由参与互连的各网络使用自己的物理层和数据链路层协议，然后与 TCP/IP 的网络接入层进行连接，因此具体的实现方法将随着网络类型的不同而有所差异。 ","date":"2022-09-18","objectID":"/computer-network/:0:2","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#-网络接入层"},{"categories":["Tech"],"content":"二者的异同 相同点 ① OSI 参考模型与 TCP/IP 参考模型都采用了层次结构。 ② 都能够提供面向连接和无连接两种通信服务机制。 不同点 ① OSI 采用的七层模型； TCP/IP 是四层结构。 ② TCP/IP 参考模型没有对网络接口层进行细分，只是一些概念性的描述； OSI 参考模型对服务和协议做了明确的区分。 ③ OSI 先有模型，后有协议规范，适合于描述各种网络；TCP/IP 是先有协议集然后建立模型，不适用于非 TCP/IP 网络。 ④ TCP/IP 一开始就提出面向连接和无连接服务，而 OSI 一开始只强调面向连接服务，直到很晚才开始制定无连接的服务标准。 ⑤ OSI 参考模型虽然被看好，但将网络划分为七层，实现起来较困难；相反，TCP/IP 参考模型虽然有许多不尽人意的地方，但作为一种简化的分层结构还是比较成功的。 应用数据报→传输层报文段→ip成组→链路层成帧→物理层比特流； HTTP请求头的相关知识： https://www.51cto.com/article/608197.html 非keep-alive: 重新请求重新建立TCP连接。但每一个这样的连接，客户机和服务器都要分配 TCP 的缓冲区和变量，这给服务器带来的严重的负担。 Keep-alive: HTTP1.1默认持久连接，同一客户机可以连续请求通过相同的连接进行传送，一台服务器多个web页面也可通过单个TCP连接传送给同一个客户机。但长时间保持TCP连接会导致系统资源被无效占用。 ","date":"2022-09-18","objectID":"/computer-network/:0:3","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#二者的异同"},{"categories":["Tech"],"content":"二者的异同 相同点 ① OSI 参考模型与 TCP/IP 参考模型都采用了层次结构。 ② 都能够提供面向连接和无连接两种通信服务机制。 不同点 ① OSI 采用的七层模型； TCP/IP 是四层结构。 ② TCP/IP 参考模型没有对网络接口层进行细分，只是一些概念性的描述； OSI 参考模型对服务和协议做了明确的区分。 ③ OSI 先有模型，后有协议规范，适合于描述各种网络；TCP/IP 是先有协议集然后建立模型，不适用于非 TCP/IP 网络。 ④ TCP/IP 一开始就提出面向连接和无连接服务，而 OSI 一开始只强调面向连接服务，直到很晚才开始制定无连接的服务标准。 ⑤ OSI 参考模型虽然被看好，但将网络划分为七层，实现起来较困难；相反，TCP/IP 参考模型虽然有许多不尽人意的地方，但作为一种简化的分层结构还是比较成功的。 应用数据报→传输层报文段→ip成组→链路层成帧→物理层比特流； HTTP请求头的相关知识： https://www.51cto.com/article/608197.html 非keep-alive: 重新请求重新建立TCP连接。但每一个这样的连接，客户机和服务器都要分配 TCP 的缓冲区和变量，这给服务器带来的严重的负担。 Keep-alive: HTTP1.1默认持久连接，同一客户机可以连续请求通过相同的连接进行传送，一台服务器多个web页面也可通过单个TCP连接传送给同一个客户机。但长时间保持TCP连接会导致系统资源被无效占用。 ","date":"2022-09-18","objectID":"/computer-network/:0:3","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#相同点"},{"categories":["Tech"],"content":"二者的异同 相同点 ① OSI 参考模型与 TCP/IP 参考模型都采用了层次结构。 ② 都能够提供面向连接和无连接两种通信服务机制。 不同点 ① OSI 采用的七层模型； TCP/IP 是四层结构。 ② TCP/IP 参考模型没有对网络接口层进行细分，只是一些概念性的描述； OSI 参考模型对服务和协议做了明确的区分。 ③ OSI 先有模型，后有协议规范，适合于描述各种网络；TCP/IP 是先有协议集然后建立模型，不适用于非 TCP/IP 网络。 ④ TCP/IP 一开始就提出面向连接和无连接服务，而 OSI 一开始只强调面向连接服务，直到很晚才开始制定无连接的服务标准。 ⑤ OSI 参考模型虽然被看好，但将网络划分为七层，实现起来较困难；相反，TCP/IP 参考模型虽然有许多不尽人意的地方，但作为一种简化的分层结构还是比较成功的。 应用数据报→传输层报文段→ip成组→链路层成帧→物理层比特流； HTTP请求头的相关知识： https://www.51cto.com/article/608197.html 非keep-alive: 重新请求重新建立TCP连接。但每一个这样的连接，客户机和服务器都要分配 TCP 的缓冲区和变量，这给服务器带来的严重的负担。 Keep-alive: HTTP1.1默认持久连接，同一客户机可以连续请求通过相同的连接进行传送，一台服务器多个web页面也可通过单个TCP连接传送给同一个客户机。但长时间保持TCP连接会导致系统资源被无效占用。 ","date":"2022-09-18","objectID":"/computer-network/:0:3","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#不同点"},{"categories":["Tech"],"content":"应用层 HTTP 头部本质上是一个传递额外重要信息的键值对。主要分为：通用头部，请求头部，响应头部和实体头部。 在HTTP1.0中，每次发起http请求都要创建一个新的TCP连接，请求完成后直接断开，开销很大。 ","date":"2022-09-18","objectID":"/computer-network/:1:0","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#应用层"},{"categories":["Tech"],"content":"HTTP 长连接短连接 长连接：多用于操作频繁，点对点的通讯，而且客户端连接数目较少的情况。例如即时通讯、网络游戏等。 短连接：用户数目较多的Web网站的 HTTP 服务一般用短连接。例如京东，淘宝这样的大型网站一般客户端数量达到千万级甚至上亿，若采用长连接势必会使得服务端大量的资源被无效占用，所以一般使用的是短连接。 面试高频指数：★★★☆☆ 长连接：多用于操作频繁，点对点的通讯，而且客户端连接数目较少的情况。例如即时通讯、网络游戏等。 短连接：用户数目较多的Web网站的 HTTP 服务一般用短连接。例如京东，淘宝这样的大型网站一般客户端数量达到千万级甚至上亿，若采用长连接势必会使得服务端大量的资源被无效占用，所以一般使用的是短连接 实际上，说HTTP请求和HTTP响应会更准确一些，而HTTP请求和HTTP响应，都是通过TCP连接这个通道来回传输的。 但是注意长连接短连接本质上是TCP连接 第一个问题是，是不是只要设置Connection为keep-alive就算是长连接了？ 当然是的，但要服务器和客户端都设置。 第二个问题是，我们平时用的是不是长连接？ 这个也毫无疑问，当然是的。（现在用的基本上都是HTTP1.1协议，你观察一下就会发现，基本上Connection都是keep-alive。而且HTTP协议文档上也提到了，HTTP1.1默认是长连接，也就是默认Connection的值就是keep-alive） 第三个问题，也是LZ之前最想不明白的问题，那就是我们这种普通的Web应用（比如博客园，我的个人博客这种）用长连接有啥好处？需不需要关掉长连接而使用短连接？ 这个问题LZ现在终于明白了，问题的答案是好处还是有的。 好处是什么？ 首先，刚才已经说了，长连接是为了复用，这个在之前LZ就明白。那既然长连接是指的TCP连接，也就是说复用的是TCP连接。那这就很好解释了，也就是说，长连接情况下，多个HTTP请求可以复用同一个TCP连接，这就节省了很多TCP连接建立和断开的消耗。 比如你请求了博客园的一个网页，这个网页里肯定还包含了CSS、JS等等一系列资源，如果你是短连接（也就是每次都要重新建立TCP连接）的话，那你每打开一个网页，基本要建立几个甚至几十个TCP连接，这浪费了多少资源就不用LZ去说了吧。 但如果是长连接的话，那么这么多次HTTP请求（这些请求包括请求网页内容，CSS文件，JS文件，图片等等），其实使用的都是一个TCP连接，很显然是可以节省很多消耗的。 这样一解释，就很明白了，不知道大家看了这些解释感觉如何，反正LZ在自己想明白以后，有种豁然开朗的感觉。 另外，最后关于长连接还要多提一句，那就是，长连接并不是永久连接的。如果一段时间内（具体的时间长短，是可以在header当中进行设置的，也就是所谓的超时时间），这个连接没有HTTP请求发出的话，那么这个长连接就会被断掉。 什么时候用长连接，短连接？ 长连接多用于操作频繁，点对点的通讯，而且连接数不能太多情况，。每个TCP连接都需要三步握手，这需要时间，如果每个操作都是先连接，再操作的话那么处理速度会降低很多，所以每个操作完后都不断开，次处理时直接发送数据包就OK了，不用建立TCP连接。例如：数据库的连接用长连接， 如果用短连接频繁的通信会造成socket错误，而且频繁的socket 创建也是对资源的浪费。 而像WEB网站的http服务一般都用短链接，因为长连接对于服务端来说会耗费一定的资源，而像WEB网站这么频繁的成千上万甚至上亿客户端的连接用短连接会更省一些资源，如果用长连接，而且同时有成千上万的用户，如果每个用户都占用一个连接的话，那可想而知吧。所以并发量大，但每个用户无需频繁操作情况下需用短连好。 长连接与短连接区别: **长连接:**连接→数据传输→保持连接(心跳)→数据传输→保持连接(心跳)→……→关闭连接（一个TCP连接通道多个读写通信）；_ _ **短连接:**连接→数据传输→关闭连接； 使用连接接受时HTTP报文的长度 在直接传输时，服务器使用header中的content-length来告诉客户端需要接收多少数据。 但是在HTTP 1.1中，服务器可以分段分块发送数据，此时服务器不告诉客户端需要接收多少数据，而是在传输结束时传输一个长度为0的块来表示接受完毕。 GET and POST get 提交的数据会放在 URL 之后，并且请求参数会被完整的保留在浏览器的记录里，由于参数直接暴露在 URL 中，可能会存在安全问题，因此往往用于获取资源信息。而 post 参数放在请求主体中，并且参数不会被保留，相比 get 方法，post 方法更安全，主要用于修改服务器上的资源。 get 请求只支持 URL 编码，post 请求支持多种编码格式。 get 只支持 ASCII 字符格式的参数，而 post 方法没有限制。 get 提交的数据大小有限制（这里所说的限制是针对浏览器而言的），而 post 方法提交的数据没限制 get 方式需要使用 Request.QueryString 来取得变量的值，而 post 方式通过 Request.Form 来获取。 get 方法产生一个 TCP 数据包，post 方法产生两个（并不是所有的浏览器中都产生两个）。 get请求的长度限制取决于浏览器 IE 2KB+35 FireFox 64KB Chrome 8KB-10 这个长度不是只针对数据部分，而是针对整个 URL 而言 URL构成：协议 + :// + 认证信息 + @ + 域名 or IP地址 + 端口号 + 资源路径 + ? + 查询字符串 + # + 片段标识符; ","date":"2022-09-18","objectID":"/computer-network/:1:1","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#http-长连接短连接"},{"categories":["Tech"],"content":"HTTP 长连接短连接 长连接：多用于操作频繁，点对点的通讯，而且客户端连接数目较少的情况。例如即时通讯、网络游戏等。 短连接：用户数目较多的Web网站的 HTTP 服务一般用短连接。例如京东，淘宝这样的大型网站一般客户端数量达到千万级甚至上亿，若采用长连接势必会使得服务端大量的资源被无效占用，所以一般使用的是短连接。 面试高频指数：★★★☆☆ 长连接：多用于操作频繁，点对点的通讯，而且客户端连接数目较少的情况。例如即时通讯、网络游戏等。 短连接：用户数目较多的Web网站的 HTTP 服务一般用短连接。例如京东，淘宝这样的大型网站一般客户端数量达到千万级甚至上亿，若采用长连接势必会使得服务端大量的资源被无效占用，所以一般使用的是短连接 实际上，说HTTP请求和HTTP响应会更准确一些，而HTTP请求和HTTP响应，都是通过TCP连接这个通道来回传输的。 但是注意长连接短连接本质上是TCP连接 第一个问题是，是不是只要设置Connection为keep-alive就算是长连接了？ 当然是的，但要服务器和客户端都设置。 第二个问题是，我们平时用的是不是长连接？ 这个也毫无疑问，当然是的。（现在用的基本上都是HTTP1.1协议，你观察一下就会发现，基本上Connection都是keep-alive。而且HTTP协议文档上也提到了，HTTP1.1默认是长连接，也就是默认Connection的值就是keep-alive） 第三个问题，也是LZ之前最想不明白的问题，那就是我们这种普通的Web应用（比如博客园，我的个人博客这种）用长连接有啥好处？需不需要关掉长连接而使用短连接？ 这个问题LZ现在终于明白了，问题的答案是好处还是有的。 好处是什么？ 首先，刚才已经说了，长连接是为了复用，这个在之前LZ就明白。那既然长连接是指的TCP连接，也就是说复用的是TCP连接。那这就很好解释了，也就是说，长连接情况下，多个HTTP请求可以复用同一个TCP连接，这就节省了很多TCP连接建立和断开的消耗。 比如你请求了博客园的一个网页，这个网页里肯定还包含了CSS、JS等等一系列资源，如果你是短连接（也就是每次都要重新建立TCP连接）的话，那你每打开一个网页，基本要建立几个甚至几十个TCP连接，这浪费了多少资源就不用LZ去说了吧。 但如果是长连接的话，那么这么多次HTTP请求（这些请求包括请求网页内容，CSS文件，JS文件，图片等等），其实使用的都是一个TCP连接，很显然是可以节省很多消耗的。 这样一解释，就很明白了，不知道大家看了这些解释感觉如何，反正LZ在自己想明白以后，有种豁然开朗的感觉。 另外，最后关于长连接还要多提一句，那就是，长连接并不是永久连接的。如果一段时间内（具体的时间长短，是可以在header当中进行设置的，也就是所谓的超时时间），这个连接没有HTTP请求发出的话，那么这个长连接就会被断掉。 什么时候用长连接，短连接？ 长连接多用于操作频繁，点对点的通讯，而且连接数不能太多情况，。每个TCP连接都需要三步握手，这需要时间，如果每个操作都是先连接，再操作的话那么处理速度会降低很多，所以每个操作完后都不断开，次处理时直接发送数据包就OK了，不用建立TCP连接。例如：数据库的连接用长连接， 如果用短连接频繁的通信会造成socket错误，而且频繁的socket 创建也是对资源的浪费。 而像WEB网站的http服务一般都用短链接，因为长连接对于服务端来说会耗费一定的资源，而像WEB网站这么频繁的成千上万甚至上亿客户端的连接用短连接会更省一些资源，如果用长连接，而且同时有成千上万的用户，如果每个用户都占用一个连接的话，那可想而知吧。所以并发量大，但每个用户无需频繁操作情况下需用短连好。 长连接与短连接区别: **长连接:**连接→数据传输→保持连接(心跳)→数据传输→保持连接(心跳)→……→关闭连接（一个TCP连接通道多个读写通信）；_ _ **短连接:**连接→数据传输→关闭连接； 使用连接接受时HTTP报文的长度 在直接传输时，服务器使用header中的content-length来告诉客户端需要接收多少数据。 但是在HTTP 1.1中，服务器可以分段分块发送数据，此时服务器不告诉客户端需要接收多少数据，而是在传输结束时传输一个长度为0的块来表示接受完毕。 GET and POST get 提交的数据会放在 URL 之后，并且请求参数会被完整的保留在浏览器的记录里，由于参数直接暴露在 URL 中，可能会存在安全问题，因此往往用于获取资源信息。而 post 参数放在请求主体中，并且参数不会被保留，相比 get 方法，post 方法更安全，主要用于修改服务器上的资源。 get 请求只支持 URL 编码，post 请求支持多种编码格式。 get 只支持 ASCII 字符格式的参数，而 post 方法没有限制。 get 提交的数据大小有限制（这里所说的限制是针对浏览器而言的），而 post 方法提交的数据没限制 get 方式需要使用 Request.QueryString 来取得变量的值，而 post 方式通过 Request.Form 来获取。 get 方法产生一个 TCP 数据包，post 方法产生两个（并不是所有的浏览器中都产生两个）。 get请求的长度限制取决于浏览器 IE 2KB+35 FireFox 64KB Chrome 8KB-10 这个长度不是只针对数据部分，而是针对整个 URL 而言 URL构成：协议 + :// + 认证信息 + @ + 域名 or IP地址 + 端口号 + 资源路径 + ? + 查询字符串 + # + 片段标识符; ","date":"2022-09-18","objectID":"/computer-network/:1:1","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#第一个问题是是不是只要设置connection为keep-alive就算是长连接了"},{"categories":["Tech"],"content":"HTTP 长连接短连接 长连接：多用于操作频繁，点对点的通讯，而且客户端连接数目较少的情况。例如即时通讯、网络游戏等。 短连接：用户数目较多的Web网站的 HTTP 服务一般用短连接。例如京东，淘宝这样的大型网站一般客户端数量达到千万级甚至上亿，若采用长连接势必会使得服务端大量的资源被无效占用，所以一般使用的是短连接。 面试高频指数：★★★☆☆ 长连接：多用于操作频繁，点对点的通讯，而且客户端连接数目较少的情况。例如即时通讯、网络游戏等。 短连接：用户数目较多的Web网站的 HTTP 服务一般用短连接。例如京东，淘宝这样的大型网站一般客户端数量达到千万级甚至上亿，若采用长连接势必会使得服务端大量的资源被无效占用，所以一般使用的是短连接 实际上，说HTTP请求和HTTP响应会更准确一些，而HTTP请求和HTTP响应，都是通过TCP连接这个通道来回传输的。 但是注意长连接短连接本质上是TCP连接 第一个问题是，是不是只要设置Connection为keep-alive就算是长连接了？ 当然是的，但要服务器和客户端都设置。 第二个问题是，我们平时用的是不是长连接？ 这个也毫无疑问，当然是的。（现在用的基本上都是HTTP1.1协议，你观察一下就会发现，基本上Connection都是keep-alive。而且HTTP协议文档上也提到了，HTTP1.1默认是长连接，也就是默认Connection的值就是keep-alive） 第三个问题，也是LZ之前最想不明白的问题，那就是我们这种普通的Web应用（比如博客园，我的个人博客这种）用长连接有啥好处？需不需要关掉长连接而使用短连接？ 这个问题LZ现在终于明白了，问题的答案是好处还是有的。 好处是什么？ 首先，刚才已经说了，长连接是为了复用，这个在之前LZ就明白。那既然长连接是指的TCP连接，也就是说复用的是TCP连接。那这就很好解释了，也就是说，长连接情况下，多个HTTP请求可以复用同一个TCP连接，这就节省了很多TCP连接建立和断开的消耗。 比如你请求了博客园的一个网页，这个网页里肯定还包含了CSS、JS等等一系列资源，如果你是短连接（也就是每次都要重新建立TCP连接）的话，那你每打开一个网页，基本要建立几个甚至几十个TCP连接，这浪费了多少资源就不用LZ去说了吧。 但如果是长连接的话，那么这么多次HTTP请求（这些请求包括请求网页内容，CSS文件，JS文件，图片等等），其实使用的都是一个TCP连接，很显然是可以节省很多消耗的。 这样一解释，就很明白了，不知道大家看了这些解释感觉如何，反正LZ在自己想明白以后，有种豁然开朗的感觉。 另外，最后关于长连接还要多提一句，那就是，长连接并不是永久连接的。如果一段时间内（具体的时间长短，是可以在header当中进行设置的，也就是所谓的超时时间），这个连接没有HTTP请求发出的话，那么这个长连接就会被断掉。 什么时候用长连接，短连接？ 长连接多用于操作频繁，点对点的通讯，而且连接数不能太多情况，。每个TCP连接都需要三步握手，这需要时间，如果每个操作都是先连接，再操作的话那么处理速度会降低很多，所以每个操作完后都不断开，次处理时直接发送数据包就OK了，不用建立TCP连接。例如：数据库的连接用长连接， 如果用短连接频繁的通信会造成socket错误，而且频繁的socket 创建也是对资源的浪费。 而像WEB网站的http服务一般都用短链接，因为长连接对于服务端来说会耗费一定的资源，而像WEB网站这么频繁的成千上万甚至上亿客户端的连接用短连接会更省一些资源，如果用长连接，而且同时有成千上万的用户，如果每个用户都占用一个连接的话，那可想而知吧。所以并发量大，但每个用户无需频繁操作情况下需用短连好。 长连接与短连接区别: **长连接:**连接→数据传输→保持连接(心跳)→数据传输→保持连接(心跳)→……→关闭连接（一个TCP连接通道多个读写通信）；_ _ **短连接:**连接→数据传输→关闭连接； 使用连接接受时HTTP报文的长度 在直接传输时，服务器使用header中的content-length来告诉客户端需要接收多少数据。 但是在HTTP 1.1中，服务器可以分段分块发送数据，此时服务器不告诉客户端需要接收多少数据，而是在传输结束时传输一个长度为0的块来表示接受完毕。 GET and POST get 提交的数据会放在 URL 之后，并且请求参数会被完整的保留在浏览器的记录里，由于参数直接暴露在 URL 中，可能会存在安全问题，因此往往用于获取资源信息。而 post 参数放在请求主体中，并且参数不会被保留，相比 get 方法，post 方法更安全，主要用于修改服务器上的资源。 get 请求只支持 URL 编码，post 请求支持多种编码格式。 get 只支持 ASCII 字符格式的参数，而 post 方法没有限制。 get 提交的数据大小有限制（这里所说的限制是针对浏览器而言的），而 post 方法提交的数据没限制 get 方式需要使用 Request.QueryString 来取得变量的值，而 post 方式通过 Request.Form 来获取。 get 方法产生一个 TCP 数据包，post 方法产生两个（并不是所有的浏览器中都产生两个）。 get请求的长度限制取决于浏览器 IE 2KB+35 FireFox 64KB Chrome 8KB-10 这个长度不是只针对数据部分，而是针对整个 URL 而言 URL构成：协议 + :// + 认证信息 + @ + 域名 or IP地址 + 端口号 + 资源路径 + ? + 查询字符串 + # + 片段标识符; ","date":"2022-09-18","objectID":"/computer-network/:1:1","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#第二个问题是我们平时用的是不是长连接"},{"categories":["Tech"],"content":"HTTP 长连接短连接 长连接：多用于操作频繁，点对点的通讯，而且客户端连接数目较少的情况。例如即时通讯、网络游戏等。 短连接：用户数目较多的Web网站的 HTTP 服务一般用短连接。例如京东，淘宝这样的大型网站一般客户端数量达到千万级甚至上亿，若采用长连接势必会使得服务端大量的资源被无效占用，所以一般使用的是短连接。 面试高频指数：★★★☆☆ 长连接：多用于操作频繁，点对点的通讯，而且客户端连接数目较少的情况。例如即时通讯、网络游戏等。 短连接：用户数目较多的Web网站的 HTTP 服务一般用短连接。例如京东，淘宝这样的大型网站一般客户端数量达到千万级甚至上亿，若采用长连接势必会使得服务端大量的资源被无效占用，所以一般使用的是短连接 实际上，说HTTP请求和HTTP响应会更准确一些，而HTTP请求和HTTP响应，都是通过TCP连接这个通道来回传输的。 但是注意长连接短连接本质上是TCP连接 第一个问题是，是不是只要设置Connection为keep-alive就算是长连接了？ 当然是的，但要服务器和客户端都设置。 第二个问题是，我们平时用的是不是长连接？ 这个也毫无疑问，当然是的。（现在用的基本上都是HTTP1.1协议，你观察一下就会发现，基本上Connection都是keep-alive。而且HTTP协议文档上也提到了，HTTP1.1默认是长连接，也就是默认Connection的值就是keep-alive） 第三个问题，也是LZ之前最想不明白的问题，那就是我们这种普通的Web应用（比如博客园，我的个人博客这种）用长连接有啥好处？需不需要关掉长连接而使用短连接？ 这个问题LZ现在终于明白了，问题的答案是好处还是有的。 好处是什么？ 首先，刚才已经说了，长连接是为了复用，这个在之前LZ就明白。那既然长连接是指的TCP连接，也就是说复用的是TCP连接。那这就很好解释了，也就是说，长连接情况下，多个HTTP请求可以复用同一个TCP连接，这就节省了很多TCP连接建立和断开的消耗。 比如你请求了博客园的一个网页，这个网页里肯定还包含了CSS、JS等等一系列资源，如果你是短连接（也就是每次都要重新建立TCP连接）的话，那你每打开一个网页，基本要建立几个甚至几十个TCP连接，这浪费了多少资源就不用LZ去说了吧。 但如果是长连接的话，那么这么多次HTTP请求（这些请求包括请求网页内容，CSS文件，JS文件，图片等等），其实使用的都是一个TCP连接，很显然是可以节省很多消耗的。 这样一解释，就很明白了，不知道大家看了这些解释感觉如何，反正LZ在自己想明白以后，有种豁然开朗的感觉。 另外，最后关于长连接还要多提一句，那就是，长连接并不是永久连接的。如果一段时间内（具体的时间长短，是可以在header当中进行设置的，也就是所谓的超时时间），这个连接没有HTTP请求发出的话，那么这个长连接就会被断掉。 什么时候用长连接，短连接？ 长连接多用于操作频繁，点对点的通讯，而且连接数不能太多情况，。每个TCP连接都需要三步握手，这需要时间，如果每个操作都是先连接，再操作的话那么处理速度会降低很多，所以每个操作完后都不断开，次处理时直接发送数据包就OK了，不用建立TCP连接。例如：数据库的连接用长连接， 如果用短连接频繁的通信会造成socket错误，而且频繁的socket 创建也是对资源的浪费。 而像WEB网站的http服务一般都用短链接，因为长连接对于服务端来说会耗费一定的资源，而像WEB网站这么频繁的成千上万甚至上亿客户端的连接用短连接会更省一些资源，如果用长连接，而且同时有成千上万的用户，如果每个用户都占用一个连接的话，那可想而知吧。所以并发量大，但每个用户无需频繁操作情况下需用短连好。 长连接与短连接区别: **长连接:**连接→数据传输→保持连接(心跳)→数据传输→保持连接(心跳)→……→关闭连接（一个TCP连接通道多个读写通信）；_ _ **短连接:**连接→数据传输→关闭连接； 使用连接接受时HTTP报文的长度 在直接传输时，服务器使用header中的content-length来告诉客户端需要接收多少数据。 但是在HTTP 1.1中，服务器可以分段分块发送数据，此时服务器不告诉客户端需要接收多少数据，而是在传输结束时传输一个长度为0的块来表示接受完毕。 GET and POST get 提交的数据会放在 URL 之后，并且请求参数会被完整的保留在浏览器的记录里，由于参数直接暴露在 URL 中，可能会存在安全问题，因此往往用于获取资源信息。而 post 参数放在请求主体中，并且参数不会被保留，相比 get 方法，post 方法更安全，主要用于修改服务器上的资源。 get 请求只支持 URL 编码，post 请求支持多种编码格式。 get 只支持 ASCII 字符格式的参数，而 post 方法没有限制。 get 提交的数据大小有限制（这里所说的限制是针对浏览器而言的），而 post 方法提交的数据没限制 get 方式需要使用 Request.QueryString 来取得变量的值，而 post 方式通过 Request.Form 来获取。 get 方法产生一个 TCP 数据包，post 方法产生两个（并不是所有的浏览器中都产生两个）。 get请求的长度限制取决于浏览器 IE 2KB+35 FireFox 64KB Chrome 8KB-10 这个长度不是只针对数据部分，而是针对整个 URL 而言 URL构成：协议 + :// + 认证信息 + @ + 域名 or IP地址 + 端口号 + 资源路径 + ? + 查询字符串 + # + 片段标识符; ","date":"2022-09-18","objectID":"/computer-network/:1:1","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#第三个问题也是lz之前最想不明白的问题那就是我们这种普通的web应用比如博客园我的个人博客这种用长连接有啥好处需不需要关掉长连接而使用短连接"},{"categories":["Tech"],"content":"HTTP 长连接短连接 长连接：多用于操作频繁，点对点的通讯，而且客户端连接数目较少的情况。例如即时通讯、网络游戏等。 短连接：用户数目较多的Web网站的 HTTP 服务一般用短连接。例如京东，淘宝这样的大型网站一般客户端数量达到千万级甚至上亿，若采用长连接势必会使得服务端大量的资源被无效占用，所以一般使用的是短连接。 面试高频指数：★★★☆☆ 长连接：多用于操作频繁，点对点的通讯，而且客户端连接数目较少的情况。例如即时通讯、网络游戏等。 短连接：用户数目较多的Web网站的 HTTP 服务一般用短连接。例如京东，淘宝这样的大型网站一般客户端数量达到千万级甚至上亿，若采用长连接势必会使得服务端大量的资源被无效占用，所以一般使用的是短连接 实际上，说HTTP请求和HTTP响应会更准确一些，而HTTP请求和HTTP响应，都是通过TCP连接这个通道来回传输的。 但是注意长连接短连接本质上是TCP连接 第一个问题是，是不是只要设置Connection为keep-alive就算是长连接了？ 当然是的，但要服务器和客户端都设置。 第二个问题是，我们平时用的是不是长连接？ 这个也毫无疑问，当然是的。（现在用的基本上都是HTTP1.1协议，你观察一下就会发现，基本上Connection都是keep-alive。而且HTTP协议文档上也提到了，HTTP1.1默认是长连接，也就是默认Connection的值就是keep-alive） 第三个问题，也是LZ之前最想不明白的问题，那就是我们这种普通的Web应用（比如博客园，我的个人博客这种）用长连接有啥好处？需不需要关掉长连接而使用短连接？ 这个问题LZ现在终于明白了，问题的答案是好处还是有的。 好处是什么？ 首先，刚才已经说了，长连接是为了复用，这个在之前LZ就明白。那既然长连接是指的TCP连接，也就是说复用的是TCP连接。那这就很好解释了，也就是说，长连接情况下，多个HTTP请求可以复用同一个TCP连接，这就节省了很多TCP连接建立和断开的消耗。 比如你请求了博客园的一个网页，这个网页里肯定还包含了CSS、JS等等一系列资源，如果你是短连接（也就是每次都要重新建立TCP连接）的话，那你每打开一个网页，基本要建立几个甚至几十个TCP连接，这浪费了多少资源就不用LZ去说了吧。 但如果是长连接的话，那么这么多次HTTP请求（这些请求包括请求网页内容，CSS文件，JS文件，图片等等），其实使用的都是一个TCP连接，很显然是可以节省很多消耗的。 这样一解释，就很明白了，不知道大家看了这些解释感觉如何，反正LZ在自己想明白以后，有种豁然开朗的感觉。 另外，最后关于长连接还要多提一句，那就是，长连接并不是永久连接的。如果一段时间内（具体的时间长短，是可以在header当中进行设置的，也就是所谓的超时时间），这个连接没有HTTP请求发出的话，那么这个长连接就会被断掉。 什么时候用长连接，短连接？ 长连接多用于操作频繁，点对点的通讯，而且连接数不能太多情况，。每个TCP连接都需要三步握手，这需要时间，如果每个操作都是先连接，再操作的话那么处理速度会降低很多，所以每个操作完后都不断开，次处理时直接发送数据包就OK了，不用建立TCP连接。例如：数据库的连接用长连接， 如果用短连接频繁的通信会造成socket错误，而且频繁的socket 创建也是对资源的浪费。 而像WEB网站的http服务一般都用短链接，因为长连接对于服务端来说会耗费一定的资源，而像WEB网站这么频繁的成千上万甚至上亿客户端的连接用短连接会更省一些资源，如果用长连接，而且同时有成千上万的用户，如果每个用户都占用一个连接的话，那可想而知吧。所以并发量大，但每个用户无需频繁操作情况下需用短连好。 长连接与短连接区别: **长连接:**连接→数据传输→保持连接(心跳)→数据传输→保持连接(心跳)→……→关闭连接（一个TCP连接通道多个读写通信）；_ _ **短连接:**连接→数据传输→关闭连接； 使用连接接受时HTTP报文的长度 在直接传输时，服务器使用header中的content-length来告诉客户端需要接收多少数据。 但是在HTTP 1.1中，服务器可以分段分块发送数据，此时服务器不告诉客户端需要接收多少数据，而是在传输结束时传输一个长度为0的块来表示接受完毕。 GET and POST get 提交的数据会放在 URL 之后，并且请求参数会被完整的保留在浏览器的记录里，由于参数直接暴露在 URL 中，可能会存在安全问题，因此往往用于获取资源信息。而 post 参数放在请求主体中，并且参数不会被保留，相比 get 方法，post 方法更安全，主要用于修改服务器上的资源。 get 请求只支持 URL 编码，post 请求支持多种编码格式。 get 只支持 ASCII 字符格式的参数，而 post 方法没有限制。 get 提交的数据大小有限制（这里所说的限制是针对浏览器而言的），而 post 方法提交的数据没限制 get 方式需要使用 Request.QueryString 来取得变量的值，而 post 方式通过 Request.Form 来获取。 get 方法产生一个 TCP 数据包，post 方法产生两个（并不是所有的浏览器中都产生两个）。 get请求的长度限制取决于浏览器 IE 2KB+35 FireFox 64KB Chrome 8KB-10 这个长度不是只针对数据部分，而是针对整个 URL 而言 URL构成：协议 + :// + 认证信息 + @ + 域名 or IP地址 + 端口号 + 资源路径 + ? + 查询字符串 + # + 片段标识符; ","date":"2022-09-18","objectID":"/computer-network/:1:1","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#什么时候用长连接短连接"},{"categories":["Tech"],"content":"HTTP 长连接短连接 长连接：多用于操作频繁，点对点的通讯，而且客户端连接数目较少的情况。例如即时通讯、网络游戏等。 短连接：用户数目较多的Web网站的 HTTP 服务一般用短连接。例如京东，淘宝这样的大型网站一般客户端数量达到千万级甚至上亿，若采用长连接势必会使得服务端大量的资源被无效占用，所以一般使用的是短连接。 面试高频指数：★★★☆☆ 长连接：多用于操作频繁，点对点的通讯，而且客户端连接数目较少的情况。例如即时通讯、网络游戏等。 短连接：用户数目较多的Web网站的 HTTP 服务一般用短连接。例如京东，淘宝这样的大型网站一般客户端数量达到千万级甚至上亿，若采用长连接势必会使得服务端大量的资源被无效占用，所以一般使用的是短连接 实际上，说HTTP请求和HTTP响应会更准确一些，而HTTP请求和HTTP响应，都是通过TCP连接这个通道来回传输的。 但是注意长连接短连接本质上是TCP连接 第一个问题是，是不是只要设置Connection为keep-alive就算是长连接了？ 当然是的，但要服务器和客户端都设置。 第二个问题是，我们平时用的是不是长连接？ 这个也毫无疑问，当然是的。（现在用的基本上都是HTTP1.1协议，你观察一下就会发现，基本上Connection都是keep-alive。而且HTTP协议文档上也提到了，HTTP1.1默认是长连接，也就是默认Connection的值就是keep-alive） 第三个问题，也是LZ之前最想不明白的问题，那就是我们这种普通的Web应用（比如博客园，我的个人博客这种）用长连接有啥好处？需不需要关掉长连接而使用短连接？ 这个问题LZ现在终于明白了，问题的答案是好处还是有的。 好处是什么？ 首先，刚才已经说了，长连接是为了复用，这个在之前LZ就明白。那既然长连接是指的TCP连接，也就是说复用的是TCP连接。那这就很好解释了，也就是说，长连接情况下，多个HTTP请求可以复用同一个TCP连接，这就节省了很多TCP连接建立和断开的消耗。 比如你请求了博客园的一个网页，这个网页里肯定还包含了CSS、JS等等一系列资源，如果你是短连接（也就是每次都要重新建立TCP连接）的话，那你每打开一个网页，基本要建立几个甚至几十个TCP连接，这浪费了多少资源就不用LZ去说了吧。 但如果是长连接的话，那么这么多次HTTP请求（这些请求包括请求网页内容，CSS文件，JS文件，图片等等），其实使用的都是一个TCP连接，很显然是可以节省很多消耗的。 这样一解释，就很明白了，不知道大家看了这些解释感觉如何，反正LZ在自己想明白以后，有种豁然开朗的感觉。 另外，最后关于长连接还要多提一句，那就是，长连接并不是永久连接的。如果一段时间内（具体的时间长短，是可以在header当中进行设置的，也就是所谓的超时时间），这个连接没有HTTP请求发出的话，那么这个长连接就会被断掉。 什么时候用长连接，短连接？ 长连接多用于操作频繁，点对点的通讯，而且连接数不能太多情况，。每个TCP连接都需要三步握手，这需要时间，如果每个操作都是先连接，再操作的话那么处理速度会降低很多，所以每个操作完后都不断开，次处理时直接发送数据包就OK了，不用建立TCP连接。例如：数据库的连接用长连接， 如果用短连接频繁的通信会造成socket错误，而且频繁的socket 创建也是对资源的浪费。 而像WEB网站的http服务一般都用短链接，因为长连接对于服务端来说会耗费一定的资源，而像WEB网站这么频繁的成千上万甚至上亿客户端的连接用短连接会更省一些资源，如果用长连接，而且同时有成千上万的用户，如果每个用户都占用一个连接的话，那可想而知吧。所以并发量大，但每个用户无需频繁操作情况下需用短连好。 长连接与短连接区别: **长连接:**连接→数据传输→保持连接(心跳)→数据传输→保持连接(心跳)→……→关闭连接（一个TCP连接通道多个读写通信）；_ _ **短连接:**连接→数据传输→关闭连接； 使用连接接受时HTTP报文的长度 在直接传输时，服务器使用header中的content-length来告诉客户端需要接收多少数据。 但是在HTTP 1.1中，服务器可以分段分块发送数据，此时服务器不告诉客户端需要接收多少数据，而是在传输结束时传输一个长度为0的块来表示接受完毕。 GET and POST get 提交的数据会放在 URL 之后，并且请求参数会被完整的保留在浏览器的记录里，由于参数直接暴露在 URL 中，可能会存在安全问题，因此往往用于获取资源信息。而 post 参数放在请求主体中，并且参数不会被保留，相比 get 方法，post 方法更安全，主要用于修改服务器上的资源。 get 请求只支持 URL 编码，post 请求支持多种编码格式。 get 只支持 ASCII 字符格式的参数，而 post 方法没有限制。 get 提交的数据大小有限制（这里所说的限制是针对浏览器而言的），而 post 方法提交的数据没限制 get 方式需要使用 Request.QueryString 来取得变量的值，而 post 方式通过 Request.Form 来获取。 get 方法产生一个 TCP 数据包，post 方法产生两个（并不是所有的浏览器中都产生两个）。 get请求的长度限制取决于浏览器 IE 2KB+35 FireFox 64KB Chrome 8KB-10 这个长度不是只针对数据部分，而是针对整个 URL 而言 URL构成：协议 + :// + 认证信息 + @ + 域名 or IP地址 + 端口号 + 资源路径 + ? + 查询字符串 + # + 片段标识符; ","date":"2022-09-18","objectID":"/computer-network/:1:1","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#长连接与短连接区别"},{"categories":["Tech"],"content":"HTTP 长连接短连接 长连接：多用于操作频繁，点对点的通讯，而且客户端连接数目较少的情况。例如即时通讯、网络游戏等。 短连接：用户数目较多的Web网站的 HTTP 服务一般用短连接。例如京东，淘宝这样的大型网站一般客户端数量达到千万级甚至上亿，若采用长连接势必会使得服务端大量的资源被无效占用，所以一般使用的是短连接。 面试高频指数：★★★☆☆ 长连接：多用于操作频繁，点对点的通讯，而且客户端连接数目较少的情况。例如即时通讯、网络游戏等。 短连接：用户数目较多的Web网站的 HTTP 服务一般用短连接。例如京东，淘宝这样的大型网站一般客户端数量达到千万级甚至上亿，若采用长连接势必会使得服务端大量的资源被无效占用，所以一般使用的是短连接 实际上，说HTTP请求和HTTP响应会更准确一些，而HTTP请求和HTTP响应，都是通过TCP连接这个通道来回传输的。 但是注意长连接短连接本质上是TCP连接 第一个问题是，是不是只要设置Connection为keep-alive就算是长连接了？ 当然是的，但要服务器和客户端都设置。 第二个问题是，我们平时用的是不是长连接？ 这个也毫无疑问，当然是的。（现在用的基本上都是HTTP1.1协议，你观察一下就会发现，基本上Connection都是keep-alive。而且HTTP协议文档上也提到了，HTTP1.1默认是长连接，也就是默认Connection的值就是keep-alive） 第三个问题，也是LZ之前最想不明白的问题，那就是我们这种普通的Web应用（比如博客园，我的个人博客这种）用长连接有啥好处？需不需要关掉长连接而使用短连接？ 这个问题LZ现在终于明白了，问题的答案是好处还是有的。 好处是什么？ 首先，刚才已经说了，长连接是为了复用，这个在之前LZ就明白。那既然长连接是指的TCP连接，也就是说复用的是TCP连接。那这就很好解释了，也就是说，长连接情况下，多个HTTP请求可以复用同一个TCP连接，这就节省了很多TCP连接建立和断开的消耗。 比如你请求了博客园的一个网页，这个网页里肯定还包含了CSS、JS等等一系列资源，如果你是短连接（也就是每次都要重新建立TCP连接）的话，那你每打开一个网页，基本要建立几个甚至几十个TCP连接，这浪费了多少资源就不用LZ去说了吧。 但如果是长连接的话，那么这么多次HTTP请求（这些请求包括请求网页内容，CSS文件，JS文件，图片等等），其实使用的都是一个TCP连接，很显然是可以节省很多消耗的。 这样一解释，就很明白了，不知道大家看了这些解释感觉如何，反正LZ在自己想明白以后，有种豁然开朗的感觉。 另外，最后关于长连接还要多提一句，那就是，长连接并不是永久连接的。如果一段时间内（具体的时间长短，是可以在header当中进行设置的，也就是所谓的超时时间），这个连接没有HTTP请求发出的话，那么这个长连接就会被断掉。 什么时候用长连接，短连接？ 长连接多用于操作频繁，点对点的通讯，而且连接数不能太多情况，。每个TCP连接都需要三步握手，这需要时间，如果每个操作都是先连接，再操作的话那么处理速度会降低很多，所以每个操作完后都不断开，次处理时直接发送数据包就OK了，不用建立TCP连接。例如：数据库的连接用长连接， 如果用短连接频繁的通信会造成socket错误，而且频繁的socket 创建也是对资源的浪费。 而像WEB网站的http服务一般都用短链接，因为长连接对于服务端来说会耗费一定的资源，而像WEB网站这么频繁的成千上万甚至上亿客户端的连接用短连接会更省一些资源，如果用长连接，而且同时有成千上万的用户，如果每个用户都占用一个连接的话，那可想而知吧。所以并发量大，但每个用户无需频繁操作情况下需用短连好。 长连接与短连接区别: **长连接:**连接→数据传输→保持连接(心跳)→数据传输→保持连接(心跳)→……→关闭连接（一个TCP连接通道多个读写通信）；_ _ **短连接:**连接→数据传输→关闭连接； 使用连接接受时HTTP报文的长度 在直接传输时，服务器使用header中的content-length来告诉客户端需要接收多少数据。 但是在HTTP 1.1中，服务器可以分段分块发送数据，此时服务器不告诉客户端需要接收多少数据，而是在传输结束时传输一个长度为0的块来表示接受完毕。 GET and POST get 提交的数据会放在 URL 之后，并且请求参数会被完整的保留在浏览器的记录里，由于参数直接暴露在 URL 中，可能会存在安全问题，因此往往用于获取资源信息。而 post 参数放在请求主体中，并且参数不会被保留，相比 get 方法，post 方法更安全，主要用于修改服务器上的资源。 get 请求只支持 URL 编码，post 请求支持多种编码格式。 get 只支持 ASCII 字符格式的参数，而 post 方法没有限制。 get 提交的数据大小有限制（这里所说的限制是针对浏览器而言的），而 post 方法提交的数据没限制 get 方式需要使用 Request.QueryString 来取得变量的值，而 post 方式通过 Request.Form 来获取。 get 方法产生一个 TCP 数据包，post 方法产生两个（并不是所有的浏览器中都产生两个）。 get请求的长度限制取决于浏览器 IE 2KB+35 FireFox 64KB Chrome 8KB-10 这个长度不是只针对数据部分，而是针对整个 URL 而言 URL构成：协议 + :// + 认证信息 + @ + 域名 or IP地址 + 端口号 + 资源路径 + ? + 查询字符串 + # + 片段标识符; ","date":"2022-09-18","objectID":"/computer-network/:1:1","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#get-and-post"},{"categories":["Tech"],"content":"HTTP HTTP（Hyper Text Transfer Protocol: 超文本传输协议） 是一种简单的请求 - 响应协议，被用于在 Web 浏览器和网站服务器之间传递消息。HTTP 使用 TCP（而不是 UDP）作为它的支撑运输层协议。其默认工作在 TCP 协议 80 端口，HTTP 客户机发起一个与服务器的 TCP 连接，一旦连接建立，浏览器和服务器进程就可以通过套接字接口访问 TCP。客户机从套接字接口发送 HTTP 请求报文和接收 HTTP 响应报文。类似地，服务器也是从套接字接口接收 HTTP 请求报文和发送 HTTP 响应报文。其通信内容以明文的方式发送，不通过任何方式的数据加密。当通信结束时，客户端与服务器关闭连接。 ","date":"2022-09-18","objectID":"/computer-network/:1:2","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#http"},{"categories":["Tech"],"content":"HTTPS SSL验证过程如下： 前提：公钥加密的信息只能用私钥解锁 Client发起一个HTTPS（https:/demo.linianhui.dev）的请求，根据RFC2818的规定，Client知道需要连接Server的443（默认）端口。 Server把事先配置好的公钥证书（public key certificate）返回给客户端。 Client验证公钥证书：比如是否在有效期内，证书的用途是不是匹配Client请求的站点，是不是在CRL吊销列表里面，它的上一级证书是否有效，这是一个递归的过程，直到验证到根证书（操作系统内置的Root证书或者Client内置的Root证书）。如果验证通过则继续，不通过则显示警告信息。 Client使用伪随机数生成器生成加密所使用的会话密钥，然后用证书的公钥加密这个会话密钥，发给Server。 Server使用自己的私钥（private key）解密这个消息，得到会话密钥。至此，Client和Server双方都持有了相同的会话密钥。 Server使用会话密钥加密“明文内容A”，发送给Client。 Client使用会话密钥解密响应的密文，得到“明文内容A”。 Client再次发起HTTPS的请求，使用会话密钥加密请求的“明文内容B”，然后Server使用会话密钥解密密文，得到“明文内容B”。 HTTP和HTTPS HTTP 协议以明文方式发送内容，数据都是未加密的，安全性较差。HTTPS 数据传输过程是加密的，安全性较好。 HTTP 和 HTTPS 使用的是完全不同的连接方式，用的端口也不一样，前者是 80 端口，后者是 443 端口。 HTTPS 协议需要到数字认证机构（Certificate Authority, CA）申请证书，一般需要一定的费用。 HTTP 页面响应比 HTTPS 快，主要因为 HTTP 使用 3 次握手建立连接，客户端和服务器需要握手 3 次，而 HTTPS 除了 TCP 的 3 次握手，还需要经历一个 SSL 协商过程。 对称加密虽然性能好但有密钥泄漏的风险，非对称加密（2组公钥+2私钥双向传输）安全但性能低下，因此考虑用非对称加密来传输对称加密所需的密钥，然后进行对称加密，但是为了防止非对称过程产生的中间人攻击，需要对服务器公钥和服务器身份进行配对的数字认证，然后引入了CA数字签名+数字证书验证的方式 补充：非常清晰 https://zhuanlan.zhihu.com/p/43789231 HTTP如何维持用户状态？ 如何维持用户状态？ 基于Session，服务器创建并保存键值对：SessionId-Session，然后将SessionId下发给客户端，客户端将其存在Cookie中，每次请求带上这个SessionId，服务器就可以将状态和会话联系起来。（安全 但是分布式系统中不一定每次请求都落在存储有sessionid的服务器上，可以使用redis存储来解决） 基于Cookie，服务器发送响应消息时在响应头中设置Set-Cookie字段，存储客户端的状态信息。客户端根据这个字段来创建Cookie并在请求时带上（每个Cookie都包含着客户端的状态信息），从而实现状态保持。（服务器压力较小，但不够安全并且http请求需要发送额外信息） 二者的区别：后者完全将会话状态存储在浏览器Cookie中。 Cookie被禁用了，可以通过重写URL的方式将会话标识放在URL的参数里。 Http状态码 200 请求成功 204 请求成功但无内容返回 206 范围请求成功 301 永久重定向； 30(2|3|7)临时重定向，语义和实现有略微区别； 304 带if-modified-since 请求首部的条件请求，条件没有满足 400 语法错误（前端挨打） 401 需要认证信息 403 拒绝访问 404 找不到资源 412 除if-modified-since 以外的条件请求，条件未满足 500 服务器错误（后端挨打） 503 服务器宕机了（DevOps or IT 挨打） ① 状态码 301 和 302 的区别？ 301：永久移动。请求的资源已被永久的移动到新的URI，旧的地址已经被永久的删除了。返回信息会包括新的URI，浏览器会自动定向到新的URI。今后新的请求都应使用新的URI代替。 302：临时移动。与301类似，客户端拿到服务端的响应消息后会跳转到一个新的 URL 地址。但资源只是临时被移动，旧的地址还在，客户端应继续使用原有URI。 HTTP1.0 and HTTP1.1 HTTP1.0、1.1 长连接：1.0默认浏览器和服务器之间是短连接，服务器发送完之后会直接关闭TCP连接，如果想多个HTTP请求复用TCP连接，需要在头部里增加Connection：Keep-Alive ； 1.1的时候默认使用持久连接，减少了资源消耗 Keep-Alive 并不是没有缺点的，当长时间的保持 TCP 连接时容易导致系统资源被无效占用，若对 Keep-Alive 模式配置不当，将有可能比非 Keep-Alive 模式带来的损失更大。 缓存处理：1.1请求头中增加了一些和缓存相关的字段，比如If-Match，可以更加灵活地控制缓存策略 HTTP缓存 访问一个网站的时候，浏览器会向服务器请求很多资源，比如css、js这些静态文件，如果每次请求都要让服务器发送所有资源，请求多了会对服务器造成很大的压力 HTTP为了解决这个问题，就引入了缓存，缓存有两种策略： 1、强缓存：每次请求资源的时候会先去缓存里找，如果有就直接用，没有才去向服务器请求资源 2、对比缓存：每次请求资源的时候先发个消息和服务器确认一下，如果服务器返回304，说明缓存可以用，浏览器就会去缓存里寻找资源 节约带宽：1.0默认把资源相关的整个对象都发给客户端，但是可能客户端不需要所有的信息，这就浪费了带宽资源；1.1的请求头引入了Range头域，可以只请求部分资源，比如断点下载的时候就可以用Range 错误通知的管理：1.1增加了一些错误状态响应码（24个），比如410表示请求的资源已经被永久删除 Host请求头：1.0的时候每台服务器都绑定唯一的IP地址，后面出现了虚拟主机，一个物理服务器可以有多个虚拟主机，一起共享同一个IP地址，所以为了区分不同虚拟主机，1.1添加了host请求头存放主机名 HTTP2.0 二进制传送：之前版本 数据都是用文本传输，因为文本有多种格式，所以不能很好地适应所有场景； 2.0传送的是二进制，相当于统一了格式 多路复用：1.1虽然默认复用TCP连接，但是每个请求是串行执行的，如果前面的请求超时，后面的请求只能等着（也就是线头阻塞）； 2.0的时候每个请求有自己的ID，多个请求可以在同一个TCP连接上并行执行，不会互相影响 header压缩：每次进行HTTP请求响应的时候，头部里很多的字段都是重复的，在2.0中，将字段记录到一张表中，头部只需要存放字段对应的编号就行，用的时候只需要拿着编号去表里查找就行，减少了传输的数据量 服务端推送：服务器会在客户端没发起请求的时候主动推送一些需要的资源，比如客户端请求一个html文件，服务器发送完之后会把和这个html页面相关的静态文件也发送给客户端，当客户端准备向服务器请求静态文件的时候，就可以直接从缓存中获取，就不需要再发起请求了 HTTP3.0 ","date":"2022-09-18","objectID":"/computer-network/:1:3","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#https"},{"categories":["Tech"],"content":"HTTPS SSL验证过程如下： 前提：公钥加密的信息只能用私钥解锁 Client发起一个HTTPS（https:/demo.linianhui.dev）的请求，根据RFC2818的规定，Client知道需要连接Server的443（默认）端口。 Server把事先配置好的公钥证书（public key certificate）返回给客户端。 Client验证公钥证书：比如是否在有效期内，证书的用途是不是匹配Client请求的站点，是不是在CRL吊销列表里面，它的上一级证书是否有效，这是一个递归的过程，直到验证到根证书（操作系统内置的Root证书或者Client内置的Root证书）。如果验证通过则继续，不通过则显示警告信息。 Client使用伪随机数生成器生成加密所使用的会话密钥，然后用证书的公钥加密这个会话密钥，发给Server。 Server使用自己的私钥（private key）解密这个消息，得到会话密钥。至此，Client和Server双方都持有了相同的会话密钥。 Server使用会话密钥加密“明文内容A”，发送给Client。 Client使用会话密钥解密响应的密文，得到“明文内容A”。 Client再次发起HTTPS的请求，使用会话密钥加密请求的“明文内容B”，然后Server使用会话密钥解密密文，得到“明文内容B”。 HTTP和HTTPS HTTP 协议以明文方式发送内容，数据都是未加密的，安全性较差。HTTPS 数据传输过程是加密的，安全性较好。 HTTP 和 HTTPS 使用的是完全不同的连接方式，用的端口也不一样，前者是 80 端口，后者是 443 端口。 HTTPS 协议需要到数字认证机构（Certificate Authority, CA）申请证书，一般需要一定的费用。 HTTP 页面响应比 HTTPS 快，主要因为 HTTP 使用 3 次握手建立连接，客户端和服务器需要握手 3 次，而 HTTPS 除了 TCP 的 3 次握手，还需要经历一个 SSL 协商过程。 对称加密虽然性能好但有密钥泄漏的风险，非对称加密（2组公钥+2私钥双向传输）安全但性能低下，因此考虑用非对称加密来传输对称加密所需的密钥，然后进行对称加密，但是为了防止非对称过程产生的中间人攻击，需要对服务器公钥和服务器身份进行配对的数字认证，然后引入了CA数字签名+数字证书验证的方式 补充：非常清晰 https://zhuanlan.zhihu.com/p/43789231 HTTP如何维持用户状态？ 如何维持用户状态？ 基于Session，服务器创建并保存键值对：SessionId-Session，然后将SessionId下发给客户端，客户端将其存在Cookie中，每次请求带上这个SessionId，服务器就可以将状态和会话联系起来。（安全 但是分布式系统中不一定每次请求都落在存储有sessionid的服务器上，可以使用redis存储来解决） 基于Cookie，服务器发送响应消息时在响应头中设置Set-Cookie字段，存储客户端的状态信息。客户端根据这个字段来创建Cookie并在请求时带上（每个Cookie都包含着客户端的状态信息），从而实现状态保持。（服务器压力较小，但不够安全并且http请求需要发送额外信息） 二者的区别：后者完全将会话状态存储在浏览器Cookie中。 Cookie被禁用了，可以通过重写URL的方式将会话标识放在URL的参数里。 Http状态码 200 请求成功 204 请求成功但无内容返回 206 范围请求成功 301 永久重定向； 30(2|3|7)临时重定向，语义和实现有略微区别； 304 带if-modified-since 请求首部的条件请求，条件没有满足 400 语法错误（前端挨打） 401 需要认证信息 403 拒绝访问 404 找不到资源 412 除if-modified-since 以外的条件请求，条件未满足 500 服务器错误（后端挨打） 503 服务器宕机了（DevOps or IT 挨打） ① 状态码 301 和 302 的区别？ 301：永久移动。请求的资源已被永久的移动到新的URI，旧的地址已经被永久的删除了。返回信息会包括新的URI，浏览器会自动定向到新的URI。今后新的请求都应使用新的URI代替。 302：临时移动。与301类似，客户端拿到服务端的响应消息后会跳转到一个新的 URL 地址。但资源只是临时被移动，旧的地址还在，客户端应继续使用原有URI。 HTTP1.0 and HTTP1.1 HTTP1.0、1.1 长连接：1.0默认浏览器和服务器之间是短连接，服务器发送完之后会直接关闭TCP连接，如果想多个HTTP请求复用TCP连接，需要在头部里增加Connection：Keep-Alive ； 1.1的时候默认使用持久连接，减少了资源消耗 Keep-Alive 并不是没有缺点的，当长时间的保持 TCP 连接时容易导致系统资源被无效占用，若对 Keep-Alive 模式配置不当，将有可能比非 Keep-Alive 模式带来的损失更大。 缓存处理：1.1请求头中增加了一些和缓存相关的字段，比如If-Match，可以更加灵活地控制缓存策略 HTTP缓存 访问一个网站的时候，浏览器会向服务器请求很多资源，比如css、js这些静态文件，如果每次请求都要让服务器发送所有资源，请求多了会对服务器造成很大的压力 HTTP为了解决这个问题，就引入了缓存，缓存有两种策略： 1、强缓存：每次请求资源的时候会先去缓存里找，如果有就直接用，没有才去向服务器请求资源 2、对比缓存：每次请求资源的时候先发个消息和服务器确认一下，如果服务器返回304，说明缓存可以用，浏览器就会去缓存里寻找资源 节约带宽：1.0默认把资源相关的整个对象都发给客户端，但是可能客户端不需要所有的信息，这就浪费了带宽资源；1.1的请求头引入了Range头域，可以只请求部分资源，比如断点下载的时候就可以用Range 错误通知的管理：1.1增加了一些错误状态响应码（24个），比如410表示请求的资源已经被永久删除 Host请求头：1.0的时候每台服务器都绑定唯一的IP地址，后面出现了虚拟主机，一个物理服务器可以有多个虚拟主机，一起共享同一个IP地址，所以为了区分不同虚拟主机，1.1添加了host请求头存放主机名 HTTP2.0 二进制传送：之前版本 数据都是用文本传输，因为文本有多种格式，所以不能很好地适应所有场景； 2.0传送的是二进制，相当于统一了格式 多路复用：1.1虽然默认复用TCP连接，但是每个请求是串行执行的，如果前面的请求超时，后面的请求只能等着（也就是线头阻塞）； 2.0的时候每个请求有自己的ID，多个请求可以在同一个TCP连接上并行执行，不会互相影响 header压缩：每次进行HTTP请求响应的时候，头部里很多的字段都是重复的，在2.0中，将字段记录到一张表中，头部只需要存放字段对应的编号就行，用的时候只需要拿着编号去表里查找就行，减少了传输的数据量 服务端推送：服务器会在客户端没发起请求的时候主动推送一些需要的资源，比如客户端请求一个html文件，服务器发送完之后会把和这个html页面相关的静态文件也发送给客户端，当客户端准备向服务器请求静态文件的时候，就可以直接从缓存中获取，就不需要再发起请求了 HTTP3.0 ","date":"2022-09-18","objectID":"/computer-network/:1:3","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#http和https"},{"categories":["Tech"],"content":"HTTPS SSL验证过程如下： 前提：公钥加密的信息只能用私钥解锁 Client发起一个HTTPS（https:/demo.linianhui.dev）的请求，根据RFC2818的规定，Client知道需要连接Server的443（默认）端口。 Server把事先配置好的公钥证书（public key certificate）返回给客户端。 Client验证公钥证书：比如是否在有效期内，证书的用途是不是匹配Client请求的站点，是不是在CRL吊销列表里面，它的上一级证书是否有效，这是一个递归的过程，直到验证到根证书（操作系统内置的Root证书或者Client内置的Root证书）。如果验证通过则继续，不通过则显示警告信息。 Client使用伪随机数生成器生成加密所使用的会话密钥，然后用证书的公钥加密这个会话密钥，发给Server。 Server使用自己的私钥（private key）解密这个消息，得到会话密钥。至此，Client和Server双方都持有了相同的会话密钥。 Server使用会话密钥加密“明文内容A”，发送给Client。 Client使用会话密钥解密响应的密文，得到“明文内容A”。 Client再次发起HTTPS的请求，使用会话密钥加密请求的“明文内容B”，然后Server使用会话密钥解密密文，得到“明文内容B”。 HTTP和HTTPS HTTP 协议以明文方式发送内容，数据都是未加密的，安全性较差。HTTPS 数据传输过程是加密的，安全性较好。 HTTP 和 HTTPS 使用的是完全不同的连接方式，用的端口也不一样，前者是 80 端口，后者是 443 端口。 HTTPS 协议需要到数字认证机构（Certificate Authority, CA）申请证书，一般需要一定的费用。 HTTP 页面响应比 HTTPS 快，主要因为 HTTP 使用 3 次握手建立连接，客户端和服务器需要握手 3 次，而 HTTPS 除了 TCP 的 3 次握手，还需要经历一个 SSL 协商过程。 对称加密虽然性能好但有密钥泄漏的风险，非对称加密（2组公钥+2私钥双向传输）安全但性能低下，因此考虑用非对称加密来传输对称加密所需的密钥，然后进行对称加密，但是为了防止非对称过程产生的中间人攻击，需要对服务器公钥和服务器身份进行配对的数字认证，然后引入了CA数字签名+数字证书验证的方式 补充：非常清晰 https://zhuanlan.zhihu.com/p/43789231 HTTP如何维持用户状态？ 如何维持用户状态？ 基于Session，服务器创建并保存键值对：SessionId-Session，然后将SessionId下发给客户端，客户端将其存在Cookie中，每次请求带上这个SessionId，服务器就可以将状态和会话联系起来。（安全 但是分布式系统中不一定每次请求都落在存储有sessionid的服务器上，可以使用redis存储来解决） 基于Cookie，服务器发送响应消息时在响应头中设置Set-Cookie字段，存储客户端的状态信息。客户端根据这个字段来创建Cookie并在请求时带上（每个Cookie都包含着客户端的状态信息），从而实现状态保持。（服务器压力较小，但不够安全并且http请求需要发送额外信息） 二者的区别：后者完全将会话状态存储在浏览器Cookie中。 Cookie被禁用了，可以通过重写URL的方式将会话标识放在URL的参数里。 Http状态码 200 请求成功 204 请求成功但无内容返回 206 范围请求成功 301 永久重定向； 30(2|3|7)临时重定向，语义和实现有略微区别； 304 带if-modified-since 请求首部的条件请求，条件没有满足 400 语法错误（前端挨打） 401 需要认证信息 403 拒绝访问 404 找不到资源 412 除if-modified-since 以外的条件请求，条件未满足 500 服务器错误（后端挨打） 503 服务器宕机了（DevOps or IT 挨打） ① 状态码 301 和 302 的区别？ 301：永久移动。请求的资源已被永久的移动到新的URI，旧的地址已经被永久的删除了。返回信息会包括新的URI，浏览器会自动定向到新的URI。今后新的请求都应使用新的URI代替。 302：临时移动。与301类似，客户端拿到服务端的响应消息后会跳转到一个新的 URL 地址。但资源只是临时被移动，旧的地址还在，客户端应继续使用原有URI。 HTTP1.0 and HTTP1.1 HTTP1.0、1.1 长连接：1.0默认浏览器和服务器之间是短连接，服务器发送完之后会直接关闭TCP连接，如果想多个HTTP请求复用TCP连接，需要在头部里增加Connection：Keep-Alive ； 1.1的时候默认使用持久连接，减少了资源消耗 Keep-Alive 并不是没有缺点的，当长时间的保持 TCP 连接时容易导致系统资源被无效占用，若对 Keep-Alive 模式配置不当，将有可能比非 Keep-Alive 模式带来的损失更大。 缓存处理：1.1请求头中增加了一些和缓存相关的字段，比如If-Match，可以更加灵活地控制缓存策略 HTTP缓存 访问一个网站的时候，浏览器会向服务器请求很多资源，比如css、js这些静态文件，如果每次请求都要让服务器发送所有资源，请求多了会对服务器造成很大的压力 HTTP为了解决这个问题，就引入了缓存，缓存有两种策略： 1、强缓存：每次请求资源的时候会先去缓存里找，如果有就直接用，没有才去向服务器请求资源 2、对比缓存：每次请求资源的时候先发个消息和服务器确认一下，如果服务器返回304，说明缓存可以用，浏览器就会去缓存里寻找资源 节约带宽：1.0默认把资源相关的整个对象都发给客户端，但是可能客户端不需要所有的信息，这就浪费了带宽资源；1.1的请求头引入了Range头域，可以只请求部分资源，比如断点下载的时候就可以用Range 错误通知的管理：1.1增加了一些错误状态响应码（24个），比如410表示请求的资源已经被永久删除 Host请求头：1.0的时候每台服务器都绑定唯一的IP地址，后面出现了虚拟主机，一个物理服务器可以有多个虚拟主机，一起共享同一个IP地址，所以为了区分不同虚拟主机，1.1添加了host请求头存放主机名 HTTP2.0 二进制传送：之前版本 数据都是用文本传输，因为文本有多种格式，所以不能很好地适应所有场景； 2.0传送的是二进制，相当于统一了格式 多路复用：1.1虽然默认复用TCP连接，但是每个请求是串行执行的，如果前面的请求超时，后面的请求只能等着（也就是线头阻塞）； 2.0的时候每个请求有自己的ID，多个请求可以在同一个TCP连接上并行执行，不会互相影响 header压缩：每次进行HTTP请求响应的时候，头部里很多的字段都是重复的，在2.0中，将字段记录到一张表中，头部只需要存放字段对应的编号就行，用的时候只需要拿着编号去表里查找就行，减少了传输的数据量 服务端推送：服务器会在客户端没发起请求的时候主动推送一些需要的资源，比如客户端请求一个html文件，服务器发送完之后会把和这个html页面相关的静态文件也发送给客户端，当客户端准备向服务器请求静态文件的时候，就可以直接从缓存中获取，就不需要再发起请求了 HTTP3.0 ","date":"2022-09-18","objectID":"/computer-network/:1:3","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#http如何维持用户状态"},{"categories":["Tech"],"content":"HTTPS SSL验证过程如下： 前提：公钥加密的信息只能用私钥解锁 Client发起一个HTTPS（https:/demo.linianhui.dev）的请求，根据RFC2818的规定，Client知道需要连接Server的443（默认）端口。 Server把事先配置好的公钥证书（public key certificate）返回给客户端。 Client验证公钥证书：比如是否在有效期内，证书的用途是不是匹配Client请求的站点，是不是在CRL吊销列表里面，它的上一级证书是否有效，这是一个递归的过程，直到验证到根证书（操作系统内置的Root证书或者Client内置的Root证书）。如果验证通过则继续，不通过则显示警告信息。 Client使用伪随机数生成器生成加密所使用的会话密钥，然后用证书的公钥加密这个会话密钥，发给Server。 Server使用自己的私钥（private key）解密这个消息，得到会话密钥。至此，Client和Server双方都持有了相同的会话密钥。 Server使用会话密钥加密“明文内容A”，发送给Client。 Client使用会话密钥解密响应的密文，得到“明文内容A”。 Client再次发起HTTPS的请求，使用会话密钥加密请求的“明文内容B”，然后Server使用会话密钥解密密文，得到“明文内容B”。 HTTP和HTTPS HTTP 协议以明文方式发送内容，数据都是未加密的，安全性较差。HTTPS 数据传输过程是加密的，安全性较好。 HTTP 和 HTTPS 使用的是完全不同的连接方式，用的端口也不一样，前者是 80 端口，后者是 443 端口。 HTTPS 协议需要到数字认证机构（Certificate Authority, CA）申请证书，一般需要一定的费用。 HTTP 页面响应比 HTTPS 快，主要因为 HTTP 使用 3 次握手建立连接，客户端和服务器需要握手 3 次，而 HTTPS 除了 TCP 的 3 次握手，还需要经历一个 SSL 协商过程。 对称加密虽然性能好但有密钥泄漏的风险，非对称加密（2组公钥+2私钥双向传输）安全但性能低下，因此考虑用非对称加密来传输对称加密所需的密钥，然后进行对称加密，但是为了防止非对称过程产生的中间人攻击，需要对服务器公钥和服务器身份进行配对的数字认证，然后引入了CA数字签名+数字证书验证的方式 补充：非常清晰 https://zhuanlan.zhihu.com/p/43789231 HTTP如何维持用户状态？ 如何维持用户状态？ 基于Session，服务器创建并保存键值对：SessionId-Session，然后将SessionId下发给客户端，客户端将其存在Cookie中，每次请求带上这个SessionId，服务器就可以将状态和会话联系起来。（安全 但是分布式系统中不一定每次请求都落在存储有sessionid的服务器上，可以使用redis存储来解决） 基于Cookie，服务器发送响应消息时在响应头中设置Set-Cookie字段，存储客户端的状态信息。客户端根据这个字段来创建Cookie并在请求时带上（每个Cookie都包含着客户端的状态信息），从而实现状态保持。（服务器压力较小，但不够安全并且http请求需要发送额外信息） 二者的区别：后者完全将会话状态存储在浏览器Cookie中。 Cookie被禁用了，可以通过重写URL的方式将会话标识放在URL的参数里。 Http状态码 200 请求成功 204 请求成功但无内容返回 206 范围请求成功 301 永久重定向； 30(2|3|7)临时重定向，语义和实现有略微区别； 304 带if-modified-since 请求首部的条件请求，条件没有满足 400 语法错误（前端挨打） 401 需要认证信息 403 拒绝访问 404 找不到资源 412 除if-modified-since 以外的条件请求，条件未满足 500 服务器错误（后端挨打） 503 服务器宕机了（DevOps or IT 挨打） ① 状态码 301 和 302 的区别？ 301：永久移动。请求的资源已被永久的移动到新的URI，旧的地址已经被永久的删除了。返回信息会包括新的URI，浏览器会自动定向到新的URI。今后新的请求都应使用新的URI代替。 302：临时移动。与301类似，客户端拿到服务端的响应消息后会跳转到一个新的 URL 地址。但资源只是临时被移动，旧的地址还在，客户端应继续使用原有URI。 HTTP1.0 and HTTP1.1 HTTP1.0、1.1 长连接：1.0默认浏览器和服务器之间是短连接，服务器发送完之后会直接关闭TCP连接，如果想多个HTTP请求复用TCP连接，需要在头部里增加Connection：Keep-Alive ； 1.1的时候默认使用持久连接，减少了资源消耗 Keep-Alive 并不是没有缺点的，当长时间的保持 TCP 连接时容易导致系统资源被无效占用，若对 Keep-Alive 模式配置不当，将有可能比非 Keep-Alive 模式带来的损失更大。 缓存处理：1.1请求头中增加了一些和缓存相关的字段，比如If-Match，可以更加灵活地控制缓存策略 HTTP缓存 访问一个网站的时候，浏览器会向服务器请求很多资源，比如css、js这些静态文件，如果每次请求都要让服务器发送所有资源，请求多了会对服务器造成很大的压力 HTTP为了解决这个问题，就引入了缓存，缓存有两种策略： 1、强缓存：每次请求资源的时候会先去缓存里找，如果有就直接用，没有才去向服务器请求资源 2、对比缓存：每次请求资源的时候先发个消息和服务器确认一下，如果服务器返回304，说明缓存可以用，浏览器就会去缓存里寻找资源 节约带宽：1.0默认把资源相关的整个对象都发给客户端，但是可能客户端不需要所有的信息，这就浪费了带宽资源；1.1的请求头引入了Range头域，可以只请求部分资源，比如断点下载的时候就可以用Range 错误通知的管理：1.1增加了一些错误状态响应码（24个），比如410表示请求的资源已经被永久删除 Host请求头：1.0的时候每台服务器都绑定唯一的IP地址，后面出现了虚拟主机，一个物理服务器可以有多个虚拟主机，一起共享同一个IP地址，所以为了区分不同虚拟主机，1.1添加了host请求头存放主机名 HTTP2.0 二进制传送：之前版本 数据都是用文本传输，因为文本有多种格式，所以不能很好地适应所有场景； 2.0传送的是二进制，相当于统一了格式 多路复用：1.1虽然默认复用TCP连接，但是每个请求是串行执行的，如果前面的请求超时，后面的请求只能等着（也就是线头阻塞）； 2.0的时候每个请求有自己的ID，多个请求可以在同一个TCP连接上并行执行，不会互相影响 header压缩：每次进行HTTP请求响应的时候，头部里很多的字段都是重复的，在2.0中，将字段记录到一张表中，头部只需要存放字段对应的编号就行，用的时候只需要拿着编号去表里查找就行，减少了传输的数据量 服务端推送：服务器会在客户端没发起请求的时候主动推送一些需要的资源，比如客户端请求一个html文件，服务器发送完之后会把和这个html页面相关的静态文件也发送给客户端，当客户端准备向服务器请求静态文件的时候，就可以直接从缓存中获取，就不需要再发起请求了 HTTP3.0 ","date":"2022-09-18","objectID":"/computer-network/:1:3","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#http状态码"},{"categories":["Tech"],"content":"HTTPS SSL验证过程如下： 前提：公钥加密的信息只能用私钥解锁 Client发起一个HTTPS（https:/demo.linianhui.dev）的请求，根据RFC2818的规定，Client知道需要连接Server的443（默认）端口。 Server把事先配置好的公钥证书（public key certificate）返回给客户端。 Client验证公钥证书：比如是否在有效期内，证书的用途是不是匹配Client请求的站点，是不是在CRL吊销列表里面，它的上一级证书是否有效，这是一个递归的过程，直到验证到根证书（操作系统内置的Root证书或者Client内置的Root证书）。如果验证通过则继续，不通过则显示警告信息。 Client使用伪随机数生成器生成加密所使用的会话密钥，然后用证书的公钥加密这个会话密钥，发给Server。 Server使用自己的私钥（private key）解密这个消息，得到会话密钥。至此，Client和Server双方都持有了相同的会话密钥。 Server使用会话密钥加密“明文内容A”，发送给Client。 Client使用会话密钥解密响应的密文，得到“明文内容A”。 Client再次发起HTTPS的请求，使用会话密钥加密请求的“明文内容B”，然后Server使用会话密钥解密密文，得到“明文内容B”。 HTTP和HTTPS HTTP 协议以明文方式发送内容，数据都是未加密的，安全性较差。HTTPS 数据传输过程是加密的，安全性较好。 HTTP 和 HTTPS 使用的是完全不同的连接方式，用的端口也不一样，前者是 80 端口，后者是 443 端口。 HTTPS 协议需要到数字认证机构（Certificate Authority, CA）申请证书，一般需要一定的费用。 HTTP 页面响应比 HTTPS 快，主要因为 HTTP 使用 3 次握手建立连接，客户端和服务器需要握手 3 次，而 HTTPS 除了 TCP 的 3 次握手，还需要经历一个 SSL 协商过程。 对称加密虽然性能好但有密钥泄漏的风险，非对称加密（2组公钥+2私钥双向传输）安全但性能低下，因此考虑用非对称加密来传输对称加密所需的密钥，然后进行对称加密，但是为了防止非对称过程产生的中间人攻击，需要对服务器公钥和服务器身份进行配对的数字认证，然后引入了CA数字签名+数字证书验证的方式 补充：非常清晰 https://zhuanlan.zhihu.com/p/43789231 HTTP如何维持用户状态？ 如何维持用户状态？ 基于Session，服务器创建并保存键值对：SessionId-Session，然后将SessionId下发给客户端，客户端将其存在Cookie中，每次请求带上这个SessionId，服务器就可以将状态和会话联系起来。（安全 但是分布式系统中不一定每次请求都落在存储有sessionid的服务器上，可以使用redis存储来解决） 基于Cookie，服务器发送响应消息时在响应头中设置Set-Cookie字段，存储客户端的状态信息。客户端根据这个字段来创建Cookie并在请求时带上（每个Cookie都包含着客户端的状态信息），从而实现状态保持。（服务器压力较小，但不够安全并且http请求需要发送额外信息） 二者的区别：后者完全将会话状态存储在浏览器Cookie中。 Cookie被禁用了，可以通过重写URL的方式将会话标识放在URL的参数里。 Http状态码 200 请求成功 204 请求成功但无内容返回 206 范围请求成功 301 永久重定向； 30(2|3|7)临时重定向，语义和实现有略微区别； 304 带if-modified-since 请求首部的条件请求，条件没有满足 400 语法错误（前端挨打） 401 需要认证信息 403 拒绝访问 404 找不到资源 412 除if-modified-since 以外的条件请求，条件未满足 500 服务器错误（后端挨打） 503 服务器宕机了（DevOps or IT 挨打） ① 状态码 301 和 302 的区别？ 301：永久移动。请求的资源已被永久的移动到新的URI，旧的地址已经被永久的删除了。返回信息会包括新的URI，浏览器会自动定向到新的URI。今后新的请求都应使用新的URI代替。 302：临时移动。与301类似，客户端拿到服务端的响应消息后会跳转到一个新的 URL 地址。但资源只是临时被移动，旧的地址还在，客户端应继续使用原有URI。 HTTP1.0 and HTTP1.1 HTTP1.0、1.1 长连接：1.0默认浏览器和服务器之间是短连接，服务器发送完之后会直接关闭TCP连接，如果想多个HTTP请求复用TCP连接，需要在头部里增加Connection：Keep-Alive ； 1.1的时候默认使用持久连接，减少了资源消耗 Keep-Alive 并不是没有缺点的，当长时间的保持 TCP 连接时容易导致系统资源被无效占用，若对 Keep-Alive 模式配置不当，将有可能比非 Keep-Alive 模式带来的损失更大。 缓存处理：1.1请求头中增加了一些和缓存相关的字段，比如If-Match，可以更加灵活地控制缓存策略 HTTP缓存 访问一个网站的时候，浏览器会向服务器请求很多资源，比如css、js这些静态文件，如果每次请求都要让服务器发送所有资源，请求多了会对服务器造成很大的压力 HTTP为了解决这个问题，就引入了缓存，缓存有两种策略： 1、强缓存：每次请求资源的时候会先去缓存里找，如果有就直接用，没有才去向服务器请求资源 2、对比缓存：每次请求资源的时候先发个消息和服务器确认一下，如果服务器返回304，说明缓存可以用，浏览器就会去缓存里寻找资源 节约带宽：1.0默认把资源相关的整个对象都发给客户端，但是可能客户端不需要所有的信息，这就浪费了带宽资源；1.1的请求头引入了Range头域，可以只请求部分资源，比如断点下载的时候就可以用Range 错误通知的管理：1.1增加了一些错误状态响应码（24个），比如410表示请求的资源已经被永久删除 Host请求头：1.0的时候每台服务器都绑定唯一的IP地址，后面出现了虚拟主机，一个物理服务器可以有多个虚拟主机，一起共享同一个IP地址，所以为了区分不同虚拟主机，1.1添加了host请求头存放主机名 HTTP2.0 二进制传送：之前版本 数据都是用文本传输，因为文本有多种格式，所以不能很好地适应所有场景； 2.0传送的是二进制，相当于统一了格式 多路复用：1.1虽然默认复用TCP连接，但是每个请求是串行执行的，如果前面的请求超时，后面的请求只能等着（也就是线头阻塞）； 2.0的时候每个请求有自己的ID，多个请求可以在同一个TCP连接上并行执行，不会互相影响 header压缩：每次进行HTTP请求响应的时候，头部里很多的字段都是重复的，在2.0中，将字段记录到一张表中，头部只需要存放字段对应的编号就行，用的时候只需要拿着编号去表里查找就行，减少了传输的数据量 服务端推送：服务器会在客户端没发起请求的时候主动推送一些需要的资源，比如客户端请求一个html文件，服务器发送完之后会把和这个html页面相关的静态文件也发送给客户端，当客户端准备向服务器请求静态文件的时候，就可以直接从缓存中获取，就不需要再发起请求了 HTTP3.0 ","date":"2022-09-18","objectID":"/computer-network/:1:3","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#http10-and-http11"},{"categories":["Tech"],"content":"HTTPS SSL验证过程如下： 前提：公钥加密的信息只能用私钥解锁 Client发起一个HTTPS（https:/demo.linianhui.dev）的请求，根据RFC2818的规定，Client知道需要连接Server的443（默认）端口。 Server把事先配置好的公钥证书（public key certificate）返回给客户端。 Client验证公钥证书：比如是否在有效期内，证书的用途是不是匹配Client请求的站点，是不是在CRL吊销列表里面，它的上一级证书是否有效，这是一个递归的过程，直到验证到根证书（操作系统内置的Root证书或者Client内置的Root证书）。如果验证通过则继续，不通过则显示警告信息。 Client使用伪随机数生成器生成加密所使用的会话密钥，然后用证书的公钥加密这个会话密钥，发给Server。 Server使用自己的私钥（private key）解密这个消息，得到会话密钥。至此，Client和Server双方都持有了相同的会话密钥。 Server使用会话密钥加密“明文内容A”，发送给Client。 Client使用会话密钥解密响应的密文，得到“明文内容A”。 Client再次发起HTTPS的请求，使用会话密钥加密请求的“明文内容B”，然后Server使用会话密钥解密密文，得到“明文内容B”。 HTTP和HTTPS HTTP 协议以明文方式发送内容，数据都是未加密的，安全性较差。HTTPS 数据传输过程是加密的，安全性较好。 HTTP 和 HTTPS 使用的是完全不同的连接方式，用的端口也不一样，前者是 80 端口，后者是 443 端口。 HTTPS 协议需要到数字认证机构（Certificate Authority, CA）申请证书，一般需要一定的费用。 HTTP 页面响应比 HTTPS 快，主要因为 HTTP 使用 3 次握手建立连接，客户端和服务器需要握手 3 次，而 HTTPS 除了 TCP 的 3 次握手，还需要经历一个 SSL 协商过程。 对称加密虽然性能好但有密钥泄漏的风险，非对称加密（2组公钥+2私钥双向传输）安全但性能低下，因此考虑用非对称加密来传输对称加密所需的密钥，然后进行对称加密，但是为了防止非对称过程产生的中间人攻击，需要对服务器公钥和服务器身份进行配对的数字认证，然后引入了CA数字签名+数字证书验证的方式 补充：非常清晰 https://zhuanlan.zhihu.com/p/43789231 HTTP如何维持用户状态？ 如何维持用户状态？ 基于Session，服务器创建并保存键值对：SessionId-Session，然后将SessionId下发给客户端，客户端将其存在Cookie中，每次请求带上这个SessionId，服务器就可以将状态和会话联系起来。（安全 但是分布式系统中不一定每次请求都落在存储有sessionid的服务器上，可以使用redis存储来解决） 基于Cookie，服务器发送响应消息时在响应头中设置Set-Cookie字段，存储客户端的状态信息。客户端根据这个字段来创建Cookie并在请求时带上（每个Cookie都包含着客户端的状态信息），从而实现状态保持。（服务器压力较小，但不够安全并且http请求需要发送额外信息） 二者的区别：后者完全将会话状态存储在浏览器Cookie中。 Cookie被禁用了，可以通过重写URL的方式将会话标识放在URL的参数里。 Http状态码 200 请求成功 204 请求成功但无内容返回 206 范围请求成功 301 永久重定向； 30(2|3|7)临时重定向，语义和实现有略微区别； 304 带if-modified-since 请求首部的条件请求，条件没有满足 400 语法错误（前端挨打） 401 需要认证信息 403 拒绝访问 404 找不到资源 412 除if-modified-since 以外的条件请求，条件未满足 500 服务器错误（后端挨打） 503 服务器宕机了（DevOps or IT 挨打） ① 状态码 301 和 302 的区别？ 301：永久移动。请求的资源已被永久的移动到新的URI，旧的地址已经被永久的删除了。返回信息会包括新的URI，浏览器会自动定向到新的URI。今后新的请求都应使用新的URI代替。 302：临时移动。与301类似，客户端拿到服务端的响应消息后会跳转到一个新的 URL 地址。但资源只是临时被移动，旧的地址还在，客户端应继续使用原有URI。 HTTP1.0 and HTTP1.1 HTTP1.0、1.1 长连接：1.0默认浏览器和服务器之间是短连接，服务器发送完之后会直接关闭TCP连接，如果想多个HTTP请求复用TCP连接，需要在头部里增加Connection：Keep-Alive ； 1.1的时候默认使用持久连接，减少了资源消耗 Keep-Alive 并不是没有缺点的，当长时间的保持 TCP 连接时容易导致系统资源被无效占用，若对 Keep-Alive 模式配置不当，将有可能比非 Keep-Alive 模式带来的损失更大。 缓存处理：1.1请求头中增加了一些和缓存相关的字段，比如If-Match，可以更加灵活地控制缓存策略 HTTP缓存 访问一个网站的时候，浏览器会向服务器请求很多资源，比如css、js这些静态文件，如果每次请求都要让服务器发送所有资源，请求多了会对服务器造成很大的压力 HTTP为了解决这个问题，就引入了缓存，缓存有两种策略： 1、强缓存：每次请求资源的时候会先去缓存里找，如果有就直接用，没有才去向服务器请求资源 2、对比缓存：每次请求资源的时候先发个消息和服务器确认一下，如果服务器返回304，说明缓存可以用，浏览器就会去缓存里寻找资源 节约带宽：1.0默认把资源相关的整个对象都发给客户端，但是可能客户端不需要所有的信息，这就浪费了带宽资源；1.1的请求头引入了Range头域，可以只请求部分资源，比如断点下载的时候就可以用Range 错误通知的管理：1.1增加了一些错误状态响应码（24个），比如410表示请求的资源已经被永久删除 Host请求头：1.0的时候每台服务器都绑定唯一的IP地址，后面出现了虚拟主机，一个物理服务器可以有多个虚拟主机，一起共享同一个IP地址，所以为了区分不同虚拟主机，1.1添加了host请求头存放主机名 HTTP2.0 二进制传送：之前版本 数据都是用文本传输，因为文本有多种格式，所以不能很好地适应所有场景； 2.0传送的是二进制，相当于统一了格式 多路复用：1.1虽然默认复用TCP连接，但是每个请求是串行执行的，如果前面的请求超时，后面的请求只能等着（也就是线头阻塞）； 2.0的时候每个请求有自己的ID，多个请求可以在同一个TCP连接上并行执行，不会互相影响 header压缩：每次进行HTTP请求响应的时候，头部里很多的字段都是重复的，在2.0中，将字段记录到一张表中，头部只需要存放字段对应的编号就行，用的时候只需要拿着编号去表里查找就行，减少了传输的数据量 服务端推送：服务器会在客户端没发起请求的时候主动推送一些需要的资源，比如客户端请求一个html文件，服务器发送完之后会把和这个html页面相关的静态文件也发送给客户端，当客户端准备向服务器请求静态文件的时候，就可以直接从缓存中获取，就不需要再发起请求了 HTTP3.0 ","date":"2022-09-18","objectID":"/computer-network/:1:3","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#http20"},{"categories":["Tech"],"content":"HTTPS SSL验证过程如下： 前提：公钥加密的信息只能用私钥解锁 Client发起一个HTTPS（https:/demo.linianhui.dev）的请求，根据RFC2818的规定，Client知道需要连接Server的443（默认）端口。 Server把事先配置好的公钥证书（public key certificate）返回给客户端。 Client验证公钥证书：比如是否在有效期内，证书的用途是不是匹配Client请求的站点，是不是在CRL吊销列表里面，它的上一级证书是否有效，这是一个递归的过程，直到验证到根证书（操作系统内置的Root证书或者Client内置的Root证书）。如果验证通过则继续，不通过则显示警告信息。 Client使用伪随机数生成器生成加密所使用的会话密钥，然后用证书的公钥加密这个会话密钥，发给Server。 Server使用自己的私钥（private key）解密这个消息，得到会话密钥。至此，Client和Server双方都持有了相同的会话密钥。 Server使用会话密钥加密“明文内容A”，发送给Client。 Client使用会话密钥解密响应的密文，得到“明文内容A”。 Client再次发起HTTPS的请求，使用会话密钥加密请求的“明文内容B”，然后Server使用会话密钥解密密文，得到“明文内容B”。 HTTP和HTTPS HTTP 协议以明文方式发送内容，数据都是未加密的，安全性较差。HTTPS 数据传输过程是加密的，安全性较好。 HTTP 和 HTTPS 使用的是完全不同的连接方式，用的端口也不一样，前者是 80 端口，后者是 443 端口。 HTTPS 协议需要到数字认证机构（Certificate Authority, CA）申请证书，一般需要一定的费用。 HTTP 页面响应比 HTTPS 快，主要因为 HTTP 使用 3 次握手建立连接，客户端和服务器需要握手 3 次，而 HTTPS 除了 TCP 的 3 次握手，还需要经历一个 SSL 协商过程。 对称加密虽然性能好但有密钥泄漏的风险，非对称加密（2组公钥+2私钥双向传输）安全但性能低下，因此考虑用非对称加密来传输对称加密所需的密钥，然后进行对称加密，但是为了防止非对称过程产生的中间人攻击，需要对服务器公钥和服务器身份进行配对的数字认证，然后引入了CA数字签名+数字证书验证的方式 补充：非常清晰 https://zhuanlan.zhihu.com/p/43789231 HTTP如何维持用户状态？ 如何维持用户状态？ 基于Session，服务器创建并保存键值对：SessionId-Session，然后将SessionId下发给客户端，客户端将其存在Cookie中，每次请求带上这个SessionId，服务器就可以将状态和会话联系起来。（安全 但是分布式系统中不一定每次请求都落在存储有sessionid的服务器上，可以使用redis存储来解决） 基于Cookie，服务器发送响应消息时在响应头中设置Set-Cookie字段，存储客户端的状态信息。客户端根据这个字段来创建Cookie并在请求时带上（每个Cookie都包含着客户端的状态信息），从而实现状态保持。（服务器压力较小，但不够安全并且http请求需要发送额外信息） 二者的区别：后者完全将会话状态存储在浏览器Cookie中。 Cookie被禁用了，可以通过重写URL的方式将会话标识放在URL的参数里。 Http状态码 200 请求成功 204 请求成功但无内容返回 206 范围请求成功 301 永久重定向； 30(2|3|7)临时重定向，语义和实现有略微区别； 304 带if-modified-since 请求首部的条件请求，条件没有满足 400 语法错误（前端挨打） 401 需要认证信息 403 拒绝访问 404 找不到资源 412 除if-modified-since 以外的条件请求，条件未满足 500 服务器错误（后端挨打） 503 服务器宕机了（DevOps or IT 挨打） ① 状态码 301 和 302 的区别？ 301：永久移动。请求的资源已被永久的移动到新的URI，旧的地址已经被永久的删除了。返回信息会包括新的URI，浏览器会自动定向到新的URI。今后新的请求都应使用新的URI代替。 302：临时移动。与301类似，客户端拿到服务端的响应消息后会跳转到一个新的 URL 地址。但资源只是临时被移动，旧的地址还在，客户端应继续使用原有URI。 HTTP1.0 and HTTP1.1 HTTP1.0、1.1 长连接：1.0默认浏览器和服务器之间是短连接，服务器发送完之后会直接关闭TCP连接，如果想多个HTTP请求复用TCP连接，需要在头部里增加Connection：Keep-Alive ； 1.1的时候默认使用持久连接，减少了资源消耗 Keep-Alive 并不是没有缺点的，当长时间的保持 TCP 连接时容易导致系统资源被无效占用，若对 Keep-Alive 模式配置不当，将有可能比非 Keep-Alive 模式带来的损失更大。 缓存处理：1.1请求头中增加了一些和缓存相关的字段，比如If-Match，可以更加灵活地控制缓存策略 HTTP缓存 访问一个网站的时候，浏览器会向服务器请求很多资源，比如css、js这些静态文件，如果每次请求都要让服务器发送所有资源，请求多了会对服务器造成很大的压力 HTTP为了解决这个问题，就引入了缓存，缓存有两种策略： 1、强缓存：每次请求资源的时候会先去缓存里找，如果有就直接用，没有才去向服务器请求资源 2、对比缓存：每次请求资源的时候先发个消息和服务器确认一下，如果服务器返回304，说明缓存可以用，浏览器就会去缓存里寻找资源 节约带宽：1.0默认把资源相关的整个对象都发给客户端，但是可能客户端不需要所有的信息，这就浪费了带宽资源；1.1的请求头引入了Range头域，可以只请求部分资源，比如断点下载的时候就可以用Range 错误通知的管理：1.1增加了一些错误状态响应码（24个），比如410表示请求的资源已经被永久删除 Host请求头：1.0的时候每台服务器都绑定唯一的IP地址，后面出现了虚拟主机，一个物理服务器可以有多个虚拟主机，一起共享同一个IP地址，所以为了区分不同虚拟主机，1.1添加了host请求头存放主机名 HTTP2.0 二进制传送：之前版本 数据都是用文本传输，因为文本有多种格式，所以不能很好地适应所有场景； 2.0传送的是二进制，相当于统一了格式 多路复用：1.1虽然默认复用TCP连接，但是每个请求是串行执行的，如果前面的请求超时，后面的请求只能等着（也就是线头阻塞）； 2.0的时候每个请求有自己的ID，多个请求可以在同一个TCP连接上并行执行，不会互相影响 header压缩：每次进行HTTP请求响应的时候，头部里很多的字段都是重复的，在2.0中，将字段记录到一张表中，头部只需要存放字段对应的编号就行，用的时候只需要拿着编号去表里查找就行，减少了传输的数据量 服务端推送：服务器会在客户端没发起请求的时候主动推送一些需要的资源，比如客户端请求一个html文件，服务器发送完之后会把和这个html页面相关的静态文件也发送给客户端，当客户端准备向服务器请求静态文件的时候，就可以直接从缓存中获取，就不需要再发起请求了 HTTP3.0 ","date":"2022-09-18","objectID":"/computer-network/:1:3","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#http30"},{"categories":["Tech"],"content":"DNS DNS DNS（Domain Name System）是域名系统的英文缩写，是一种组织成域层次结构的计算机和网络服务命名系统，用于 TCP/IP 网络。 DNS 的作用 通常我们有两种方式识别主机：通过主机名或者 IP 地址。人们喜欢便于记忆的主机名表示，而路由器则喜欢定长的、有着层次结构的 IP 地址。为了满足这些不同的偏好，我们就需要一种能够进行主机名到 IP 地址转换的目录服务，域名系统作为将域名和 IP 地址相互映射的一个分布式数据库，能够使人更方便地访问互联网。 上图展示了 DNS 服务器的部分层次结构，从上到下依次为根域名服务器、顶级域名服务器和权威域名服务器。其实根域名服务器在因特网上有13个，大部分位于北美洲。第二层为顶级域服务器，这些服务器负责顶级域名（如 com、org、net、edu）和所有国家的顶级域名（如uk、fr、ca 和 jp）。在第三层为权威 DNS 服务器，因特网上具有公共可访问主机（例如 Web 服务器和邮件服务器）的每个组织机构必须提供公共可访问的 DNS 记录，这些记录由组织机构的权威 DNS 服务器负责保存，这些记录将这些主机的名称映射为 IP 地址。 先问本地，本地问根，根帮忙向下寻找，在一级或者其他等级中进行迭代，一级不知道就踢皮球给其他的一级。 拓展：域名解析查询的两种方式 递归查询：如果主机所询问的本地域名服务器不知道被查询域名的 IP 地址，那么本地域名服务器就以 DNS 客户端的身份，向其他根域名服务器继续发出查询请求报文，即替主机继续查询，而不是让主机自己进行下一步查询，如上图步骤（1）和（10）。 迭代查询：当根域名服务器收到本地域名服务器发出的迭代查询请求报文时，要么给出所要查询的 IP 地址，要么告诉本地服务器下一步应该找哪个域名服务器进行查询，然后让本地服务器进行后续的查询，如上图步骤（2）~（9）。 DNS 在进行查询时使用UDP，因为更加迅速且传输的信息一般不会超过UDP的长度上限。 ","date":"2022-09-18","objectID":"/computer-network/:1:4","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#dns"},{"categories":["Tech"],"content":"socket Socket是应用层与TCP/IP协议族通信的中间软件抽象层，它是一组接口。在设计模式中，Socket其实就是一个门面模式，它把复杂的TCP/IP协议族隐藏在Socket接口后面，对用户来说，一组简单的接口就是全部，让Socket去组织数据，以符合指定的协议。 可以理解为socket是电话，两个电话打通就可以进行通话（数据传输） URI and URL URI包含URL(locator)和URN(name) ","date":"2022-09-18","objectID":"/computer-network/:1:5","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#socket"},{"categories":["Tech"],"content":"网页解析过程 ","date":"2022-09-18","objectID":"/computer-network/:1:6","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#网页解析过程"},{"categories":["Tech"],"content":"传输层 ","date":"2022-09-18","objectID":"/computer-network/:2:0","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#传输层"},{"categories":["Tech"],"content":"TCP三次握手 客户端发送一个SYN段，并指明客户端的初始序列号，即ISN(c). 服务端发送自己的SYN段作为应答，同样指明自己的ISN(s)。为了确认客户端的SYN，将ISN(c)+1作为ACK数值。这样，每发送一个SYN，序列号就会加1. 如果有丢失的情况，则会重传。 为了确认服务器端的SYN，客户端将ISN(s)+1作为返回的ACK数值。 客户端发送随机数和序列号 服务端收到后回发收到的随机数和ACK，再自己生成一个序列号y并且将收到的序列号+1返回给客户端 客户端回复服务端 为什么要进行三次握手？ 三次握手对于双方来说分别验证了对方收发的正常能力： 三次握手异常情况处理： 简单来说就是 重传 上一次的包 或者 这一次的包，因为这一次的不成功可能是由这一次的包未成功发送或者上一次的包未成功接收导致的 重传有重传次数限制，可配置，超过重传次数代表建立连接失败 第一次握手SYN丢失：客户端重传SYN（每次超时重传的RTO是翻倍上涨的） 第二次握手ACK、SYN丢失：客户端重传SYN + 服务端重传ACK、SYN 第三次握手ACK丢失：服务端重传ACK、SYN ","date":"2022-09-18","objectID":"/computer-network/:2:1","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#tcp三次握手"},{"categories":["Tech"],"content":"TCP三次握手 客户端发送一个SYN段，并指明客户端的初始序列号，即ISN(c). 服务端发送自己的SYN段作为应答，同样指明自己的ISN(s)。为了确认客户端的SYN，将ISN(c)+1作为ACK数值。这样，每发送一个SYN，序列号就会加1. 如果有丢失的情况，则会重传。 为了确认服务器端的SYN，客户端将ISN(s)+1作为返回的ACK数值。 客户端发送随机数和序列号 服务端收到后回发收到的随机数和ACK，再自己生成一个序列号y并且将收到的序列号+1返回给客户端 客户端回复服务端 为什么要进行三次握手？ 三次握手对于双方来说分别验证了对方收发的正常能力： 三次握手异常情况处理： 简单来说就是 重传 上一次的包 或者 这一次的包，因为这一次的不成功可能是由这一次的包未成功发送或者上一次的包未成功接收导致的 重传有重传次数限制，可配置，超过重传次数代表建立连接失败 第一次握手SYN丢失：客户端重传SYN（每次超时重传的RTO是翻倍上涨的） 第二次握手ACK、SYN丢失：客户端重传SYN + 服务端重传ACK、SYN 第三次握手ACK丢失：服务端重传ACK、SYN ","date":"2022-09-18","objectID":"/computer-network/:2:1","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#为什么要进行三次握手"},{"categories":["Tech"],"content":"TCP三次握手 客户端发送一个SYN段，并指明客户端的初始序列号，即ISN(c). 服务端发送自己的SYN段作为应答，同样指明自己的ISN(s)。为了确认客户端的SYN，将ISN(c)+1作为ACK数值。这样，每发送一个SYN，序列号就会加1. 如果有丢失的情况，则会重传。 为了确认服务器端的SYN，客户端将ISN(s)+1作为返回的ACK数值。 客户端发送随机数和序列号 服务端收到后回发收到的随机数和ACK，再自己生成一个序列号y并且将收到的序列号+1返回给客户端 客户端回复服务端 为什么要进行三次握手？ 三次握手对于双方来说分别验证了对方收发的正常能力： 三次握手异常情况处理： 简单来说就是 重传 上一次的包 或者 这一次的包，因为这一次的不成功可能是由这一次的包未成功发送或者上一次的包未成功接收导致的 重传有重传次数限制，可配置，超过重传次数代表建立连接失败 第一次握手SYN丢失：客户端重传SYN（每次超时重传的RTO是翻倍上涨的） 第二次握手ACK、SYN丢失：客户端重传SYN + 服务端重传ACK、SYN 第三次握手ACK丢失：服务端重传ACK、SYN ","date":"2022-09-18","objectID":"/computer-network/:2:1","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#三次握手异常情况处理"},{"categories":["Tech"],"content":"四次挥手 TCP连接是双向传输的对等的模式，就是说双方都可以同时向对方发送或接收数据。当有一方要关闭连接时，会发送指令告知对方，我要关闭连接了。这时对方会回一个ACK，此时一个方向的连接关闭。但是另一个方向仍然可以继续传输数据，等到发送完了所有的数据后，会发送一个FIN段来关闭此方向上的连接。接收方发送ACK确认关闭连接。注意，接收到FIN报文的一方只能回复一个ACK, 它是无法马上返回对方一个FIN报文段的，因为结束数据传输的“指令”是上层应用层给出的，我只是一个“搬运工”，我无法了解“上层的意志”。 SYN和FIN段都是会利用重传进行可靠传输的。 客户端发送一个FIN段，并包含一个希望接收者看到的自己当前的序列号K. 同时还包含一个ACK表示确认对方最近一次发过来的数据。 服务端将K值加1作为ACK序号值，表明收到了上一个包。这时上层的应用程序会被告知另一端发起了关闭操作，通常这将引起应用程序发起自己的关闭操作。 服务端发起自己的FIN段，ACK=K+1, Seq=L 客户端确认。ACK=L+1 为什么要四次挥手？ 双方连接需要分别进行关闭，并且保证数据传输完毕。 因为TCP是全双工通信的 （1）第一次挥手 因此当主动方发送断开连接的请求（即FIN报文）给被动方时，仅仅代表主动方不会再发送数据报文了，但主动方仍可以接收数据报文。 （2）第二次挥手 被动方此时有可能还有相应的数据报文需要发送，因此需要先发送ACK报文，告知主动方“我知道你想断开连接的请求了”。这样主动方便不会因为没有收到应答而继续发送断开连接的请求（即FIN报文）。 （3）第三次挥手 被动方在处理完数据报文后，便发送给主动方FIN报文；这样可以保证数据通信正常可靠地完成。发送完FIN报文后，被动方进入LAST_ACK阶段（超时等待）。 （4）第四挥手 如果主动方及时发送ACK报文进行连接中断的确认，这时被动方就直接释放连接，进入可用状态。 CLOSE-WAIT 是服务端发出第一次挥手(整体第二次)进入的状态,表示\"我准备关闭了,但是还有自己的事情处理一下,你等我处理完\" 等服务器处理好自己的数据业务,则表示我准备好了,再发送 fin 包 TIME-WAIT 是第四次挥手后,客户端进入的状态,是客户端必要的等待时间,目的是等待:1-服务端的对应端口关闭与客户端发送到服务端的数据到达(可能出现延迟),如果不存在这个步骤就会导致两个问题: 客户端立即关闭后,立即又用同样的端口握手并建立通信,此时上次的连接残留的数据包会被误认为是本次的,造成数据异常 客户端直接关闭后,若服务端重新发送 fin 包,客户端就会回应 RST,会报异常,但是其实是没有问题的。 关于timewait中的MSL 服务器发送fin之后会等待一个最大生存时间MSL，若没有收到ACK则重新发送fin 客户端收到fin之后进入Time-wait状态，持续2MSL时间长度，在此阶段中若再次收到fin则刷新等待时间至2MSL并重发ACK。 timewait的问题 高并发场景下的time-wait会占用很多端口，长时间的time-wait占用资源。 解决方法之一是浏览器直接使用keep-alive防止频繁连接建立和断开。 另一个方法是使用SO_REUSEADDR套接字重复使用端口 ","date":"2022-09-18","objectID":"/computer-network/:2:2","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#四次挥手"},{"categories":["Tech"],"content":"四次挥手 TCP连接是双向传输的对等的模式，就是说双方都可以同时向对方发送或接收数据。当有一方要关闭连接时，会发送指令告知对方，我要关闭连接了。这时对方会回一个ACK，此时一个方向的连接关闭。但是另一个方向仍然可以继续传输数据，等到发送完了所有的数据后，会发送一个FIN段来关闭此方向上的连接。接收方发送ACK确认关闭连接。注意，接收到FIN报文的一方只能回复一个ACK, 它是无法马上返回对方一个FIN报文段的，因为结束数据传输的“指令”是上层应用层给出的，我只是一个“搬运工”，我无法了解“上层的意志”。 SYN和FIN段都是会利用重传进行可靠传输的。 客户端发送一个FIN段，并包含一个希望接收者看到的自己当前的序列号K. 同时还包含一个ACK表示确认对方最近一次发过来的数据。 服务端将K值加1作为ACK序号值，表明收到了上一个包。这时上层的应用程序会被告知另一端发起了关闭操作，通常这将引起应用程序发起自己的关闭操作。 服务端发起自己的FIN段，ACK=K+1, Seq=L 客户端确认。ACK=L+1 为什么要四次挥手？ 双方连接需要分别进行关闭，并且保证数据传输完毕。 因为TCP是全双工通信的 （1）第一次挥手 因此当主动方发送断开连接的请求（即FIN报文）给被动方时，仅仅代表主动方不会再发送数据报文了，但主动方仍可以接收数据报文。 （2）第二次挥手 被动方此时有可能还有相应的数据报文需要发送，因此需要先发送ACK报文，告知主动方“我知道你想断开连接的请求了”。这样主动方便不会因为没有收到应答而继续发送断开连接的请求（即FIN报文）。 （3）第三次挥手 被动方在处理完数据报文后，便发送给主动方FIN报文；这样可以保证数据通信正常可靠地完成。发送完FIN报文后，被动方进入LAST_ACK阶段（超时等待）。 （4）第四挥手 如果主动方及时发送ACK报文进行连接中断的确认，这时被动方就直接释放连接，进入可用状态。 CLOSE-WAIT 是服务端发出第一次挥手(整体第二次)进入的状态,表示\"我准备关闭了,但是还有自己的事情处理一下,你等我处理完\" 等服务器处理好自己的数据业务,则表示我准备好了,再发送 fin 包 TIME-WAIT 是第四次挥手后,客户端进入的状态,是客户端必要的等待时间,目的是等待:1-服务端的对应端口关闭与客户端发送到服务端的数据到达(可能出现延迟),如果不存在这个步骤就会导致两个问题: 客户端立即关闭后,立即又用同样的端口握手并建立通信,此时上次的连接残留的数据包会被误认为是本次的,造成数据异常 客户端直接关闭后,若服务端重新发送 fin 包,客户端就会回应 RST,会报异常,但是其实是没有问题的。 关于timewait中的MSL 服务器发送fin之后会等待一个最大生存时间MSL，若没有收到ACK则重新发送fin 客户端收到fin之后进入Time-wait状态，持续2MSL时间长度，在此阶段中若再次收到fin则刷新等待时间至2MSL并重发ACK。 timewait的问题 高并发场景下的time-wait会占用很多端口，长时间的time-wait占用资源。 解决方法之一是浏览器直接使用keep-alive防止频繁连接建立和断开。 另一个方法是使用SO_REUSEADDR套接字重复使用端口 ","date":"2022-09-18","objectID":"/computer-network/:2:2","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#为什么要四次挥手"},{"categories":["Tech"],"content":"四次挥手 TCP连接是双向传输的对等的模式，就是说双方都可以同时向对方发送或接收数据。当有一方要关闭连接时，会发送指令告知对方，我要关闭连接了。这时对方会回一个ACK，此时一个方向的连接关闭。但是另一个方向仍然可以继续传输数据，等到发送完了所有的数据后，会发送一个FIN段来关闭此方向上的连接。接收方发送ACK确认关闭连接。注意，接收到FIN报文的一方只能回复一个ACK, 它是无法马上返回对方一个FIN报文段的，因为结束数据传输的“指令”是上层应用层给出的，我只是一个“搬运工”，我无法了解“上层的意志”。 SYN和FIN段都是会利用重传进行可靠传输的。 客户端发送一个FIN段，并包含一个希望接收者看到的自己当前的序列号K. 同时还包含一个ACK表示确认对方最近一次发过来的数据。 服务端将K值加1作为ACK序号值，表明收到了上一个包。这时上层的应用程序会被告知另一端发起了关闭操作，通常这将引起应用程序发起自己的关闭操作。 服务端发起自己的FIN段，ACK=K+1, Seq=L 客户端确认。ACK=L+1 为什么要四次挥手？ 双方连接需要分别进行关闭，并且保证数据传输完毕。 因为TCP是全双工通信的 （1）第一次挥手 因此当主动方发送断开连接的请求（即FIN报文）给被动方时，仅仅代表主动方不会再发送数据报文了，但主动方仍可以接收数据报文。 （2）第二次挥手 被动方此时有可能还有相应的数据报文需要发送，因此需要先发送ACK报文，告知主动方“我知道你想断开连接的请求了”。这样主动方便不会因为没有收到应答而继续发送断开连接的请求（即FIN报文）。 （3）第三次挥手 被动方在处理完数据报文后，便发送给主动方FIN报文；这样可以保证数据通信正常可靠地完成。发送完FIN报文后，被动方进入LAST_ACK阶段（超时等待）。 （4）第四挥手 如果主动方及时发送ACK报文进行连接中断的确认，这时被动方就直接释放连接，进入可用状态。 CLOSE-WAIT 是服务端发出第一次挥手(整体第二次)进入的状态,表示\"我准备关闭了,但是还有自己的事情处理一下,你等我处理完\" 等服务器处理好自己的数据业务,则表示我准备好了,再发送 fin 包 TIME-WAIT 是第四次挥手后,客户端进入的状态,是客户端必要的等待时间,目的是等待:1-服务端的对应端口关闭与客户端发送到服务端的数据到达(可能出现延迟),如果不存在这个步骤就会导致两个问题: 客户端立即关闭后,立即又用同样的端口握手并建立通信,此时上次的连接残留的数据包会被误认为是本次的,造成数据异常 客户端直接关闭后,若服务端重新发送 fin 包,客户端就会回应 RST,会报异常,但是其实是没有问题的。 关于timewait中的MSL 服务器发送fin之后会等待一个最大生存时间MSL，若没有收到ACK则重新发送fin 客户端收到fin之后进入Time-wait状态，持续2MSL时间长度，在此阶段中若再次收到fin则刷新等待时间至2MSL并重发ACK。 timewait的问题 高并发场景下的time-wait会占用很多端口，长时间的time-wait占用资源。 解决方法之一是浏览器直接使用keep-alive防止频繁连接建立和断开。 另一个方法是使用SO_REUSEADDR套接字重复使用端口 ","date":"2022-09-18","objectID":"/computer-network/:2:2","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#close-wait"},{"categories":["Tech"],"content":"四次挥手 TCP连接是双向传输的对等的模式，就是说双方都可以同时向对方发送或接收数据。当有一方要关闭连接时，会发送指令告知对方，我要关闭连接了。这时对方会回一个ACK，此时一个方向的连接关闭。但是另一个方向仍然可以继续传输数据，等到发送完了所有的数据后，会发送一个FIN段来关闭此方向上的连接。接收方发送ACK确认关闭连接。注意，接收到FIN报文的一方只能回复一个ACK, 它是无法马上返回对方一个FIN报文段的，因为结束数据传输的“指令”是上层应用层给出的，我只是一个“搬运工”，我无法了解“上层的意志”。 SYN和FIN段都是会利用重传进行可靠传输的。 客户端发送一个FIN段，并包含一个希望接收者看到的自己当前的序列号K. 同时还包含一个ACK表示确认对方最近一次发过来的数据。 服务端将K值加1作为ACK序号值，表明收到了上一个包。这时上层的应用程序会被告知另一端发起了关闭操作，通常这将引起应用程序发起自己的关闭操作。 服务端发起自己的FIN段，ACK=K+1, Seq=L 客户端确认。ACK=L+1 为什么要四次挥手？ 双方连接需要分别进行关闭，并且保证数据传输完毕。 因为TCP是全双工通信的 （1）第一次挥手 因此当主动方发送断开连接的请求（即FIN报文）给被动方时，仅仅代表主动方不会再发送数据报文了，但主动方仍可以接收数据报文。 （2）第二次挥手 被动方此时有可能还有相应的数据报文需要发送，因此需要先发送ACK报文，告知主动方“我知道你想断开连接的请求了”。这样主动方便不会因为没有收到应答而继续发送断开连接的请求（即FIN报文）。 （3）第三次挥手 被动方在处理完数据报文后，便发送给主动方FIN报文；这样可以保证数据通信正常可靠地完成。发送完FIN报文后，被动方进入LAST_ACK阶段（超时等待）。 （4）第四挥手 如果主动方及时发送ACK报文进行连接中断的确认，这时被动方就直接释放连接，进入可用状态。 CLOSE-WAIT 是服务端发出第一次挥手(整体第二次)进入的状态,表示\"我准备关闭了,但是还有自己的事情处理一下,你等我处理完\" 等服务器处理好自己的数据业务,则表示我准备好了,再发送 fin 包 TIME-WAIT 是第四次挥手后,客户端进入的状态,是客户端必要的等待时间,目的是等待:1-服务端的对应端口关闭与客户端发送到服务端的数据到达(可能出现延迟),如果不存在这个步骤就会导致两个问题: 客户端立即关闭后,立即又用同样的端口握手并建立通信,此时上次的连接残留的数据包会被误认为是本次的,造成数据异常 客户端直接关闭后,若服务端重新发送 fin 包,客户端就会回应 RST,会报异常,但是其实是没有问题的。 关于timewait中的MSL 服务器发送fin之后会等待一个最大生存时间MSL，若没有收到ACK则重新发送fin 客户端收到fin之后进入Time-wait状态，持续2MSL时间长度，在此阶段中若再次收到fin则刷新等待时间至2MSL并重发ACK。 timewait的问题 高并发场景下的time-wait会占用很多端口，长时间的time-wait占用资源。 解决方法之一是浏览器直接使用keep-alive防止频繁连接建立和断开。 另一个方法是使用SO_REUSEADDR套接字重复使用端口 ","date":"2022-09-18","objectID":"/computer-network/:2:2","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#time-wait"},{"categories":["Tech"],"content":"四次挥手 TCP连接是双向传输的对等的模式，就是说双方都可以同时向对方发送或接收数据。当有一方要关闭连接时，会发送指令告知对方，我要关闭连接了。这时对方会回一个ACK，此时一个方向的连接关闭。但是另一个方向仍然可以继续传输数据，等到发送完了所有的数据后，会发送一个FIN段来关闭此方向上的连接。接收方发送ACK确认关闭连接。注意，接收到FIN报文的一方只能回复一个ACK, 它是无法马上返回对方一个FIN报文段的，因为结束数据传输的“指令”是上层应用层给出的，我只是一个“搬运工”，我无法了解“上层的意志”。 SYN和FIN段都是会利用重传进行可靠传输的。 客户端发送一个FIN段，并包含一个希望接收者看到的自己当前的序列号K. 同时还包含一个ACK表示确认对方最近一次发过来的数据。 服务端将K值加1作为ACK序号值，表明收到了上一个包。这时上层的应用程序会被告知另一端发起了关闭操作，通常这将引起应用程序发起自己的关闭操作。 服务端发起自己的FIN段，ACK=K+1, Seq=L 客户端确认。ACK=L+1 为什么要四次挥手？ 双方连接需要分别进行关闭，并且保证数据传输完毕。 因为TCP是全双工通信的 （1）第一次挥手 因此当主动方发送断开连接的请求（即FIN报文）给被动方时，仅仅代表主动方不会再发送数据报文了，但主动方仍可以接收数据报文。 （2）第二次挥手 被动方此时有可能还有相应的数据报文需要发送，因此需要先发送ACK报文，告知主动方“我知道你想断开连接的请求了”。这样主动方便不会因为没有收到应答而继续发送断开连接的请求（即FIN报文）。 （3）第三次挥手 被动方在处理完数据报文后，便发送给主动方FIN报文；这样可以保证数据通信正常可靠地完成。发送完FIN报文后，被动方进入LAST_ACK阶段（超时等待）。 （4）第四挥手 如果主动方及时发送ACK报文进行连接中断的确认，这时被动方就直接释放连接，进入可用状态。 CLOSE-WAIT 是服务端发出第一次挥手(整体第二次)进入的状态,表示\"我准备关闭了,但是还有自己的事情处理一下,你等我处理完\" 等服务器处理好自己的数据业务,则表示我准备好了,再发送 fin 包 TIME-WAIT 是第四次挥手后,客户端进入的状态,是客户端必要的等待时间,目的是等待:1-服务端的对应端口关闭与客户端发送到服务端的数据到达(可能出现延迟),如果不存在这个步骤就会导致两个问题: 客户端立即关闭后,立即又用同样的端口握手并建立通信,此时上次的连接残留的数据包会被误认为是本次的,造成数据异常 客户端直接关闭后,若服务端重新发送 fin 包,客户端就会回应 RST,会报异常,但是其实是没有问题的。 关于timewait中的MSL 服务器发送fin之后会等待一个最大生存时间MSL，若没有收到ACK则重新发送fin 客户端收到fin之后进入Time-wait状态，持续2MSL时间长度，在此阶段中若再次收到fin则刷新等待时间至2MSL并重发ACK。 timewait的问题 高并发场景下的time-wait会占用很多端口，长时间的time-wait占用资源。 解决方法之一是浏览器直接使用keep-alive防止频繁连接建立和断开。 另一个方法是使用SO_REUSEADDR套接字重复使用端口 ","date":"2022-09-18","objectID":"/computer-network/:2:2","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#关于timewait中的msl"},{"categories":["Tech"],"content":"四次挥手 TCP连接是双向传输的对等的模式，就是说双方都可以同时向对方发送或接收数据。当有一方要关闭连接时，会发送指令告知对方，我要关闭连接了。这时对方会回一个ACK，此时一个方向的连接关闭。但是另一个方向仍然可以继续传输数据，等到发送完了所有的数据后，会发送一个FIN段来关闭此方向上的连接。接收方发送ACK确认关闭连接。注意，接收到FIN报文的一方只能回复一个ACK, 它是无法马上返回对方一个FIN报文段的，因为结束数据传输的“指令”是上层应用层给出的，我只是一个“搬运工”，我无法了解“上层的意志”。 SYN和FIN段都是会利用重传进行可靠传输的。 客户端发送一个FIN段，并包含一个希望接收者看到的自己当前的序列号K. 同时还包含一个ACK表示确认对方最近一次发过来的数据。 服务端将K值加1作为ACK序号值，表明收到了上一个包。这时上层的应用程序会被告知另一端发起了关闭操作，通常这将引起应用程序发起自己的关闭操作。 服务端发起自己的FIN段，ACK=K+1, Seq=L 客户端确认。ACK=L+1 为什么要四次挥手？ 双方连接需要分别进行关闭，并且保证数据传输完毕。 因为TCP是全双工通信的 （1）第一次挥手 因此当主动方发送断开连接的请求（即FIN报文）给被动方时，仅仅代表主动方不会再发送数据报文了，但主动方仍可以接收数据报文。 （2）第二次挥手 被动方此时有可能还有相应的数据报文需要发送，因此需要先发送ACK报文，告知主动方“我知道你想断开连接的请求了”。这样主动方便不会因为没有收到应答而继续发送断开连接的请求（即FIN报文）。 （3）第三次挥手 被动方在处理完数据报文后，便发送给主动方FIN报文；这样可以保证数据通信正常可靠地完成。发送完FIN报文后，被动方进入LAST_ACK阶段（超时等待）。 （4）第四挥手 如果主动方及时发送ACK报文进行连接中断的确认，这时被动方就直接释放连接，进入可用状态。 CLOSE-WAIT 是服务端发出第一次挥手(整体第二次)进入的状态,表示\"我准备关闭了,但是还有自己的事情处理一下,你等我处理完\" 等服务器处理好自己的数据业务,则表示我准备好了,再发送 fin 包 TIME-WAIT 是第四次挥手后,客户端进入的状态,是客户端必要的等待时间,目的是等待:1-服务端的对应端口关闭与客户端发送到服务端的数据到达(可能出现延迟),如果不存在这个步骤就会导致两个问题: 客户端立即关闭后,立即又用同样的端口握手并建立通信,此时上次的连接残留的数据包会被误认为是本次的,造成数据异常 客户端直接关闭后,若服务端重新发送 fin 包,客户端就会回应 RST,会报异常,但是其实是没有问题的。 关于timewait中的MSL 服务器发送fin之后会等待一个最大生存时间MSL，若没有收到ACK则重新发送fin 客户端收到fin之后进入Time-wait状态，持续2MSL时间长度，在此阶段中若再次收到fin则刷新等待时间至2MSL并重发ACK。 timewait的问题 高并发场景下的time-wait会占用很多端口，长时间的time-wait占用资源。 解决方法之一是浏览器直接使用keep-alive防止频繁连接建立和断开。 另一个方法是使用SO_REUSEADDR套接字重复使用端口 ","date":"2022-09-18","objectID":"/computer-network/:2:2","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#timewait的问题"},{"categories":["Tech"],"content":"TCP 如何保证可靠性 数据分块：应用数据被分割成 TCP 认为最适合发送的数据块。 序列号和确认应答：TCP 给发送的每一个包进行编号，在传输的过程中，每次接收方收到数据后，都会对传输方进行确认应答，即发送 ACK 报文，这个 ACK 报文当中带有对应的确认序列号，告诉发送方成功接收了哪些数据以及下一次的数据从哪里开始发。除此之外，接收方可以根据序列号对数据包进行排序，把有序数据传送给应用层，并丢弃重复的数据。 校验和： TCP 将保持它首部和数据部分的检验和。这是一个端到端的检验和，目的是检测数据在传输过程中的任何变化。如果收到报文段的检验和有差错，TCP 将丢弃这个报文段并且不确认收到此报文段。 流量控制： TCP 连接的双方都有一个固定大小的缓冲空间，发送方发送的数据量不能超过接收端缓冲区的大小。当接收方来不及处理发送方的数据，会提示发送方降低发送的速率，防止产生丢包。TCP 通过滑动窗口协议来支持流量控制机制。 拥塞控制： 当网络某个节点发生拥塞时，减少数据的发送。 ARQ协议： 也是为了实现可靠传输的，它的基本原理就是每发完一个分组就停止发送，等待对方确认。在收到确认后再发下一个分组。 超时重传： 当 TCP 发出一个报文段后，它启动一个定时器，等待目的端确认收到这个报文段。如果超过某个时间还没有收到确认，将重发这个报文段。 停止等待协议 ","date":"2022-09-18","objectID":"/computer-network/:2:3","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#tcp-如何保证可靠性"},{"categories":["Tech"],"content":"TCP流量控制和拥塞控制 流量控制:发送窗口=接收窗口 拥塞控制：发送窗口=min(接收窗口,拥塞窗口)。接收窗口由确认报文中的窗口大小字段确定，拥塞窗口大小由拥塞控制算法确定。 流量控制和拥塞控制的手段相同，都是抑制发送方发送速率；但目的不同，流量控制为了协调发送与接受的速率，拥塞控制是为了避免网络涌入大量报文而造成拥塞。 接收方窗口满了怎么办？ 基于 TCP 流量控制中的滑动窗口协议，我们知道接收方返回给发送方的 ACK 包中会包含自己的接收窗口大小，若接收窗口已满，此时接收方返回给发送方的接收窗口大小为 0，此时发送方会等待接收方发送的窗口大小直到变为非 0 为止，然而，接收方回应的 ACK 包是存在丢失的可能的，为了防止双方一直等待而出现死锁情况，此时就需要坚持计时器来辅助发送方周期性地向接收方查询，以便发现窗口是否变大【坚持计时器参考问题】，当发现窗口大小变为非零时，发送方便继续发送数据。 ","date":"2022-09-18","objectID":"/computer-network/:2:4","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#tcp流量控制和拥塞控制"},{"categories":["Tech"],"content":"TCP流量控制和拥塞控制 流量控制:发送窗口=接收窗口 拥塞控制：发送窗口=min(接收窗口,拥塞窗口)。接收窗口由确认报文中的窗口大小字段确定，拥塞窗口大小由拥塞控制算法确定。 流量控制和拥塞控制的手段相同，都是抑制发送方发送速率；但目的不同，流量控制为了协调发送与接受的速率，拥塞控制是为了避免网络涌入大量报文而造成拥塞。 接收方窗口满了怎么办？ 基于 TCP 流量控制中的滑动窗口协议，我们知道接收方返回给发送方的 ACK 包中会包含自己的接收窗口大小，若接收窗口已满，此时接收方返回给发送方的接收窗口大小为 0，此时发送方会等待接收方发送的窗口大小直到变为非 0 为止，然而，接收方回应的 ACK 包是存在丢失的可能的，为了防止双方一直等待而出现死锁情况，此时就需要坚持计时器来辅助发送方周期性地向接收方查询，以便发现窗口是否变大【坚持计时器参考问题】，当发现窗口大小变为非零时，发送方便继续发送数据。 ","date":"2022-09-18","objectID":"/computer-network/:2:4","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#接收方窗口满了怎么办"},{"categories":["Tech"],"content":"UDP ","date":"2022-09-18","objectID":"/computer-network/:2:5","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#udp"},{"categories":["Tech"],"content":"网络层 ","date":"2022-09-18","objectID":"/computer-network/:3:0","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#网络层"},{"categories":["Tech"],"content":"IP IP 协议（Internet Protocol）又称互联网协议，是支持网间互联的数据包协议。该协议工作在网络层，主要目的就是为了提高网络的可扩展性，和传输层 TCP 相比，IP 协议提供一种无连接/不可靠、尽力而为的数据包传输服务，其与TCP协议（传输控制协议）一起构成了TCP/IP 协议族的核心。IP 协议主要有以下几个作用： 寻址和路由：在IP 数据包中会携带源 IP 地址和目的 IP 地址来标识该数据包的源主机和目的主机。IP 数据报在传输过程中，每个中间节点（IP 网关、路由器）只根据网络地址进行转发，如果中间节点是路由器，则路由器会根据路由表选择合适的路径。IP 协议根据路由选择协议提供的路由信息对 IP 数据报进行转发，直至抵达目的主机。 分段与重组：IP 数据包在传输过程中可能会经过不同的网络，在不同的网络中数据包的最大长度限制是不同的，IP 协议通过给每个 IP 数据包分配一个标识符以及分段与组装的相关信息，使得数据包在不同的网络中能够传输，被分段后的 IP 数据报可以独立地在网络中进行转发，在到达目的主机后由目的主机完成重组工作，恢复出原来的 IP 数据包。 ","date":"2022-09-18","objectID":"/computer-network/:3:1","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#ip"},{"categories":["Tech"],"content":"路由器和交换机的区别 交换机：交换机用于局域网，利用主机的物理地址（MAC 地址）确定数据转发的目的地址，它工作于数据链路层。 路由器：路由器通过数据包中的目的 IP 地址识别不同的网络从而确定数据转发的目的地址，网络号是唯一的。路由器根据路由选择协议和路由表信息从而确定数据的转发路径，直到到达目的网络，它工作于网络层。 网络层协议负责提供主机间的逻辑通信；运输层协议负责提供进程间的逻辑通信。 网络层协议:主机间的逻辑通信；无连接不可靠； 传输层协议：进程间逻辑通信；其中tcp面向连接可靠； ","date":"2022-09-18","objectID":"/computer-network/:3:2","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#路由器和交换机的区别"},{"categories":["Tech"],"content":"路由器和交换机的区别 交换机：交换机用于局域网，利用主机的物理地址（MAC 地址）确定数据转发的目的地址，它工作于数据链路层。 路由器：路由器通过数据包中的目的 IP 地址识别不同的网络从而确定数据转发的目的地址，网络号是唯一的。路由器根据路由选择协议和路由表信息从而确定数据的转发路径，直到到达目的网络，它工作于网络层。 网络层协议负责提供主机间的逻辑通信；运输层协议负责提供进程间的逻辑通信。 网络层协议:主机间的逻辑通信；无连接不可靠； 传输层协议：进程间逻辑通信；其中tcp面向连接可靠； ","date":"2022-09-18","objectID":"/computer-network/:3:2","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#网络层协议负责提供主机间的逻辑通信运输层协议负责提供进程间的逻辑通信"},{"categories":["Tech"],"content":"数据链路层 MAC 地址是数据链路层和物理层使用的地址，是写在网卡上的物理地址。MAC 地址用来定义网络设备的位置。 IP 地址是网络层和以上各层使用的地址，是一种逻辑地址。IP 地址用来区别网络上的计算机。 为什么有MAC还需要IP地址 mac地址是详细的物理地址，可以理解为身份证号，但每次查询时都记录48位的mac地址显然并不现实，ip地址可以理解为家庭住址，可以变化，在一个子网中的ip地址前缀相同，因此减少了路由器所需要的内存。 有IP为什么要MAC 只有当设备连入网络时，才能根据他进入了哪个子网来为其分配 IP 地址，在设备还没有 IP 地址的时候或者在分配 IP 地址的过程中，我们需要 MAC 地址来区分不同的设备。 NAT（网络地址转换）有三种方法：静态转换、动态转换、端口复用。 静态转换：私网地址转换为固定公网地址，当公网地址被占用，则相应设备无法联网。 动态转换：私网地址转换为不固定的公网地址，避免了公网地址被占用的问题。 端口复用：私网地址通过同一个公网地址的不同端口传输。这样隐藏了内部网络中的主机，避免了根据地址的攻击。同时节约了ip地址资源。 ","date":"2022-09-18","objectID":"/computer-network/:4:0","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#数据链路层"},{"categories":["Tech"],"content":"数据链路层 MAC 地址是数据链路层和物理层使用的地址，是写在网卡上的物理地址。MAC 地址用来定义网络设备的位置。 IP 地址是网络层和以上各层使用的地址，是一种逻辑地址。IP 地址用来区别网络上的计算机。 为什么有MAC还需要IP地址 mac地址是详细的物理地址，可以理解为身份证号，但每次查询时都记录48位的mac地址显然并不现实，ip地址可以理解为家庭住址，可以变化，在一个子网中的ip地址前缀相同，因此减少了路由器所需要的内存。 有IP为什么要MAC 只有当设备连入网络时，才能根据他进入了哪个子网来为其分配 IP 地址，在设备还没有 IP 地址的时候或者在分配 IP 地址的过程中，我们需要 MAC 地址来区分不同的设备。 NAT（网络地址转换）有三种方法：静态转换、动态转换、端口复用。 静态转换：私网地址转换为固定公网地址，当公网地址被占用，则相应设备无法联网。 动态转换：私网地址转换为不固定的公网地址，避免了公网地址被占用的问题。 端口复用：私网地址通过同一个公网地址的不同端口传输。这样隐藏了内部网络中的主机，避免了根据地址的攻击。同时节约了ip地址资源。 ","date":"2022-09-18","objectID":"/computer-network/:4:0","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#为什么有mac还需要ip地址"},{"categories":["Tech"],"content":"数据链路层 MAC 地址是数据链路层和物理层使用的地址，是写在网卡上的物理地址。MAC 地址用来定义网络设备的位置。 IP 地址是网络层和以上各层使用的地址，是一种逻辑地址。IP 地址用来区别网络上的计算机。 为什么有MAC还需要IP地址 mac地址是详细的物理地址，可以理解为身份证号，但每次查询时都记录48位的mac地址显然并不现实，ip地址可以理解为家庭住址，可以变化，在一个子网中的ip地址前缀相同，因此减少了路由器所需要的内存。 有IP为什么要MAC 只有当设备连入网络时，才能根据他进入了哪个子网来为其分配 IP 地址，在设备还没有 IP 地址的时候或者在分配 IP 地址的过程中，我们需要 MAC 地址来区分不同的设备。 NAT（网络地址转换）有三种方法：静态转换、动态转换、端口复用。 静态转换：私网地址转换为固定公网地址，当公网地址被占用，则相应设备无法联网。 动态转换：私网地址转换为不固定的公网地址，避免了公网地址被占用的问题。 端口复用：私网地址通过同一个公网地址的不同端口传输。这样隐藏了内部网络中的主机，避免了根据地址的攻击。同时节约了ip地址资源。 ","date":"2022-09-18","objectID":"/computer-network/:4:0","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#有ip为什么要mac"},{"categories":["Tech"],"content":"数据链路层 MAC 地址是数据链路层和物理层使用的地址，是写在网卡上的物理地址。MAC 地址用来定义网络设备的位置。 IP 地址是网络层和以上各层使用的地址，是一种逻辑地址。IP 地址用来区别网络上的计算机。 为什么有MAC还需要IP地址 mac地址是详细的物理地址，可以理解为身份证号，但每次查询时都记录48位的mac地址显然并不现实，ip地址可以理解为家庭住址，可以变化，在一个子网中的ip地址前缀相同，因此减少了路由器所需要的内存。 有IP为什么要MAC 只有当设备连入网络时，才能根据他进入了哪个子网来为其分配 IP 地址，在设备还没有 IP 地址的时候或者在分配 IP 地址的过程中，我们需要 MAC 地址来区分不同的设备。 NAT（网络地址转换）有三种方法：静态转换、动态转换、端口复用。 静态转换：私网地址转换为固定公网地址，当公网地址被占用，则相应设备无法联网。 动态转换：私网地址转换为不固定的公网地址，避免了公网地址被占用的问题。 端口复用：私网地址通过同一个公网地址的不同端口传输。这样隐藏了内部网络中的主机，避免了根据地址的攻击。同时节约了ip地址资源。 ","date":"2022-09-18","objectID":"/computer-network/:4:0","series":null,"tags":["CS"],"title":"Computer Network","uri":"/computer-network/#nat网络地址转换有三种方法静态转换动态转换端口复用"},{"categories":["Tech"],"content":"缓存是什么？ 2.1.1 为什么要使用缓存 一句话:因为速度快,好用 缓存数据存储于代码中,而代码运行在内存中,内存的读写性能远高于磁盘,缓存可以大大降低用户访问并发量带来的服务器读写压力 实际开发过程中,企业的数据量,少则几十万,多则几千万,这么大数据量,如果没有缓存来作为\"避震器\",系统是几乎撑不住的,所以企业会大量运用到缓存技术; 但是缓存也会增加代码复杂度和运营的成本: 2.1.2 如何使用缓存 实际开发中,会构筑多级缓存来使系统运行速度进一步提升,例如:本地缓存与redis中的缓存并发使用 浏览器缓存：主要是存在于浏览器端的缓存 **应用层缓存：**可以分为tomcat本地缓存，比如之前提到的map，或者是使用redis作为缓存 **数据库缓存：**在数据库中有一片空间是 buffer pool，增改查数据都会先加载到mysql的缓存中 **CPU缓存：**当代计算机最大的问题是 cpu性能提升了，但内存读写速度没有跟上，所以为了适应当下的情况，增加了cpu的L1，L2，L3级的缓存 业务逻辑： ","date":"2022-09-15","objectID":"/redis/:0:0","series":null,"tags":null,"title":"Redis","uri":"/redis/#"},{"categories":["Tech"],"content":"缓存是什么？ 2.1.1 为什么要使用缓存 一句话:因为速度快,好用 缓存数据存储于代码中,而代码运行在内存中,内存的读写性能远高于磁盘,缓存可以大大降低用户访问并发量带来的服务器读写压力 实际开发过程中,企业的数据量,少则几十万,多则几千万,这么大数据量,如果没有缓存来作为\"避震器\",系统是几乎撑不住的,所以企业会大量运用到缓存技术; 但是缓存也会增加代码复杂度和运营的成本: 2.1.2 如何使用缓存 实际开发中,会构筑多级缓存来使系统运行速度进一步提升,例如:本地缓存与redis中的缓存并发使用 浏览器缓存：主要是存在于浏览器端的缓存 **应用层缓存：**可以分为tomcat本地缓存，比如之前提到的map，或者是使用redis作为缓存 **数据库缓存：**在数据库中有一片空间是 buffer pool，增改查数据都会先加载到mysql的缓存中 **CPU缓存：**当代计算机最大的问题是 cpu性能提升了，但内存读写速度没有跟上，所以为了适应当下的情况，增加了cpu的L1，L2，L3级的缓存 业务逻辑： ","date":"2022-09-15","objectID":"/redis/:0:0","series":null,"tags":null,"title":"Redis","uri":"/redis/#211-为什么要使用缓存"},{"categories":["Tech"],"content":"缓存是什么？ 2.1.1 为什么要使用缓存 一句话:因为速度快,好用 缓存数据存储于代码中,而代码运行在内存中,内存的读写性能远高于磁盘,缓存可以大大降低用户访问并发量带来的服务器读写压力 实际开发过程中,企业的数据量,少则几十万,多则几千万,这么大数据量,如果没有缓存来作为\"避震器\",系统是几乎撑不住的,所以企业会大量运用到缓存技术; 但是缓存也会增加代码复杂度和运营的成本: 2.1.2 如何使用缓存 实际开发中,会构筑多级缓存来使系统运行速度进一步提升,例如:本地缓存与redis中的缓存并发使用 浏览器缓存：主要是存在于浏览器端的缓存 **应用层缓存：**可以分为tomcat本地缓存，比如之前提到的map，或者是使用redis作为缓存 **数据库缓存：**在数据库中有一片空间是 buffer pool，增改查数据都会先加载到mysql的缓存中 **CPU缓存：**当代计算机最大的问题是 cpu性能提升了，但内存读写速度没有跟上，所以为了适应当下的情况，增加了cpu的L1，L2，L3级的缓存 业务逻辑： ","date":"2022-09-15","objectID":"/redis/:0:0","series":null,"tags":null,"title":"Redis","uri":"/redis/#212-如何使用缓存"},{"categories":["Tech"],"content":"2.3 缓存更新策略 缓存更新是redis为了节约内存而设计出来的一个东西，主要是因为内存数据宝贵，当我们向redis插入太多数据，此时就可能会导致缓存中的数据过多，所以redis会对部分数据进行更新，或者把他叫为淘汰更合适。 **内存淘汰：**redis自动进行，当redis内存达到咱们设定的max-memery的时候，会自动触发淘汰机制，淘汰掉一些不重要的数据(可以自己设置策略方式) **超时剔除：**当我们给redis设置了过期时间ttl之后，redis会将超时的数据进行删除，方便咱们继续使用缓存 **主动更新：**我们可以手动调用方法把缓存删掉，通常用于解决缓存和数据库不一致问题 三种解决方案： Cache Aside Pattern 人工编码方式：缓存调用者在更新完数据库后再去更新缓存，也称之为双写方案（常用） Read/Write Through Pattern : 由系统本身完成，数据库与缓存的问题交由系统本身去处理 Write Behind Caching Pattern ：调用者只操作缓存，其他线程去异步处理数据库，实现最终一致（复杂，一致性难以保证） 如果采用第一个方案，那么假设我们每次操作数据库后，都操作缓存，但是中间如果没有人查询，那么这个更新动作实际上只有最后一次生效，中间的更新动作意义并不大，我们可以把缓存删除，等待再次查询时，将缓存中的数据加载出来 删除缓存还是更新缓存？ 更新缓存：每次更新数据库都更新缓存，无效写操作较多 删除缓存：更新数据库时让缓存失效，查询时再更新缓存（选这个） 如何保证缓存与数据库的操作的同时成功或失败？ 单体系统，将缓存与数据库操作放在一个事务 分布式系统，利用TCC等分布式事务方案 应该具体操作缓存还是操作数据库，我们应当是先操作数据库，再删除缓存，原因在于，如果你选择第一种方案，在两个线程并发来访问时，假设线程1先来，他先把缓存删了，此时线程2过来，他查询缓存数据并不存在，此时他写入缓存，当他写入缓存后，线程1再执行更新动作时，实际上写入的就是旧的数据，新的数据被旧数据覆盖了。 先操作缓存还是先操作数据库？ 先删除缓存，再操作数据库 先操作数据库，再删除缓存（线程不安全的可能性较低）使用这种方案的同时用超时剔除作为兜底 ","date":"2022-09-15","objectID":"/redis/:0:1","series":null,"tags":null,"title":"Redis","uri":"/redis/#23-缓存更新策略"},{"categories":["Tech"],"content":"2.5 缓存穿透问题的解决思路 缓存穿透 ：缓存穿透是指客户端请求的数据在缓存中和数据库中都不存在，这样缓存永远不会生效，这些请求都会打到数据库。 常见的解决方案有两种： 缓存空对象 （防止重复请求空对象 可以设置ttl） 优点：实现简单，维护方便 缺点： 额外的内存消耗 可能造成短期的不一致 布隆过滤 （一个过滤器先判断是否key存在） 优点：内存占用较少，没有多余key 缺点： 实现复杂 存在误判可能 缓存穿透产生的原因是什么？ 用户请求的数据在缓存中和数据库中都不存在，不断发起这样的请求，给数据库带来巨大压力 缓存穿透的解决方案有哪些？ 缓存null值 布隆过滤 增强id的复杂度，避免被猜测id规律 做好数据的基础格式校验 加强用户权限校验 做好热点参数的限流 ","date":"2022-09-15","objectID":"/redis/:0:2","series":null,"tags":null,"title":"Redis","uri":"/redis/#25-缓存穿透问题的解决思路"},{"categories":["Tech"],"content":"2.7 缓存雪崩问题及解决思路 缓存雪崩是指在同一时段大量的缓存key同时失效或者Redis服务宕机，导致大量请求到达数据库，带来巨大压力。 解决方案： 给不同的Key的TTL添加随机值 利用Redis集群提高服务的可用性 给缓存业务添加降级限流策略 给业务添加多级缓存 ","date":"2022-09-15","objectID":"/redis/:0:3","series":null,"tags":null,"title":"Redis","uri":"/redis/#27-缓存雪崩问题及解决思路"},{"categories":["Tech"],"content":"2.8 缓存击穿问题及解决思路 缓存击穿问题也叫热点Key问题，就是一个被高并发访问并且缓存重建业务较复杂的key突然失效了，无数的请求访问会在瞬间给数据库带来巨大的冲击。 常见的解决方案有两种： 互斥锁 逻辑过期 逻辑分析：假设线程1在查询缓存之后，本来应该去查询数据库，然后把这个数据重新加载到缓存的，此时只要线程1走完这个逻辑，其他线程就都能从缓存中加载这些数据了，但是假设在线程1没有走完的时候，后续的线程2，线程3，线程4同时过来访问当前这个方法， 那么这些线程都不能从缓存中查询到数据，那么他们就会同一时刻来访问查询缓存，都没查到，接着同一时间去访问数据库，同时的去执行数据库代码，对数据库访问压力过大 解决方案一、使用锁来解决： 因为锁能实现互斥性。假设线程过来，只能一个人一个人的来访问数据库，从而避免对于数据库访问压力过大，但这也会影响查询的性能，因为此时会让查询的性能从并行变成了串行，我们可以采用tryLock方法 + double check来解决这样的问题。 假设现在线程1过来访问，他查询缓存没有命中，但是此时他获得到了锁的资源，那么线程1就会一个人去执行逻辑，假设现在线程2过来，线程2在执行过程中，并没有获得到锁，那么线程2就可以进行到休眠，直到线程1把锁释放后，线程2获得到锁，然后再来执行逻辑，此时就能够从缓存中拿到数据了。 解决方案二、逻辑过期方案 方案分析：我们之所以会出现这个缓存击穿问题，主要原因是在于我们对key设置了过期时间，假设我们不设置过期时间，其实就不会有缓存击穿的问题，但是不设置过期时间，这样数据不就一直占用我们内存了吗，我们可以采用逻辑过期方案。 我们把过期时间设置在 redis的value中，注意：这个过期时间并不会直接作用于redis，而是我们后续通过逻辑去处理。假设线程1去查询缓存，然后从value中判断出来当前的数据已经过期了，此时线程1去获得互斥锁，那么其他线程会进行阻塞，获得了锁的线程他会开启一个 线程去进行 以前的重构数据的逻辑，直到新开的线程完成这个逻辑后，才释放锁， 而线程1直接进行返回，假设现在线程3过来访问，由于线程线程2持有着锁，所以线程3无法获得锁，线程3也直接返回数据，只有等到新开的线程2把重建数据构建完后，其他线程才能走返回正确的数据。 这种方案巧妙在于，异步的构建缓存，缺点在于在构建完缓存之前，返回的都是脏数据。 一致性vs可用性 互斥锁的简单实现： 核心思路就是利用redis的setnx方法来表示获取锁，该方法含义是redis中如果没有这个key，则插入成功，返回1，在stringRedisTemplate中返回true， 如果有这个key则插入失败，则返回0，在stringRedisTemplate返回false，我们可以通过true，或者是false，来表示是否有线程成功插入key，成功插入的key的线程我们认为他就是获得到锁的线程。 ","date":"2022-09-15","objectID":"/redis/:0:4","series":null,"tags":null,"title":"Redis","uri":"/redis/#28-缓存击穿问题及解决思路"},{"categories":["Tech"],"content":"3.1、封装Redis工具类 基于StringRedisTemplate封装一个缓存工具类，满足下列需求： 方法1：将任意Java对象序列化为json并存储在string类型的key中，并且可以设置TTL过期时间 方法2：将任意Java对象序列化为json并存储在string类型的key中，并且可以设置逻辑过期时间，用于处理缓 存击穿问题 方法3：根据指定的key查询缓存，并反序列化为指定类型，利用缓存空值的方式解决缓存穿透问题 方法4：根据指定的key查询缓存，并反序列化为指定类型，需要利用逻辑过期解决缓存击穿问题 将逻辑进行封装 封装过程中的一些知识点： 函数式编程和泛型 Redis is a key-value database. SQL vs NoSQL 存储方式 关系型数据库基于磁盘进行存储，会有大量的磁盘IO，对性能有一定影响 非关系型数据库，他们的操作更多的是依赖于内存来操作，内存的读写速度会非常快，性能自然会好一些 扩展性 关系型数据库集群模式一般是主从，主从数据一致，起到数据备份的作用，称为垂直扩展。 非关系型数据库可以将数据拆分，存储在不同机器上，可以保存海量数据，解决内存大小有限的问题。称为水平扩展。 关系型数据库因为表之间存在关联关系，如果做水平扩展会给数据查询带来很多麻烦 ","date":"2022-09-15","objectID":"/redis/:0:5","series":null,"tags":null,"title":"Redis","uri":"/redis/#31封装redis工具类"},{"categories":["Tech"],"content":"Redis特征： 键值（key-value）型，value支持多种不同数据结构，功能丰富 单线程，每个命令具备原子性 低延迟，速度快（基于内存.IO多路复用.良好的编码）。 支持数据持久化 支持主从集群.分片集群 支持多语言客户端 ","date":"2022-09-15","objectID":"/redis/:0:6","series":null,"tags":null,"title":"Redis","uri":"/redis/#redis特征"},{"categories":["Tech"],"content":"4.Redis常见命令 ","date":"2022-09-15","objectID":"/redis/:1:0","series":null,"tags":null,"title":"Redis","uri":"/redis/#4redis常见命令"},{"categories":["Tech"],"content":"4.1 Redis数据结构介绍 Redis是一个key-value的数据库，key一般是String类型，不过value的类型多种多样： 命令不要死记，学会查询就好啦 ","date":"2022-09-15","objectID":"/redis/:1:1","series":null,"tags":null,"title":"Redis","uri":"/redis/#41-redis数据结构介绍"},{"categories":["Tech"],"content":"4.2 Redis 通用命令 通用指令是部分数据类型的，都可以使用的指令，常见的有： KEYS：查看符合模板的所有key DEL：删除一个指定的key EXISTS：判断key是否存在 EXPIRE：给一个key设置有效期，有效期到期时该key会被自动删除 TTL：查看一个KEY的剩余有效期 KEYS 127.0.0.1:6379\u003e keys * 1) \"name\" 2) \"age\" 127.0.0.1:6379\u003e # 查询以a开头的key 127.0.0.1:6379\u003e keys a* 1) \"age\" 127.0.0.1:6379\u003e 在生产环境下，不推荐使用keys 命令，因为这个命令在key过多的情况下，效率不高 DEL 127.0.0.1:6379\u003e help del DEL key [key ...] summary: Delete a key since: 1.0.0 group: generic 127.0.0.1:6379\u003e del name #删除单个 (integer) 1 #成功删除1个 127.0.0.1:6379\u003e keys * 1) \"age\" 127.0.0.1:6379\u003e MSET k1 v1 k2 v2 k3 v3 #批量添加数据 OK 127.0.0.1:6379\u003e keys * 1) \"k3\" 2) \"k2\" 3) \"k1\" 4) \"age\" 127.0.0.1:6379\u003e del k1 k2 k3 k4 (integer) 3 #此处返回的是成功删除的key，由于redis中只有k1,k2,k3 所以只成功删除3个，最终返回 127.0.0.1:6379\u003e 127.0.0.1:6379\u003e keys * #再查询全部的key 1) \"age\" #只剩下一个了 127.0.0.1:6379\u003e EXISTS 127.0.0.1:6379\u003e help EXISTS EXISTS key [key ...] summary: Determine if a key exists since: 1.0.0 group: generic 127.0.0.1:6379\u003e exists age (integer) 1 127.0.0.1:6379\u003e exists name (integer) 0 EXPIRE 内存非常宝贵，对于一些数据，我们应当给他一些过期时间，当过期时间到了之后，他就会自动被删除~ 127.0.0.1:6379\u003e expire age 10 (integer) 1 127.0.0.1:6379\u003e ttl age (integer) 8 127.0.0.1:6379\u003e ttl age (integer) 6 127.0.0.1:6379\u003e ttl age (integer) -2 127.0.0.1:6379\u003e ttl age (integer) -2 #当这个key过期了，那么此时查询出来就是-2 127.0.0.1:6379\u003e keys * (empty list or set) 127.0.0.1:6379\u003e set age 10 #如果没有设置过期时间 OK 127.0.0.1:6379\u003e ttl age (integer) -1 # ttl的返回值就是-1 ","date":"2022-09-15","objectID":"/redis/:1:2","series":null,"tags":null,"title":"Redis","uri":"/redis/#42-redis-通用命令"},{"categories":["Tech"],"content":"4.3 Redis命令-String命令 String类型，也就是字符串类型，是Redis中最简单的存储类型。 其value是字符串，不过根据字符串的格式不同，又可以分为3类： string：普通字符串 int：整数类型，可以做自增.自减操作 float：浮点类型，可以做自增.自减操作 String的常见命令有： SET：添加或者修改已经存在的一个String类型的键值对 GET：根据key获取String类型的value MSET：批量添加多个String类型的键值对 MGET：根据多个key获取多个String类型的value INCR：让一个整型的key自增1 INCRBY:让一个整型的key自增并指定步长，例如：incrby num 2 让num值自增2 INCRBYFLOAT：让一个浮点类型的数字自增并指定步长 SETNX：添加一个String类型的键值对，前提是这个key不存在，否则不执行 SETEX：添加一个String类型的键值对，并且指定有效期 贴心小提示：以上命令除了INCRBYFLOAT 都是常用命令 SET 和GET: 如果key不存在则是新增，如果存在则是修改 127.0.0.1:6379\u003e set name Rose //原来不存在 OK 127.0.0.1:6379\u003e get name \"Rose\" 127.0.0.1:6379\u003e set name Jack //原来存在，就是修改 OK 127.0.0.1:6379\u003e get name \"Jack\" MSET和MGET 127.0.0.1:6379\u003e MSET k1 v1 k2 v2 k3 v3 OK 127.0.0.1:6379\u003e MGET name age k1 k2 k3 1) \"Jack\" //之前存在的name 2) \"10\" //之前存在的age 3) \"v1\" 4) \"v2\" 5) \"v3\" INCR和INCRBY和DECY 127.0.0.1:6379\u003e get age \"10\" 127.0.0.1:6379\u003e incr age //增加1 (integer) 11 127.0.0.1:6379\u003e get age //获得age \"11\" 127.0.0.1:6379\u003e incrby age 2 //一次增加2 (integer) 13 //返回目前的age的值 127.0.0.1:6379\u003e incrby age 2 (integer) 15 127.0.0.1:6379\u003e incrby age -1 //也可以增加负数，相当于减 (integer) 14 127.0.0.1:6379\u003e incrby age -2 //一次减少2个 (integer) 12 127.0.0.1:6379\u003e DECR age //相当于 incr 负数，减少正常用法 (integer) 11 127.0.0.1:6379\u003e get age \"11\" SETNX 127.0.0.1:6379\u003e help setnx SETNX key value summary: Set the value of a key, only if the key does not exist since: 1.0.0 group: string 127.0.0.1:6379\u003e set name Jack //设置名称 OK 127.0.0.1:6379\u003e setnx name lisi //如果key不存在，则添加成功 (integer) 0 127.0.0.1:6379\u003e get name //由于name已经存在，所以lisi的操作失败 \"Jack\" 127.0.0.1:6379\u003e setnx name2 lisi //name2 不存在，所以操作成功 (integer) 1 127.0.0.1:6379\u003e get name2 \"lisi\" SETEX 127.0.0.1:6379\u003e setex name 10 jack OK 127.0.0.1:6379\u003e ttl name (integer) 8 127.0.0.1:6379\u003e ttl name (integer) 7 127.0.0.1:6379\u003e ttl name (integer) 5 ","date":"2022-09-15","objectID":"/redis/:1:3","series":null,"tags":null,"title":"Redis","uri":"/redis/#43-redis命令-string命令"},{"categories":["Tech"],"content":"4.4 Redis命令-Key的层级结构 Redis没有类似MySQL中的Table的概念，我们该如何区分不同类型的key呢？ 例如，需要存储用户.商品信息到redis，有一个用户id是1，有一个商品id恰好也是1，此时如果使用id作为key，那就会冲突了，该怎么办？ 我们可以通过给key添加前缀加以区分，不过这个前缀不是随便加的，有一定的规范：Redis的key允许有多个单词形成层级结构，多个单词之间用’:‘隔开，格式如下： project:service:type:id 这个格式并非固定，也可以根据自己的需求来删除或添加词条。例如我们的项目名称叫 heima，有user和product两种不同类型的数据，我们可以这样定义key： user相关的key：heima:user:1 product相关的key：heima:product:1 如果Value是一个Java对象，例如一个User对象，则可以将对象序列化为JSON字符串后存储： KEY VALUE heima:user:1 {“id”:1, “name”: “Jack”, “age”: 21} heima:product:1 {“id”:1, “name”: “小米11”, “price”: 4999} 一旦我们向redis采用这样的方式存储，那么在可视化界面中，redis会以层级结构来进行存储，形成类似于这样的结构，更加方便Redis获取数据 ","date":"2022-09-15","objectID":"/redis/:1:4","series":null,"tags":null,"title":"Redis","uri":"/redis/#44-redis命令-key的层级结构"},{"categories":["Tech"],"content":"4.5 Redis命令-Hash命令 Hash类型的常见命令 HSET key field value：添加或者修改hash类型key的field的值 HGET key field：获取一个hash类型key的field的值 HMSET：批量添加多个hash类型key的field的值 HMGET：批量获取多个hash类型key的field的值 HGETALL：获取一个hash类型的key中的所有的field和value HKEYS：获取一个hash类型的key中的所有的field HINCRBY:让一个hash类型key的字段值自增并指定步长 HSETNX：添加一个hash类型的key的field值，前提是这个field不存在，否则不执行 贴心小提示：哈希结构也是我们以后实际开发中常用的命令哟 HSET和HGET 127.0.0.1:6379\u003e HSET heima:user:3 name Lucy//大key是 heima:user:3 小key是name，小value是Lucy (integer) 1 127.0.0.1:6379\u003e HSET heima:user:3 age 21// 如果操作不存在的数据，则是新增 (integer) 1 127.0.0.1:6379\u003e HSET heima:user:3 age 17 //如果操作存在的数据，则是修改 (integer) 0 127.0.0.1:6379\u003e HGET heima:user:3 name \"Lucy\" 127.0.0.1:6379\u003e HGET heima:user:3 age \"17\" HMSET和HMGET 127.0.0.1:6379\u003e HMSET heima:user:4 name HanMeiMei OK 127.0.0.1:6379\u003e HMSET heima:user:4 name LiLei age 20 sex man OK 127.0.0.1:6379\u003e HMGET heima:user:4 name age sex 1) \"LiLei\" 2) \"20\" 3) \"man\" HGETALL 127.0.0.1:6379\u003e HGETALL heima:user:4 1) \"name\" 2) \"LiLei\" 3) \"age\" 4) \"20\" 5) \"sex\" 6) \"man\" HKEYS和HVALS 127.0.0.1:6379\u003e HKEYS heima:user:4 1) \"name\" 2) \"age\" 3) \"sex\" 127.0.0.1:6379\u003e HVALS heima:user:4 1) \"LiLei\" 2) \"20\" 3) \"man\" HINCRBY 127.0.0.1:6379\u003e HINCRBY heima:user:4 age 2 (integer) 22 127.0.0.1:6379\u003e HVALS heima:user:4 1) \"LiLei\" 2) \"22\" 3) \"man\" 127.0.0.1:6379\u003e HINCRBY heima:user:4 age -2 (integer) 20 HSETNX 127.0.0.1:6379\u003e HSETNX heima:user4 sex woman (integer) 1 127.0.0.1:6379\u003e HGETALL heima:user:3 1) \"name\" 2) \"Lucy\" 3) \"age\" 4) \"17\" 127.0.0.1:6379\u003e HSETNX heima:user:3 sex woman (integer) 1 127.0.0.1:6379\u003e HGETALL heima:user:3 1) \"name\" 2) \"Lucy\" 3) \"age\" 4) \"17\" 5) \"sex\" 6) \"woman\" ","date":"2022-09-15","objectID":"/redis/:1:5","series":null,"tags":null,"title":"Redis","uri":"/redis/#45-redis命令-hash命令"},{"categories":["Tech"],"content":"4.6 Redis命令-List命令 Redis中的List类型与Java中的LinkedList类似，可以看做是一个双向链表结构。既可以支持正向检索和也可以支持反向检索。 特征也与LinkedList类似： 有序 元素可以重复 插入和删除快 查询速度一般 常用来存储一个有序数据，例如：朋友圈点赞列表，评论列表等。 ","date":"2022-09-15","objectID":"/redis/:1:6","series":null,"tags":null,"title":"Redis","uri":"/redis/#46-redis命令-list命令"},{"categories":["Tech"],"content":"4.7 Redis命令-Set命令 Redis的Set结构与Java中的HashSet类似，可以看做是一个value为null的HashMap。因为也是一个hash表，因此具备与HashSet类似的特征： 无序 元素不可重复 查找快 支持交集.并集.差集等功能 ","date":"2022-09-15","objectID":"/redis/:1:7","series":null,"tags":null,"title":"Redis","uri":"/redis/#47-redis命令-set命令"},{"categories":["Tech"],"content":"4.8 Redis命令-SortedSet类型 Redis的SortedSet是一个可排序的set集合，与Java中的TreeSet有些类似，但底层数据结构却差别很大。SortedSet中的每一个元素都带有一个score属性，可以基于score属性对元素排序，底层的实现是一个跳表（SkipList）加 hash表。 SortedSet具备下列特性： 可排序 元素不重复 查询速度快 因为SortedSet的可排序特性，经常被用来实现排行榜这样的功能。 SpringDataRedis RedisTemplate可以接收任意Object作为值写入Redis： 只不过写入前会把Object序列化为字节形式，默认是采用JDK序列化，得到的结果是这样的： 缺点： 可读性差 内存占用较大 我们可以自定义RedisTemplate的序列化方式，代码如下： @Configuration public class RedisConfig { @Bean public RedisTemplate\u003cString, Object\u003e redisTemplate(RedisConnectionFactory connectionFactory){ // 创建RedisTemplate对象 RedisTemplate\u003cString, Object\u003e template = new RedisTemplate\u003c\u003e(); // 设置连接工厂 template.setConnectionFactory(connectionFactory); // 创建JSON序列化工具 GenericJackson2JsonRedisSerializer jsonRedisSerializer = new GenericJackson2JsonRedisSerializer(); // 设置Key的序列化 template.setKeySerializer(RedisSerializer.string()); template.setHashKeySerializer(RedisSerializer.string()); // 设置Value的序列化 template.setValueSerializer(jsonRedisSerializer); template.setHashValueSerializer(jsonRedisSerializer); // 返回 return template; } } 这里采用了JSON序列化来代替默认的JDK序列化方式。最终结果如图： 整体可读性有了很大提升，并且能将Java对象自动的序列化为JSON字符串，并且查询时能自动把JSON反序列化为Java对象。不过，其中记录了序列化时对应的class名称，目的是为了查询时实现自动反序列化。这会带来额外的内存开销。 但是这里的内存消耗过大，解决方法是直接全部使用string序列化优化空间。 RedisTemplate的两种序列化实践方案： 方案一： 自定义RedisTemplate 修改RedisTemplate的序列化器为GenericJackson2JsonRedisSerializer 方案二： 使用StringRedisTemplate 写入Redis时，手动把对象序列化为JSON 读取Redis时，手动把读取到的JSON反序列化为对象 写入redis时序列化为json对象 /** * javaBean 转化成json字符串 * @param obj * @return * @throws Exception */ public static String toJson(Object obj) { if(obj instanceof Integer || obj instanceof Long || obj instanceof Float || obj instanceof Double || obj instanceof Boolean || obj instanceof String){ return String.valueOf(obj); } try { return objMapper.writeValueAsString(obj); } catch (JsonProcessingException e) { log.error(\"转化成json字符串\",e); } return null; } 读取redis时反序列化json为对象 /** * Json字符串转化成对象 * @param jsonString * @param clazz * @param \u003cT\u003e * @return * @throws Exception */ public static \u003cT\u003e T toObj(String jsonString, Class\u003cT\u003e clazz) { objMapper.configure(DeserializationFeature.ACCEPT_SINGLE_VALUE_AS_ARRAY, true); try { return objMapper.readValue(jsonString, clazz); } catch (IOException e) { log.error(\"Json字符串转化成对象出错\",e); } return null; } 注：写项目的时候可以把这两种方法直接实现在工具类里面 方便读取写入调用 注：ThreadLocal类保证变量值存在于一个线程中，可以进行用户登录信息存储，保证线程安全。 ThreadLocal 和 syncronized区别 https://zhuanlan.zhihu.com/p/31537708#:~:text=synchronized%20%E6%98%AF%E9%87%87%E7%94%A8%E9%94%81%E7%9A%84,%E7%A9%BA%E9%97%B4%E6%9D%A5%E8%BE%BE%E5%88%B0%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E3%80%82 ","date":"2022-09-15","objectID":"/redis/:1:8","series":null,"tags":null,"title":"Redis","uri":"/redis/#48-redis命令-sortedset类型"},{"categories":["Tech"],"content":"ThreadLocal 与 Sy ","date":"2022-09-15","objectID":"/redis/:1:9","series":null,"tags":null,"title":"Redis","uri":"/redis/#threadlocal-与-sy"},{"categories":["Tech"],"content":"nchronized区别 相同：ThreadLocal和线程同步机制都是为了解决多线程中相同变量的访问冲突问题。 不同：Synchronized同步机制采用了“以时间换空间”的方式，仅提供一份变量，让不同的线程排队访问；而ThreadLocal采用了“以空间换时间”的方式，每一个线程都提供了一份变量，因此可以同时访问而互不影响。 以时间换空间-\u003e即枷锁方式，某个区域代码或变量只有一份节省了内存，但是会形成很多线程等待现象，因此浪费了时间而节省了空间。 以空间换时间-\u003e为每一个线程提供一份变量，多开销一些内存，但是呢线程不用等待，可以一起执行而相互之间没有影响。 小结：ThreadLocal是解决线程安全问题一个很好的思路，它通过为每个线程提供一个独立的变量副本解决了变量并发访问的冲突问题。在很多情况下，ThreadLocal比直接使用synchronized同步机制解决线程安全问题更简单，更方便，且结果程序拥有更高的并发性。 ","date":"2022-09-15","objectID":"/redis/:1:11","series":null,"tags":null,"title":"Redis","uri":"/redis/#nchronized区别"},{"categories":["Tech"],"content":"秒杀案例 ","date":"2022-09-15","objectID":"/redis/:2:0","series":null,"tags":null,"title":"Redis","uri":"/redis/#秒杀案例"},{"categories":["Tech"],"content":"生成全局id Redis的全局唯一id的保证： 符号位+时间戳+序列号 符号位永远为0 时间戳：31bit，以秒为单位，可以使用69年 序列号：32bit，秒内的计数器，支持每秒产生2^32个不同ID 也可以使用snowflake算法 生成id测试： 知识小贴士：关于countdownlatch countdownlatch名为信号枪：主要的作用是同步协调在多线程的等待于唤醒问题 我们如果没有CountDownLatch ，那么由于程序是异步的，当异步程序没有执行完时，主线程就已经执行完了，然后我们期望的是分线程全部走完之后，主线程再走，所以我们此时需要使用到CountDownLatch CountDownLatch 中有两个最重要的方法 1、countDown 2、await await 方法 是阻塞方法，我们担心分线程没有执行完时，main线程就先执行，所以使用await可以让main线程阻塞，那么什么时候main线程不再阻塞呢？当CountDownLatch 内部维护的 变量变为0时，就不再阻塞，直接放行，那么什么时候CountDownLatch 维护的变量变为0 呢，我们只需要调用一次countDown ，内部变量就减少1，我们让分线程和变量绑定， 执行完一个分线程就减少一个变量，当分线程全部走完，CountDownLatch 维护的变量就是0，此时await就不再阻塞，统计出来的时间也就是所有分线程执行完后的时间。 @Test void testIdWorker() throws InterruptedException { CountDownLatch latch = new CountDownLatch(300); Runnable task = () -\u003e { for (int i = 0; i \u003c 100; i++) { long id = redisIdWorker.nextId(\"order\"); System.out.println(\"id = \" + id); } latch.countDown(); }; long begin = System.currentTimeMillis(); for (int i = 0; i \u003c 300; i++) { es.submit(task); } latch.await();//主线程等待 long end = System.currentTimeMillis(); System.out.println(\"time = \" + (end - begin)); } 秒杀核心逻辑： 防止超卖 悲观锁： 悲观锁可以实现对于数据的串行化执行，比如syn，和lock都是悲观锁的代表，同时，悲观锁中又可以再细分为公平锁，非公平锁，可重入锁，等等 悲观锁性能一般 乐观锁： 乐观锁：会有一个版本号，每次操作数据会对版本号+1，再提交回数据时，会去校验是否比之前的版本大1 ，如果大1 ，则进行操作成功，这套机制的核心逻辑在于，如果在操作过程中，版本号只比原来大1 ，那么就意味着操作过程中没有人对他进行过修改，他的操作就是安全的，如果不大1，则数据被修改过，当然乐观锁还有一些变种的处理方式比如cas 乐观锁的典型代表：就是cas，利用cas进行无锁化机制加锁，var5 是操作前读取的内存值，while中的var1+var2 是预估值，如果预估值 == 内存值，则代表中间没有被人修改过，此时就将新值去替换 内存值 其中do while 是为了在操作失败时，再次进行自旋操作，即把之前的逻辑再操作一次。 性能好可以直接判断\u003e0 ","date":"2022-09-15","objectID":"/redis/:2:1","series":null,"tags":null,"title":"Redis","uri":"/redis/#生成全局id"},{"categories":["Tech"],"content":"生成全局id Redis的全局唯一id的保证： 符号位+时间戳+序列号 符号位永远为0 时间戳：31bit，以秒为单位，可以使用69年 序列号：32bit，秒内的计数器，支持每秒产生2^32个不同ID 也可以使用snowflake算法 生成id测试： 知识小贴士：关于countdownlatch countdownlatch名为信号枪：主要的作用是同步协调在多线程的等待于唤醒问题 我们如果没有CountDownLatch ，那么由于程序是异步的，当异步程序没有执行完时，主线程就已经执行完了，然后我们期望的是分线程全部走完之后，主线程再走，所以我们此时需要使用到CountDownLatch CountDownLatch 中有两个最重要的方法 1、countDown 2、await await 方法 是阻塞方法，我们担心分线程没有执行完时，main线程就先执行，所以使用await可以让main线程阻塞，那么什么时候main线程不再阻塞呢？当CountDownLatch 内部维护的 变量变为0时，就不再阻塞，直接放行，那么什么时候CountDownLatch 维护的变量变为0 呢，我们只需要调用一次countDown ，内部变量就减少1，我们让分线程和变量绑定， 执行完一个分线程就减少一个变量，当分线程全部走完，CountDownLatch 维护的变量就是0，此时await就不再阻塞，统计出来的时间也就是所有分线程执行完后的时间。 @Test void testIdWorker() throws InterruptedException { CountDownLatch latch = new CountDownLatch(300); Runnable task = () - { for (int i = 0; i 0 ","date":"2022-09-15","objectID":"/redis/:2:1","series":null,"tags":null,"title":"Redis","uri":"/redis/#防止超卖"},{"categories":["Tech"],"content":"集群环境下的并发问题 通过加锁可以解决在单机情况下的一人一单安全问题，但是在集群模式下就不行了。 1、我们将服务启动两份，端口分别为8081和8082： 2、然后修改nginx的conf目录下的nginx.conf文件，配置反向代理和负载均衡： 有关锁失效原因分析 由于现在我们部署了多个tomcat，每个tomcat都有一个属于自己的jvm，那么假设在服务器A的tomcat内部，有两个线程，这两个线程由于使用的是同一份代码，那么他们的锁对象是同一个，是可以实现互斥的，但是如果现在是服务器B的tomcat内部，又有两个线程，但是他们的锁对象写的虽然和服务器A一样，但是锁对象却不是同一个，所以线程3和线程4可以实现互斥，但是却无法和线程1和线程2实现互斥，这就是 集群环境下，syn锁失效的原因，在这种情况下，我们就需要使用分布式锁来解决这个问题。 ","date":"2022-09-15","objectID":"/redis/:2:2","series":null,"tags":null,"title":"Redis","uri":"/redis/#集群环境下的并发问题"},{"categories":["Tech"],"content":"分布式锁 ","date":"2022-09-15","objectID":"/redis/:2:3","series":null,"tags":null,"title":"Redis","uri":"/redis/#分布式锁"},{"categories":["Tech"],"content":"4.1 、基本原理和实现方式对比 分布式锁：满足分布式系统或集群模式下多进程可见并且互斥的锁。 分布式锁的核心思想就是让大家都使用同一把锁，只要大家使用的是同一把锁，那么我们就能锁住线程，不让线程进行，让程序串行执行，这就是分布式锁的核心思路 分布式锁他应该满足一些什么样的条件呢？ 可见性：多个线程都能看到相同的结果，注意：这个地方说的可见性并不是并发编程中指的内存可见性，只是说多个进程之间都能感知到变化的意思 互斥：互斥是分布式锁的最基本的条件，使得程序串行执行 高可用：程序不易崩溃，时时刻刻都保证较高的可用性 高性能：由于加锁本身就让性能降低，所有对于分布式锁本身需要他就较高的加锁性能和释放锁性能 安全性：安全也是程序中必不可少的一环 分布式锁的实现方式： ","date":"2022-09-15","objectID":"/redis/:2:4","series":null,"tags":null,"title":"Redis","uri":"/redis/#41-基本原理和实现方式对比"},{"categories":["Tech"],"content":"4.2 Redis分布式锁的实现核心思路 实现分布式锁时需要实现的两个基本方法： 获取锁： 互斥：确保只能有一个线程获取锁 非阻塞：尝试一次，成功返回true，失败返回false 释放锁： 手动释放 超时释放：获取锁时添加一个超时时间 ","date":"2022-09-15","objectID":"/redis/:2:5","series":null,"tags":null,"title":"Redis","uri":"/redis/#42-redis分布式锁的实现核心思路"},{"categories":["Tech"],"content":"Redisson Redisson是一个在Redis的基础上实现的Java驻内存数据网格（In-Memory Data Grid）。它不仅提供了一系列的分布式的Java常用对象，还提供了许多分布式服务，其中就包含了各种分布式锁的实现。 ","date":"2022-09-15","objectID":"/redis/:2:6","series":null,"tags":null,"title":"Redis","uri":"/redis/#redisson"},{"categories":["Tech"],"content":"5.3 分布式锁-redission可重入锁原理 在Lock锁中，他是借助于底层的一个voaltile的一个state变量来记录重入的状态的，比如当前没有人持有这把锁，那么state=0，假如有人持有这把锁，那么state=1，如果持有这把锁的人再次持有这把锁，那么state就会+1 ，如果是对于synchronized而言，他在c语言代码中会有一个count，原理和state类似，也是重入一次就加一，释放一次就-1 ，直到减少成0 时，表示当前这把锁没有被人持有。 申请锁时使用hash 存储lock并且计数器+1 释放时value-1，若value=0则删除锁 ","date":"2022-09-15","objectID":"/redis/:2:7","series":null,"tags":null,"title":"Redis","uri":"/redis/#53-分布式锁-redission可重入锁原理"},{"categories":["Tech"],"content":"5.4 分布式锁-redission锁重试和WatchDog机制 重试和超时释放 watchdog：每隔一段时间进行超时时间的更新 ","date":"2022-09-15","objectID":"/redis/:2:8","series":null,"tags":null,"title":"Redis","uri":"/redis/#54-分布式锁-redission锁重试和watchdog机制"},{"categories":["Tech"],"content":"5.5 分布式锁-redission锁的MutiLock原理 主从一致性问题 为了提高redis的可用性，我们会搭建集群或者主从，现在以主从为例 此时我们去写命令，写在主机上， 主机会将数据同步给从机，但是假设在主机还没有来得及把数据写入到从机去的时候，此时主机宕机，哨兵会发现主机宕机，并且选举一个slave变成master，而此时新的master中实际上并没有锁信息，此时锁信息就已经丢掉了。 为了解决这个问题，redission提出来了MutiLock锁，使用这把锁咱们就不使用主从了，每个节点的地位都是一样的， 这把锁加锁的逻辑需要写入到每一个主丛节点上，只有所有的服务器都写入成功，此时才是加锁成功，假设现在某个节点挂了，那么他去获得锁的时候，只要有一个节点拿不到，都不能算是加锁成功，就保证了加锁的可靠性。（multilock） 小总结： 秒杀业务的优化思路是什么？ 先利用Redis完成库存余量、一人一单判断，完成抢单业务 再将下单业务放入阻塞队列，利用独立线程异步下单 基于阻塞队列的异步秒杀存在哪些问题？ 内存限制问题 数据安全问题 HyperLogLog: 存储Unique value 使用的内存非常非常小 ","date":"2022-09-15","objectID":"/redis/:2:9","series":null,"tags":null,"title":"Redis","uri":"/redis/#55-分布式锁-redission锁的mutilock原理"},{"categories":["Tech"],"content":"分布式缓存 Redis持久化： redis采用master-slave架构进行主从同步，主从同步主要分为 全量同步和增量同步 简述全量同步和增量同步区别？ 全量同步：master将完整内存数据生成RDB，发送RDB到slave。后续命令则记录在repl_baklog，逐个发送给slave。 增量同步：slave提交自己的offset到master，master获取repl_baklog中从offset之后的命令给slave 什么时候执行全量同步？ slave节点第一次连接master节点时 slave节点断开时间太久，repl_baklog中的offset已经被覆盖时 什么时候执行增量同步？ slave节点断开又恢复，并且在repl_baklog中能找到offset时 ","date":"2022-09-15","objectID":"/redis/:3:0","series":null,"tags":null,"title":"Redis","uri":"/redis/#分布式缓存"},{"categories":["Tech"],"content":"分布式缓存 Redis持久化： redis采用master-slave架构进行主从同步，主从同步主要分为 全量同步和增量同步 简述全量同步和增量同步区别？ 全量同步：master将完整内存数据生成RDB，发送RDB到slave。后续命令则记录在repl_baklog，逐个发送给slave。 增量同步：slave提交自己的offset到master，master获取repl_baklog中从offset之后的命令给slave 什么时候执行全量同步？ slave节点第一次连接master节点时 slave节点断开时间太久，repl_baklog中的offset已经被覆盖时 什么时候执行增量同步？ slave节点断开又恢复，并且在repl_baklog中能找到offset时 ","date":"2022-09-15","objectID":"/redis/:3:0","series":null,"tags":null,"title":"Redis","uri":"/redis/#简述全量同步和增量同步区别"},{"categories":["Tech"],"content":"分布式缓存 Redis持久化： redis采用master-slave架构进行主从同步，主从同步主要分为 全量同步和增量同步 简述全量同步和增量同步区别？ 全量同步：master将完整内存数据生成RDB，发送RDB到slave。后续命令则记录在repl_baklog，逐个发送给slave。 增量同步：slave提交自己的offset到master，master获取repl_baklog中从offset之后的命令给slave 什么时候执行全量同步？ slave节点第一次连接master节点时 slave节点断开时间太久，repl_baklog中的offset已经被覆盖时 什么时候执行增量同步？ slave节点断开又恢复，并且在repl_baklog中能找到offset时 ","date":"2022-09-15","objectID":"/redis/:3:0","series":null,"tags":null,"title":"Redis","uri":"/redis/#什么时候执行全量同步"},{"categories":["Tech"],"content":"分布式缓存 Redis持久化： redis采用master-slave架构进行主从同步，主从同步主要分为 全量同步和增量同步 简述全量同步和增量同步区别？ 全量同步：master将完整内存数据生成RDB，发送RDB到slave。后续命令则记录在repl_baklog，逐个发送给slave。 增量同步：slave提交自己的offset到master，master获取repl_baklog中从offset之后的命令给slave 什么时候执行全量同步？ slave节点第一次连接master节点时 slave节点断开时间太久，repl_baklog中的offset已经被覆盖时 什么时候执行增量同步？ slave节点断开又恢复，并且在repl_baklog中能找到offset时 ","date":"2022-09-15","objectID":"/redis/:3:0","series":null,"tags":null,"title":"Redis","uri":"/redis/#什么时候执行增量同步"},{"categories":["Tech"],"content":"Sentinel 使用哨兵监控集群 Sentinel的三个作用是什么？ 监控 故障转移 通知 Sentinel如何判断一个redis实例是否健康？ 每隔1秒发送一次ping命令，如果超过一定时间没有相向则认为是主观下线 如果大多数sentinel都认为实例主观下线，则判定服务下线 故障转移步骤有哪些？ 首先选定一个slave作为新的master，执行slaveof no one 然后让所有节点都执行slaveof 新master 修改故障节点配置，添加slaveof 新master ","date":"2022-09-15","objectID":"/redis/:3:1","series":null,"tags":null,"title":"Redis","uri":"/redis/#sentinel"},{"categories":["Tech"],"content":"Sentinel 使用哨兵监控集群 Sentinel的三个作用是什么？ 监控 故障转移 通知 Sentinel如何判断一个redis实例是否健康？ 每隔1秒发送一次ping命令，如果超过一定时间没有相向则认为是主观下线 如果大多数sentinel都认为实例主观下线，则判定服务下线 故障转移步骤有哪些？ 首先选定一个slave作为新的master，执行slaveof no one 然后让所有节点都执行slaveof 新master 修改故障节点配置，添加slaveof 新master ","date":"2022-09-15","objectID":"/redis/:3:1","series":null,"tags":null,"title":"Redis","uri":"/redis/#sentinel的三个作用是什么"},{"categories":["Tech"],"content":"Sentinel 使用哨兵监控集群 Sentinel的三个作用是什么？ 监控 故障转移 通知 Sentinel如何判断一个redis实例是否健康？ 每隔1秒发送一次ping命令，如果超过一定时间没有相向则认为是主观下线 如果大多数sentinel都认为实例主观下线，则判定服务下线 故障转移步骤有哪些？ 首先选定一个slave作为新的master，执行slaveof no one 然后让所有节点都执行slaveof 新master 修改故障节点配置，添加slaveof 新master ","date":"2022-09-15","objectID":"/redis/:3:1","series":null,"tags":null,"title":"Redis","uri":"/redis/#sentinel如何判断一个redis实例是否健康"},{"categories":["Tech"],"content":"Sentinel 使用哨兵监控集群 Sentinel的三个作用是什么？ 监控 故障转移 通知 Sentinel如何判断一个redis实例是否健康？ 每隔1秒发送一次ping命令，如果超过一定时间没有相向则认为是主观下线 如果大多数sentinel都认为实例主观下线，则判定服务下线 故障转移步骤有哪些？ 首先选定一个slave作为新的master，执行slaveof no one 然后让所有节点都执行slaveof 新master 修改故障节点配置，添加slaveof 新master ","date":"2022-09-15","objectID":"/redis/:3:1","series":null,"tags":null,"title":"Redis","uri":"/redis/#故障转移步骤有哪些"},{"categories":["Tech"],"content":"JVM stands for Java Virtual Machine. Write Once Run Anywhere Automatic memory management and garbage collection mechanism Array out of bounds detection ","date":"2022-09-02","objectID":"/jvm/:0:0","series":null,"tags":null,"title":"JVM","uri":"/jvm/#"},{"categories":["Tech"],"content":"Program counter register To store the address of next command in JVM. Features: Thread safe. Every thread has its own time slices and will be executed when it is its turn. No memory overflow problem. ","date":"2022-09-02","objectID":"/jvm/:0:1","series":null,"tags":null,"title":"JVM","uri":"/jvm/#program-counter-register"},{"categories":["Tech"],"content":"Program counter register To store the address of next command in JVM. Features: Thread safe. Every thread has its own time slices and will be executed when it is its turn. No memory overflow problem. ","date":"2022-09-02","objectID":"/jvm/:0:1","series":null,"tags":null,"title":"JVM","uri":"/jvm/#features"},{"categories":["Tech"],"content":"JVM stacks Allocated per thread as well Every thread in the JVM needs memory, it is called frame. Every stack is made of multiple frames, it corresponds to the memory when the method is running. Every thread has only one **active **frame. Which means when the method is running it will be pushed into the stack and popped out when finished. Question: Are stacks involved in GC process? No, the finished method will be popped out when finished, no need to do grabage collection. Is it true that the bigger the stack is the better it works? No. Because physical memory is fixed. Bigger memory supports more recursion but less threads. Thread safe? If 2 threads modify the same static variable. It’s not thread safe. But for local variables, it’s thread safe. Stack overflow There are too many frames in the stack(infinite recursion). Or the single frame takes too much memory(not common). The memory for a Java Virtual Machine stack does not need to be contiguous. The variables that are not created objects and reference variables are stored in stack. int i = 1; Test test = new Test();// Variable test is in stack, while the true new Test() is in heap. Test myTest = new Test(1, 1, 2);// 1 1 2 are stored in stack. ","date":"2022-09-02","objectID":"/jvm/:0:2","series":null,"tags":null,"title":"JVM","uri":"/jvm/#jvm-stacks"},{"categories":["Tech"],"content":"JVM stacks Allocated per thread as well Every thread in the JVM needs memory, it is called frame. Every stack is made of multiple frames, it corresponds to the memory when the method is running. Every thread has only one **active **frame. Which means when the method is running it will be pushed into the stack and popped out when finished. Question: Are stacks involved in GC process? No, the finished method will be popped out when finished, no need to do grabage collection. Is it true that the bigger the stack is the better it works? No. Because physical memory is fixed. Bigger memory supports more recursion but less threads. Thread safe? If 2 threads modify the same static variable. It’s not thread safe. But for local variables, it’s thread safe. Stack overflow There are too many frames in the stack(infinite recursion). Or the single frame takes too much memory(not common). The memory for a Java Virtual Machine stack does not need to be contiguous. The variables that are not created objects and reference variables are stored in stack. int i = 1; Test test = new Test();// Variable test is in stack, while the true new Test() is in heap. Test myTest = new Test(1, 1, 2);// 1 1 2 are stored in stack. ","date":"2022-09-02","objectID":"/jvm/:0:2","series":null,"tags":null,"title":"JVM","uri":"/jvm/#question"},{"categories":["Tech"],"content":"Native Method stacks Some native methods written in C or C++ are stored here to directly interact with underlying hardware. clone wait notify hashCode ","date":"2022-09-02","objectID":"/jvm/:0:3","series":null,"tags":null,"title":"JVM","uri":"/jvm/#native-method-stacks"},{"categories":["Tech"],"content":"Heap A place to store almost all of the objects.(new Objects …)(largest memory consumer) Arrays lists Features: The Java Virtual Machine has a heap that is **shared **among all Java Virtual Machine threads. The main GC area. -Xmx -Xms to distribute heap memory(default 1/64 of the physical memory). Heap put of memory case: adding infinite number of elements into a list. jps, jmao and jconsloe are used to diagnose the heap memory. ","date":"2022-09-02","objectID":"/jvm/:0:4","series":null,"tags":null,"title":"JVM","uri":"/jvm/#heap"},{"categories":["Tech"],"content":"Heap A place to store almost all of the objects.(new Objects …)(largest memory consumer) Arrays lists Features: The Java Virtual Machine has a heap that is **shared **among all Java Virtual Machine threads. The main GC area. -Xmx -Xms to distribute heap memory(default 1/64 of the physical memory). Heap put of memory case: adding infinite number of elements into a list. jps, jmao and jconsloe are used to diagnose the heap memory. ","date":"2022-09-02","objectID":"/jvm/:0:4","series":null,"tags":null,"title":"JVM","uri":"/jvm/#features-1"},{"categories":["Tech"],"content":"Method Area Created when the JVM is on.The memory for a Method area does not need to be contiguous. Shutting down JVM will release the memory of MA. The size of MA deetermines how many classes cam be stored in the system. Too many definitions of classes will tigger out of memory error. this area stores all the compiled code. It stores per-class structures such as the run-time constant pool, field and method data, and the code for methods and constructors. Class, Interface, enum, annotation Type Information: (class, interface, enum, annotation)JVM must store the following type information in the method area: the full and valid name of this type (full name = package name. Class name) the full valid name of the direct parent class of this type (neither interface nor java. lang.Object has a parent class) Access modifiers of this type (a subset of public, abstract, and final) an ordered list of direct interfaces of this type Domain Information (member variables): the JVM must store information about all domains of the type and the declaration order of the domains in the method area. Domain information includes domain name, domain type, and domain modifier (a subset of public, private, protected, static, final, volatile, and transient). Method information: the JVM must store the following information about all methods, including the declaration order as the domain information. Method method return type (or void) number and type of method parameters in sequence method modifiers (a subset of public, private, protected, static, final,synchronized, native, and abstract) Method bytecode (bytecodes), operand stack, local variable table, and size (except abstract and native methods) exception tables (except abstract and native methods) Jdk 1.6 and earlier: permanent generation (static variables are stored on permanent generation), string constant pool (1.6 in method area) Jdk 1.7: it has permanent generation, but has been gradually “removed from permanent generation”. String constant pools and static variables are removed and stored in the heap. Jdk 1.8 and later: no permanent generation, constant pool 1.8 in the meta space. However, static variables and string constant pools are still in the heap. Why replace permanent generation with meta space? It is difficult to determine the size of the permanent generation setting space: (1) the permanent generation parameter setting is too small. In some scenarios, if there are too many dynamically loaded classes, OOM in the Perm area is easy to occur. For example, in an actual Web project, because there are many function points, many classes need to be dynamically loaded continuously during the running process, and fatal errors often occur. (2) the permanent generation parameter is too large, resulting in a waste of space. (3) by default, the size of the metadata is limited by the local memory) it is difficult to perform optimization in permanent generation: (garbage collection in the method area mainly recycles two parts: discarded constants in the constant pool and types that are no longer used, while classes that are no longer used or class loaders are complex to recycle, and full gc takes a long time) StringTable is put in heap after Java 1.8. StringTable为什么要调整? jdk7中将StringTable放到了堆空间中。因为永久代的回收效率很低,在full gc的时候才能触发。而full gc是老年代的空间不足、永久代不足才会触发。这就导致StringTable回收效率不高,而我们开发中会有大量的字符串被创建,回收效率低,导致永久代内存不足,放到堆里,能及时回收内存。 object.intern()// If the object already exists in the Stringtable, it will return the var in the ST. // if not, it will try to add the object into CP **In java 1.6, intern() simply make a copy of the object and store it into the CP, not object itself. Overflow: permanent generation before 1.8 metaspace after 1.8 Constant pool .class file not real object JVM use constant pool as a table to find class name, methods and variables. Run-time Constant Pool When a class is loaded, its constants will be put into runtime constant pool. Their real address will be recorded in the RTCP(in constant p","date":"2022-09-02","objectID":"/jvm/:0:5","series":null,"tags":null,"title":"JVM","uri":"/jvm/#method-area"},{"categories":["Tech"],"content":"Method Area Created when the JVM is on.The memory for a Method area does not need to be contiguous. Shutting down JVM will release the memory of MA. The size of MA deetermines how many classes cam be stored in the system. Too many definitions of classes will tigger out of memory error. this area stores all the compiled code. It stores per-class structures such as the run-time constant pool, field and method data, and the code for methods and constructors. Class, Interface, enum, annotation Type Information: (class, interface, enum, annotation)JVM must store the following type information in the method area: the full and valid name of this type (full name = package name. Class name) the full valid name of the direct parent class of this type (neither interface nor java. lang.Object has a parent class) Access modifiers of this type (a subset of public, abstract, and final) an ordered list of direct interfaces of this type Domain Information (member variables): the JVM must store information about all domains of the type and the declaration order of the domains in the method area. Domain information includes domain name, domain type, and domain modifier (a subset of public, private, protected, static, final, volatile, and transient). Method information: the JVM must store the following information about all methods, including the declaration order as the domain information. Method method return type (or void) number and type of method parameters in sequence method modifiers (a subset of public, private, protected, static, final,synchronized, native, and abstract) Method bytecode (bytecodes), operand stack, local variable table, and size (except abstract and native methods) exception tables (except abstract and native methods) Jdk 1.6 and earlier: permanent generation (static variables are stored on permanent generation), string constant pool (1.6 in method area) Jdk 1.7: it has permanent generation, but has been gradually “removed from permanent generation”. String constant pools and static variables are removed and stored in the heap. Jdk 1.8 and later: no permanent generation, constant pool 1.8 in the meta space. However, static variables and string constant pools are still in the heap. Why replace permanent generation with meta space? It is difficult to determine the size of the permanent generation setting space: (1) the permanent generation parameter setting is too small. In some scenarios, if there are too many dynamically loaded classes, OOM in the Perm area is easy to occur. For example, in an actual Web project, because there are many function points, many classes need to be dynamically loaded continuously during the running process, and fatal errors often occur. (2) the permanent generation parameter is too large, resulting in a waste of space. (3) by default, the size of the metadata is limited by the local memory) it is difficult to perform optimization in permanent generation: (garbage collection in the method area mainly recycles two parts: discarded constants in the constant pool and types that are no longer used, while classes that are no longer used or class loaders are complex to recycle, and full gc takes a long time) StringTable is put in heap after Java 1.8. StringTable为什么要调整? jdk7中将StringTable放到了堆空间中。因为永久代的回收效率很低,在full gc的时候才能触发。而full gc是老年代的空间不足、永久代不足才会触发。这就导致StringTable回收效率不高,而我们开发中会有大量的字符串被创建,回收效率低,导致永久代内存不足,放到堆里,能及时回收内存。 object.intern()// If the object already exists in the Stringtable, it will return the var in the ST. // if not, it will try to add the object into CP **In java 1.6, intern() simply make a copy of the object and store it into the CP, not object itself. Overflow: permanent generation before 1.8 metaspace after 1.8 Constant pool .class file not real object JVM use constant pool as a table to find class name, methods and variables. Run-time Constant Pool When a class is loaded, its constants will be put into runtime constant pool. Their real address will be recorded in the RTCP(in constant p","date":"2022-09-02","objectID":"/jvm/:0:5","series":null,"tags":null,"title":"JVM","uri":"/jvm/#constant-pool"},{"categories":["Tech"],"content":"Method Area Created when the JVM is on.The memory for a Method area does not need to be contiguous. Shutting down JVM will release the memory of MA. The size of MA deetermines how many classes cam be stored in the system. Too many definitions of classes will tigger out of memory error. this area stores all the compiled code. It stores per-class structures such as the run-time constant pool, field and method data, and the code for methods and constructors. Class, Interface, enum, annotation Type Information: (class, interface, enum, annotation)JVM must store the following type information in the method area: the full and valid name of this type (full name = package name. Class name) the full valid name of the direct parent class of this type (neither interface nor java. lang.Object has a parent class) Access modifiers of this type (a subset of public, abstract, and final) an ordered list of direct interfaces of this type Domain Information (member variables): the JVM must store information about all domains of the type and the declaration order of the domains in the method area. Domain information includes domain name, domain type, and domain modifier (a subset of public, private, protected, static, final, volatile, and transient). Method information: the JVM must store the following information about all methods, including the declaration order as the domain information. Method method return type (or void) number and type of method parameters in sequence method modifiers (a subset of public, private, protected, static, final,synchronized, native, and abstract) Method bytecode (bytecodes), operand stack, local variable table, and size (except abstract and native methods) exception tables (except abstract and native methods) Jdk 1.6 and earlier: permanent generation (static variables are stored on permanent generation), string constant pool (1.6 in method area) Jdk 1.7: it has permanent generation, but has been gradually “removed from permanent generation”. String constant pools and static variables are removed and stored in the heap. Jdk 1.8 and later: no permanent generation, constant pool 1.8 in the meta space. However, static variables and string constant pools are still in the heap. Why replace permanent generation with meta space? It is difficult to determine the size of the permanent generation setting space: (1) the permanent generation parameter setting is too small. In some scenarios, if there are too many dynamically loaded classes, OOM in the Perm area is easy to occur. For example, in an actual Web project, because there are many function points, many classes need to be dynamically loaded continuously during the running process, and fatal errors often occur. (2) the permanent generation parameter is too large, resulting in a waste of space. (3) by default, the size of the metadata is limited by the local memory) it is difficult to perform optimization in permanent generation: (garbage collection in the method area mainly recycles two parts: discarded constants in the constant pool and types that are no longer used, while classes that are no longer used or class loaders are complex to recycle, and full gc takes a long time) StringTable is put in heap after Java 1.8. StringTable为什么要调整? jdk7中将StringTable放到了堆空间中。因为永久代的回收效率很低,在full gc的时候才能触发。而full gc是老年代的空间不足、永久代不足才会触发。这就导致StringTable回收效率不高,而我们开发中会有大量的字符串被创建,回收效率低,导致永久代内存不足,放到堆里,能及时回收内存。 object.intern()// If the object already exists in the Stringtable, it will return the var in the ST. // if not, it will try to add the object into CP **In java 1.6, intern() simply make a copy of the object and store it into the CP, not object itself. Overflow: permanent generation before 1.8 metaspace after 1.8 Constant pool .class file not real object JVM use constant pool as a table to find class name, methods and variables. Run-time Constant Pool When a class is loaded, its constants will be put into runtime constant pool. Their real address will be recorded in the RTCP(in constant p","date":"2022-09-02","objectID":"/jvm/:0:5","series":null,"tags":null,"title":"JVM","uri":"/jvm/#run-time-constant-pool"},{"categories":["Tech"],"content":"Method Area Created when the JVM is on.The memory for a Method area does not need to be contiguous. Shutting down JVM will release the memory of MA. The size of MA deetermines how many classes cam be stored in the system. Too many definitions of classes will tigger out of memory error. this area stores all the compiled code. It stores per-class structures such as the run-time constant pool, field and method data, and the code for methods and constructors. Class, Interface, enum, annotation Type Information: (class, interface, enum, annotation)JVM must store the following type information in the method area: the full and valid name of this type (full name = package name. Class name) the full valid name of the direct parent class of this type (neither interface nor java. lang.Object has a parent class) Access modifiers of this type (a subset of public, abstract, and final) an ordered list of direct interfaces of this type Domain Information (member variables): the JVM must store information about all domains of the type and the declaration order of the domains in the method area. Domain information includes domain name, domain type, and domain modifier (a subset of public, private, protected, static, final, volatile, and transient). Method information: the JVM must store the following information about all methods, including the declaration order as the domain information. Method method return type (or void) number and type of method parameters in sequence method modifiers (a subset of public, private, protected, static, final,synchronized, native, and abstract) Method bytecode (bytecodes), operand stack, local variable table, and size (except abstract and native methods) exception tables (except abstract and native methods) Jdk 1.6 and earlier: permanent generation (static variables are stored on permanent generation), string constant pool (1.6 in method area) Jdk 1.7: it has permanent generation, but has been gradually “removed from permanent generation”. String constant pools and static variables are removed and stored in the heap. Jdk 1.8 and later: no permanent generation, constant pool 1.8 in the meta space. However, static variables and string constant pools are still in the heap. Why replace permanent generation with meta space? It is difficult to determine the size of the permanent generation setting space: (1) the permanent generation parameter setting is too small. In some scenarios, if there are too many dynamically loaded classes, OOM in the Perm area is easy to occur. For example, in an actual Web project, because there are many function points, many classes need to be dynamically loaded continuously during the running process, and fatal errors often occur. (2) the permanent generation parameter is too large, resulting in a waste of space. (3) by default, the size of the metadata is limited by the local memory) it is difficult to perform optimization in permanent generation: (garbage collection in the method area mainly recycles two parts: discarded constants in the constant pool and types that are no longer used, while classes that are no longer used or class loaders are complex to recycle, and full gc takes a long time) StringTable is put in heap after Java 1.8. StringTable为什么要调整? jdk7中将StringTable放到了堆空间中。因为永久代的回收效率很低,在full gc的时候才能触发。而full gc是老年代的空间不足、永久代不足才会触发。这就导致StringTable回收效率不高,而我们开发中会有大量的字符串被创建,回收效率低,导致永久代内存不足,放到堆里,能及时回收内存。 object.intern()// If the object already exists in the Stringtable, it will return the var in the ST. // if not, it will try to add the object into CP **In java 1.6, intern() simply make a copy of the object and store it into the CP, not object itself. Overflow: permanent generation before 1.8 metaspace after 1.8 Constant pool .class file not real object JVM use constant pool as a table to find class name, methods and variables. Run-time Constant Pool When a class is loaded, its constants will be put into runtime constant pool. Their real address will be recorded in the RTCP(in constant p","date":"2022-09-02","objectID":"/jvm/:0:5","series":null,"tags":null,"title":"JVM","uri":"/jvm/#stringtablein-heap"},{"categories":["Tech"],"content":"Method Area Created when the JVM is on.The memory for a Method area does not need to be contiguous. Shutting down JVM will release the memory of MA. The size of MA deetermines how many classes cam be stored in the system. Too many definitions of classes will tigger out of memory error. this area stores all the compiled code. It stores per-class structures such as the run-time constant pool, field and method data, and the code for methods and constructors. Class, Interface, enum, annotation Type Information: (class, interface, enum, annotation)JVM must store the following type information in the method area: the full and valid name of this type (full name = package name. Class name) the full valid name of the direct parent class of this type (neither interface nor java. lang.Object has a parent class) Access modifiers of this type (a subset of public, abstract, and final) an ordered list of direct interfaces of this type Domain Information (member variables): the JVM must store information about all domains of the type and the declaration order of the domains in the method area. Domain information includes domain name, domain type, and domain modifier (a subset of public, private, protected, static, final, volatile, and transient). Method information: the JVM must store the following information about all methods, including the declaration order as the domain information. Method method return type (or void) number and type of method parameters in sequence method modifiers (a subset of public, private, protected, static, final,synchronized, native, and abstract) Method bytecode (bytecodes), operand stack, local variable table, and size (except abstract and native methods) exception tables (except abstract and native methods) Jdk 1.6 and earlier: permanent generation (static variables are stored on permanent generation), string constant pool (1.6 in method area) Jdk 1.7: it has permanent generation, but has been gradually “removed from permanent generation”. String constant pools and static variables are removed and stored in the heap. Jdk 1.8 and later: no permanent generation, constant pool 1.8 in the meta space. However, static variables and string constant pools are still in the heap. Why replace permanent generation with meta space? It is difficult to determine the size of the permanent generation setting space: (1) the permanent generation parameter setting is too small. In some scenarios, if there are too many dynamically loaded classes, OOM in the Perm area is easy to occur. For example, in an actual Web project, because there are many function points, many classes need to be dynamically loaded continuously during the running process, and fatal errors often occur. (2) the permanent generation parameter is too large, resulting in a waste of space. (3) by default, the size of the metadata is limited by the local memory) it is difficult to perform optimization in permanent generation: (garbage collection in the method area mainly recycles two parts: discarded constants in the constant pool and types that are no longer used, while classes that are no longer used or class loaders are complex to recycle, and full gc takes a long time) StringTable is put in heap after Java 1.8. StringTable为什么要调整? jdk7中将StringTable放到了堆空间中。因为永久代的回收效率很低,在full gc的时候才能触发。而full gc是老年代的空间不足、永久代不足才会触发。这就导致StringTable回收效率不高,而我们开发中会有大量的字符串被创建,回收效率低,导致永久代内存不足,放到堆里,能及时回收内存。 object.intern()// If the object already exists in the Stringtable, it will return the var in the ST. // if not, it will try to add the object into CP **In java 1.6, intern() simply make a copy of the object and store it into the CP, not object itself. Overflow: permanent generation before 1.8 metaspace after 1.8 Constant pool .class file not real object JVM use constant pool as a table to find class name, methods and variables. Run-time Constant Pool When a class is loaded, its constants will be put into runtime constant pool. Their real address will be recorded in the RTCP(in constant p","date":"2022-09-02","objectID":"/jvm/:0:5","series":null,"tags":null,"title":"JVM","uri":"/jvm/#stringtable-vs-constant-pool"},{"categories":["Tech"],"content":"Method Area Created when the JVM is on.The memory for a Method area does not need to be contiguous. Shutting down JVM will release the memory of MA. The size of MA deetermines how many classes cam be stored in the system. Too many definitions of classes will tigger out of memory error. this area stores all the compiled code. It stores per-class structures such as the run-time constant pool, field and method data, and the code for methods and constructors. Class, Interface, enum, annotation Type Information: (class, interface, enum, annotation)JVM must store the following type information in the method area: the full and valid name of this type (full name = package name. Class name) the full valid name of the direct parent class of this type (neither interface nor java. lang.Object has a parent class) Access modifiers of this type (a subset of public, abstract, and final) an ordered list of direct interfaces of this type Domain Information (member variables): the JVM must store information about all domains of the type and the declaration order of the domains in the method area. Domain information includes domain name, domain type, and domain modifier (a subset of public, private, protected, static, final, volatile, and transient). Method information: the JVM must store the following information about all methods, including the declaration order as the domain information. Method method return type (or void) number and type of method parameters in sequence method modifiers (a subset of public, private, protected, static, final,synchronized, native, and abstract) Method bytecode (bytecodes), operand stack, local variable table, and size (except abstract and native methods) exception tables (except abstract and native methods) Jdk 1.6 and earlier: permanent generation (static variables are stored on permanent generation), string constant pool (1.6 in method area) Jdk 1.7: it has permanent generation, but has been gradually “removed from permanent generation”. String constant pools and static variables are removed and stored in the heap. Jdk 1.8 and later: no permanent generation, constant pool 1.8 in the meta space. However, static variables and string constant pools are still in the heap. Why replace permanent generation with meta space? It is difficult to determine the size of the permanent generation setting space: (1) the permanent generation parameter setting is too small. In some scenarios, if there are too many dynamically loaded classes, OOM in the Perm area is easy to occur. For example, in an actual Web project, because there are many function points, many classes need to be dynamically loaded continuously during the running process, and fatal errors often occur. (2) the permanent generation parameter is too large, resulting in a waste of space. (3) by default, the size of the metadata is limited by the local memory) it is difficult to perform optimization in permanent generation: (garbage collection in the method area mainly recycles two parts: discarded constants in the constant pool and types that are no longer used, while classes that are no longer used or class loaders are complex to recycle, and full gc takes a long time) StringTable is put in heap after Java 1.8. StringTable为什么要调整? jdk7中将StringTable放到了堆空间中。因为永久代的回收效率很低,在full gc的时候才能触发。而full gc是老年代的空间不足、永久代不足才会触发。这就导致StringTable回收效率不高,而我们开发中会有大量的字符串被创建,回收效率低,导致永久代内存不足,放到堆里,能及时回收内存。 object.intern()// If the object already exists in the Stringtable, it will return the var in the ST. // if not, it will try to add the object into CP **In java 1.6, intern() simply make a copy of the object and store it into the CP, not object itself. Overflow: permanent generation before 1.8 metaspace after 1.8 Constant pool .class file not real object JVM use constant pool as a table to find class name, methods and variables. Run-time Constant Pool When a class is loaded, its constants will be put into runtime constant pool. Their real address will be recorded in the RTCP(in constant p","date":"2022-09-02","objectID":"/jvm/:0:5","series":null,"tags":null,"title":"JVM","uri":"/jvm/#gc-in-na"},{"categories":["Tech"],"content":"Direct Memory(Not controlled by JVM) Direct memory is a memory section which Java code and OS can both access(efficient) Requesting direct memory needs to be conducted manually. Release is manually conducted not automatic GC process. Reason: more efficient and better I/O performance. ","date":"2022-09-02","objectID":"/jvm/:0:6","series":null,"tags":null,"title":"JVM","uri":"/jvm/#direct-memorynot-controlled-by-jvm"},{"categories":["Tech"],"content":"JVM GC Reference counting(Deprecated) if a and b have circular dependency, the memory won’t be collected. Reachability Analysis: Use algo to determine whether an object in heap is referenced, if not, recycle it. Need a GC root first(the object that definitely can’t be recycled.). Object that can be GC root: Object in stack Static reference in NA Reference contants in NA JNI reference objects in local stack. reference types: Strong: new object, a = b (When it is not referenced by any object, it will be recycled.) Even if the memory is short, JVM will throw outofmemory instead of recycle it. Soft: will be recycled when memory is not enough. SoftReference Weak: WeakReference will be recycled when GC. Soft and weak reference will be sent into reference list when the object is gone(list is used to store soft and weak reference when we neeed to remove them). PhatomRefence and final(Use finalize() method to init) reference need to be used with a list. They will be sent into reference list when the object is gone. Cleaner will regularly scan the list to remove reference. 如上图，B对象不再引用A4对象。这是终结器对象就会被放入引用队列中，引用队列会根据它，找到它所引用的对象。然后调用被引用对象的finalize()方法。调用以后，该对象就可以被垃圾回收了。 引用队列 软引用和弱引用可以配合引用队列(也可以不配合)： 在弱引用和虚引用所引用的对象被回收以后，会将这些引用放入引用队列中，方便一起回收这些软/弱引用对象。 虚引用和终结器引用必须配合引用队列： 虚引用和终结器引用在使用时会关联一个引用队列。 强引用：无论内存是否足够，不会回收。 软引用：内存不足时，回收该引用关联的对象。(可以选择配合引用队列使用，也可以不配合)。 弱引用：垃圾回收时，无论内存是否足够，都会回收。(可以选择配合引用队列使用，也可以不配合)。 虚引用：任何时候都可能被垃圾回收器回收。(必须配合引用队列使用)。 GC algorithm Mark and Sweep： Mark all reachable objects and sweep the rest. drawback: Resulting large amount of fragmental memory pieces, hard to allocate memory for large object. : Mark and compact Mark and sweep with compacting memory pieces. Good for large objects but less efficient. Copying Copy used objects to another memory area and release all of the old area. Easy but smemory consuming. Generational Collection(Common) 新创建的对象首先会被分配在伊甸园区域。 新生代空间不足时，触发Minor GC，伊甸园和 FROM幸存区需要存活的对象会被COPY到TO幸存区中，存活的对象寿命+1，并且交换FROM和TO。 Minor GC会引发 Stop The World：暂停其他用户的线程，等待垃圾回收结束后，用户线程才可以恢复执行。 当对象寿命超过阈值15时，会晋升至老年代。 如果新生代、老年代中的内存都满了，就会先触发Minor GC，再触发Full GC，扫描新生代和老年代中所有不再使用的对象并回收。 GC ","date":"2022-09-02","objectID":"/jvm/:0:7","series":null,"tags":null,"title":"JVM","uri":"/jvm/#jvm-gc"},{"categories":["Tech"],"content":"JVM GC Reference counting(Deprecated) if a and b have circular dependency, the memory won’t be collected. Reachability Analysis: Use algo to determine whether an object in heap is referenced, if not, recycle it. Need a GC root first(the object that definitely can’t be recycled.). Object that can be GC root: Object in stack Static reference in NA Reference contants in NA JNI reference objects in local stack. reference types: Strong: new object, a = b (When it is not referenced by any object, it will be recycled.) Even if the memory is short, JVM will throw outofmemory instead of recycle it. Soft: will be recycled when memory is not enough. SoftReference Weak: WeakReference will be recycled when GC. Soft and weak reference will be sent into reference list when the object is gone(list is used to store soft and weak reference when we neeed to remove them). PhatomRefence and final(Use finalize() method to init) reference need to be used with a list. They will be sent into reference list when the object is gone. Cleaner will regularly scan the list to remove reference. 如上图，B对象不再引用A4对象。这是终结器对象就会被放入引用队列中，引用队列会根据它，找到它所引用的对象。然后调用被引用对象的finalize()方法。调用以后，该对象就可以被垃圾回收了。 引用队列 软引用和弱引用可以配合引用队列(也可以不配合)： 在弱引用和虚引用所引用的对象被回收以后，会将这些引用放入引用队列中，方便一起回收这些软/弱引用对象。 虚引用和终结器引用必须配合引用队列： 虚引用和终结器引用在使用时会关联一个引用队列。 强引用：无论内存是否足够，不会回收。 软引用：内存不足时，回收该引用关联的对象。(可以选择配合引用队列使用，也可以不配合)。 弱引用：垃圾回收时，无论内存是否足够，都会回收。(可以选择配合引用队列使用，也可以不配合)。 虚引用：任何时候都可能被垃圾回收器回收。(必须配合引用队列使用)。 GC algorithm Mark and Sweep： Mark all reachable objects and sweep the rest. drawback: Resulting large amount of fragmental memory pieces, hard to allocate memory for large object. : Mark and compact Mark and sweep with compacting memory pieces. Good for large objects but less efficient. Copying Copy used objects to another memory area and release all of the old area. Easy but smemory consuming. Generational Collection(Common) 新创建的对象首先会被分配在伊甸园区域。 新生代空间不足时，触发Minor GC，伊甸园和 FROM幸存区需要存活的对象会被COPY到TO幸存区中，存活的对象寿命+1，并且交换FROM和TO。 Minor GC会引发 Stop The World：暂停其他用户的线程，等待垃圾回收结束后，用户线程才可以恢复执行。 当对象寿命超过阈值15时，会晋升至老年代。 如果新生代、老年代中的内存都满了，就会先触发Minor GC，再触发Full GC，扫描新生代和老年代中所有不再使用的对象并回收。 GC ","date":"2022-09-02","objectID":"/jvm/:0:7","series":null,"tags":null,"title":"JVM","uri":"/jvm/#引用队列"},{"categories":["Tech"],"content":"JVM GC Reference counting(Deprecated) if a and b have circular dependency, the memory won’t be collected. Reachability Analysis: Use algo to determine whether an object in heap is referenced, if not, recycle it. Need a GC root first(the object that definitely can’t be recycled.). Object that can be GC root: Object in stack Static reference in NA Reference contants in NA JNI reference objects in local stack. reference types: Strong: new object, a = b (When it is not referenced by any object, it will be recycled.) Even if the memory is short, JVM will throw outofmemory instead of recycle it. Soft: will be recycled when memory is not enough. SoftReference Weak: WeakReference will be recycled when GC. Soft and weak reference will be sent into reference list when the object is gone(list is used to store soft and weak reference when we neeed to remove them). PhatomRefence and final(Use finalize() method to init) reference need to be used with a list. They will be sent into reference list when the object is gone. Cleaner will regularly scan the list to remove reference. 如上图，B对象不再引用A4对象。这是终结器对象就会被放入引用队列中，引用队列会根据它，找到它所引用的对象。然后调用被引用对象的finalize()方法。调用以后，该对象就可以被垃圾回收了。 引用队列 软引用和弱引用可以配合引用队列(也可以不配合)： 在弱引用和虚引用所引用的对象被回收以后，会将这些引用放入引用队列中，方便一起回收这些软/弱引用对象。 虚引用和终结器引用必须配合引用队列： 虚引用和终结器引用在使用时会关联一个引用队列。 强引用：无论内存是否足够，不会回收。 软引用：内存不足时，回收该引用关联的对象。(可以选择配合引用队列使用，也可以不配合)。 弱引用：垃圾回收时，无论内存是否足够，都会回收。(可以选择配合引用队列使用，也可以不配合)。 虚引用：任何时候都可能被垃圾回收器回收。(必须配合引用队列使用)。 GC algorithm Mark and Sweep： Mark all reachable objects and sweep the rest. drawback: Resulting large amount of fragmental memory pieces, hard to allocate memory for large object. : Mark and compact Mark and sweep with compacting memory pieces. Good for large objects but less efficient. Copying Copy used objects to another memory area and release all of the old area. Easy but smemory consuming. Generational Collection(Common) 新创建的对象首先会被分配在伊甸园区域。 新生代空间不足时，触发Minor GC，伊甸园和 FROM幸存区需要存活的对象会被COPY到TO幸存区中，存活的对象寿命+1，并且交换FROM和TO。 Minor GC会引发 Stop The World：暂停其他用户的线程，等待垃圾回收结束后，用户线程才可以恢复执行。 当对象寿命超过阈值15时，会晋升至老年代。 如果新生代、老年代中的内存都满了，就会先触发Minor GC，再触发Full GC，扫描新生代和老年代中所有不再使用的对象并回收。 GC ","date":"2022-09-02","objectID":"/jvm/:0:7","series":null,"tags":null,"title":"JVM","uri":"/jvm/#gc-algorithm"},{"categories":["Tech"],"content":"JVM GC Reference counting(Deprecated) if a and b have circular dependency, the memory won’t be collected. Reachability Analysis: Use algo to determine whether an object in heap is referenced, if not, recycle it. Need a GC root first(the object that definitely can’t be recycled.). Object that can be GC root: Object in stack Static reference in NA Reference contants in NA JNI reference objects in local stack. reference types: Strong: new object, a = b (When it is not referenced by any object, it will be recycled.) Even if the memory is short, JVM will throw outofmemory instead of recycle it. Soft: will be recycled when memory is not enough. SoftReference Weak: WeakReference will be recycled when GC. Soft and weak reference will be sent into reference list when the object is gone(list is used to store soft and weak reference when we neeed to remove them). PhatomRefence and final(Use finalize() method to init) reference need to be used with a list. They will be sent into reference list when the object is gone. Cleaner will regularly scan the list to remove reference. 如上图，B对象不再引用A4对象。这是终结器对象就会被放入引用队列中，引用队列会根据它，找到它所引用的对象。然后调用被引用对象的finalize()方法。调用以后，该对象就可以被垃圾回收了。 引用队列 软引用和弱引用可以配合引用队列(也可以不配合)： 在弱引用和虚引用所引用的对象被回收以后，会将这些引用放入引用队列中，方便一起回收这些软/弱引用对象。 虚引用和终结器引用必须配合引用队列： 虚引用和终结器引用在使用时会关联一个引用队列。 强引用：无论内存是否足够，不会回收。 软引用：内存不足时，回收该引用关联的对象。(可以选择配合引用队列使用，也可以不配合)。 弱引用：垃圾回收时，无论内存是否足够，都会回收。(可以选择配合引用队列使用，也可以不配合)。 虚引用：任何时候都可能被垃圾回收器回收。(必须配合引用队列使用)。 GC algorithm Mark and Sweep： Mark all reachable objects and sweep the rest. drawback: Resulting large amount of fragmental memory pieces, hard to allocate memory for large object. : Mark and compact Mark and sweep with compacting memory pieces. Good for large objects but less efficient. Copying Copy used objects to another memory area and release all of the old area. Easy but smemory consuming. Generational Collection(Common) 新创建的对象首先会被分配在伊甸园区域。 新生代空间不足时，触发Minor GC，伊甸园和 FROM幸存区需要存活的对象会被COPY到TO幸存区中，存活的对象寿命+1，并且交换FROM和TO。 Minor GC会引发 Stop The World：暂停其他用户的线程，等待垃圾回收结束后，用户线程才可以恢复执行。 当对象寿命超过阈值15时，会晋升至老年代。 如果新生代、老年代中的内存都满了，就会先触发Minor GC，再触发Full GC，扫描新生代和老年代中所有不再使用的对象并回收。 GC ","date":"2022-09-02","objectID":"/jvm/:0:7","series":null,"tags":null,"title":"JVM","uri":"/jvm/#gc"},{"categories":["Tech"],"content":"A powerful search engine for large scale data search. Also benefit for log analysis and monitoring. Elasticsearch: store, compute, search data ","date":"2022-08-10","objectID":"/elasticsearch/:0:0","series":null,"tags":null,"title":"Elasticssearch","uri":"/elasticsearch/#"},{"categories":["Tech"],"content":"Inverted Index(less memory and faster query) inverted index (also referred to as a postings list, postings file, or inverted file) is a database index storing a mapping from content, such as words or numbers, to its locations in a table, or in a document or a set of documents (named in contrast to a forward index, which maps from documents to content). The purpose of an inverted index is to allow fast full-text searches, at a cost of increased processing when a document is added to the database. “it is what it is” “what is it” “it is a banana” We can get the following reverse file index: “a”: {2} “banana”: {2} “is”: {0, 1, 2} “it”: {0, 1, 2} “what”: {0, 1} Search criteria “what” , “is” and “it” will correspond to this collection :. For the same text, we get the following completely reverse indexes, document the number and the word result of the current query. Data . Similarly, both the number of documents and the word results Currently queried start from zero. So, “banana”: {(2, 3)} that is to say, “banana” is in the third document (), and the position in the third document is the fourth word (address is 3). “a”: {(2, 2)} “banana”: {(2, 3)} “is”: {(0, 1), (0, 4), (1, 1), (2, 1)} “it”: {(0, 0), (0, 3), (1, 2), (2, 0)} “what”: {(0, 2), (1, 0)} Mysql is suitable for writing and transactions Elasticsearch is suitable for largescale data search analysis and computing ","date":"2022-08-10","objectID":"/elasticsearch/:0:1","series":null,"tags":null,"title":"Elasticssearch","uri":"/elasticsearch/#inverted-indexless-memory-and-faster-query"},{"categories":["Tech"],"content":"Index type: text(words that can be seperated), keyword(Should not be seperated: country names) Numbers: long, integer, short, byte, double, float boolean date object index: Specifies whether to create an index. The default value is true. analyzer: Specifies which analyzer to use. properties: To specify sub strings ","date":"2022-08-10","objectID":"/elasticsearch/:0:2","series":null,"tags":null,"title":"Elasticssearch","uri":"/elasticsearch/#index"},{"categories":["Tech"],"content":"Index type: text(words that can be seperated), keyword(Should not be seperated: country names) Numbers: long, integer, short, byte, double, float boolean date object index: Specifies whether to create an index. The default value is true. analyzer: Specifies which analyzer to use. properties: To specify sub strings ","date":"2022-08-10","objectID":"/elasticsearch/:0:2","series":null,"tags":null,"title":"Elasticssearch","uri":"/elasticsearch/#type"},{"categories":["Tech"],"content":"Index type: text(words that can be seperated), keyword(Should not be seperated: country names) Numbers: long, integer, short, byte, double, float boolean date object index: Specifies whether to create an index. The default value is true. analyzer: Specifies which analyzer to use. properties: To specify sub strings ","date":"2022-08-10","objectID":"/elasticsearch/:0:2","series":null,"tags":null,"title":"Elasticssearch","uri":"/elasticsearch/#index-1"},{"categories":["Tech"],"content":"Index type: text(words that can be seperated), keyword(Should not be seperated: country names) Numbers: long, integer, short, byte, double, float boolean date object index: Specifies whether to create an index. The default value is true. analyzer: Specifies which analyzer to use. properties: To specify sub strings ","date":"2022-08-10","objectID":"/elasticsearch/:0:2","series":null,"tags":null,"title":"Elasticssearch","uri":"/elasticsearch/#analyzer"},{"categories":["Tech"],"content":"Index type: text(words that can be seperated), keyword(Should not be seperated: country names) Numbers: long, integer, short, byte, double, float boolean date object index: Specifies whether to create an index. The default value is true. analyzer: Specifies which analyzer to use. properties: To specify sub strings ","date":"2022-08-10","objectID":"/elasticsearch/:0:2","series":null,"tags":null,"title":"Elasticssearch","uri":"/elasticsearch/#properties"},{"categories":["Tech"],"content":"DSL index operations Create index and mapping using DSL: # Create index PUT /test { \"mappings\": { \"properties\": { \"info\": { \"type\": \"text\", \"analyzer\": \"standard\" }, \"email\": { \"type\": \"keyword\", \"index\": false }, \"name\": { \"type\": \"object\", \"properties\": { \"firstName\": { \"type\": \"keyword\" }, \"lastName\": { \"type\": \"keyword\" } } } } } } inspect, update and delete index # search GET /test # update(cannot be updated, so we need a brand new index) PUT /test/_mapping { \"properties\":{ \"age\":{ \"type\": \"integer\" } } } # delete DELETE /test ","date":"2022-08-10","objectID":"/elasticsearch/:0:3","series":null,"tags":null,"title":"Elasticssearch","uri":"/elasticsearch/#dsl-index-operations"},{"categories":["Tech"],"content":"DSL Document operations Insert document # Insert a new document POST /test/_doc/1 { \"info\": \"黑马程序员java讲师\", \"email\": \"112837@qq.com\", \"name\":{ \"firstName\":\"云\", \"lastName\":\"赵\" } } select and delete document # 查询 GET /heima/_doc/1 # 删除 DELETE /heima/_doc/1 update document: # delete old document and create a new one PUT /heima/_doc/1 { \"info\": \"黑马程序员java讲师\", \"email\": \"112837@qq.com\", \"name\":{ \"firstName\":\"云\", \"lastName\":\"赵\" } } 如果id在索引库里面不存在，并不会报错，而是直接新增，如果索引库存在该记录，就会先删掉该记录，然后增加一个全新的。 # 局部修改文档字段 # there has to be a \"doc\" POST /heima/_update/1 { \"doc\": { \"email\":\"lbwnb@qq.com\" } } Write DSL statements and create index libraries (equivalent to creating tables in MySQL) # 酒店的mapping PUT /hotel { \"mappings\": { \"properties\": { \"id\":{ \"type\": \"keyword\" }, \"name\":{ \"type\": \"text\" , \"analyzer\": \"ik_max_word\", \"copy_to\": \"all\" }, \"address\":{ \"type\": \"keyword\" , \"index\": false }, \"price\":{ \"type\": \"integer\" }, \"score\":{ \"type\": \"integer\" }, \"brand\":{ \"type\": \"keyword\", \"copy_to\": \"all\" }, \"city\":{ \"type\": \"keyword\" }, \"starName\":{ \"type\": \"keyword\" }, \"business\":{ \"type\": \"keyword\", \"copy_to\": \"all\" }, \"location\":{ \"type\": \"geo_point\" }, \"pic\":{ \"type\": \"keyword\" , \"index\": false }, \"all\":{ \"type\": \"text\", \"analyzer\": \"ik_max_word\" } } } } use text to do text analysis, use keyword to prevent analysis. #use copy_to to set up group index and search Store Restclient Search search all: a. match_all full text: analyze user input and search in inverted index (match_query, multi_match_query) # match 和 multi_match GET /hotel/_search { \"query\": { \"match\": { \"address\": \"如家外滩\" } } } GET /hotel/_search { \"query\": { \"multi_match\": { \"query\": \"外滩如家\", \"fields\": [\"brand\",\"name\",\"business\"] } } } keyword search: no analysis, search keywords(numbers, date,boolean): ids, range, term # 精确查询（term查询） GET /hotel/_search { \"query\": { \"term\": { \"city\": { \"value\": \"上海\" } } } } # 精确查询(范围range) GET /hotel/_search { \"query\": { \"range\": { \"price\": { \"gte\": 100, \"lte\": 300 } } } } # distance查询 GET /hotel/_search { \"query\": { \"geo_distance\":{ \"distance\": \"5km\", \"location\": \"31.21, 121.5\" } } } compound search boolean query combine multiple query must: \u0026\u0026 should: || must_not: 不算分 not filter: 不算分 GET /hotel/_search { \"query\": { \"bool\": { \"must\": [ {\"match\": { \"name\": \"如家\" }} ], \"must_not\": [ {\"range\": { \"price\": { \"gt\": 400 } }} ], \"filter\": [ { \"geo_distance\": { \"distance\": \"100km\", \"location\": { \"lat\": 31.21, \"lon\": 121.5 } } } ] } } } function score function score use BM25 algorithm GET /hotel/_search { \"query\": { \"function_score\": { \"query\": { \"match\": { \"address\": \"外滩\" } }, \"functions\": [ { \"filter\": { \"term\": { \"brand\": \"如家\" } }, \"weight\": 10 } ] } } } sort default: function score keyword, numbers, date, location Note: Elasticsearch won’t calculate scores when we are using sorting algorithm to optimize performance # sort排序 preice asc if scores are euqal GET /hotel/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"score\": \"desc\" }, { \"price\": \"asc\" } ]组合查询-function score 对应的Java RestClient代码： } //距离排序 distance to the location GET /hotel/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"_geo_distance\": { \"location\": { \"lat\": 31.03, \"lon\": 121.61 }, \"order\": \"asc\" , \"unit\": \"km\" } } ] } Page similar to limit in mysql GET /hotel/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"price\": \"asc\" } ], \"from\": 20 , \"size\": 5 } But Paging problems: the underlying layer of ES is inverted index, which is not conducive to paging, so paging query is a logical paging. For example, starting from 990, you need to intercept 10 pieces of data (10 pieces from 990 to 1000). For ES, you need to first query 0 to 1000 pieces of data, and then logically intercept 10 pieces of data by page. If it is a monomer, it is only a matter of efficiency at most, but if it is a cluster, it will be a bad thing. We need to get 1000 from each of the nodes and re-sort them. Then we get the 10 pieces from 990 to 1000. Too costly when the number is large. highlight highlight the search keyword By defau","date":"2022-08-10","objectID":"/elasticsearch/:0:4","series":null,"tags":null,"title":"Elasticssearch","uri":"/elasticsearch/#dsl-document-operations"},{"categories":["Tech"],"content":"DSL Document operations Insert document # Insert a new document POST /test/_doc/1 { \"info\": \"黑马程序员java讲师\", \"email\": \"112837@qq.com\", \"name\":{ \"firstName\":\"云\", \"lastName\":\"赵\" } } select and delete document # 查询 GET /heima/_doc/1 # 删除 DELETE /heima/_doc/1 update document: # delete old document and create a new one PUT /heima/_doc/1 { \"info\": \"黑马程序员java讲师\", \"email\": \"112837@qq.com\", \"name\":{ \"firstName\":\"云\", \"lastName\":\"赵\" } } 如果id在索引库里面不存在，并不会报错，而是直接新增，如果索引库存在该记录，就会先删掉该记录，然后增加一个全新的。 # 局部修改文档字段 # there has to be a \"doc\" POST /heima/_update/1 { \"doc\": { \"email\":\"lbwnb@qq.com\" } } Write DSL statements and create index libraries (equivalent to creating tables in MySQL) # 酒店的mapping PUT /hotel { \"mappings\": { \"properties\": { \"id\":{ \"type\": \"keyword\" }, \"name\":{ \"type\": \"text\" , \"analyzer\": \"ik_max_word\", \"copy_to\": \"all\" }, \"address\":{ \"type\": \"keyword\" , \"index\": false }, \"price\":{ \"type\": \"integer\" }, \"score\":{ \"type\": \"integer\" }, \"brand\":{ \"type\": \"keyword\", \"copy_to\": \"all\" }, \"city\":{ \"type\": \"keyword\" }, \"starName\":{ \"type\": \"keyword\" }, \"business\":{ \"type\": \"keyword\", \"copy_to\": \"all\" }, \"location\":{ \"type\": \"geo_point\" }, \"pic\":{ \"type\": \"keyword\" , \"index\": false }, \"all\":{ \"type\": \"text\", \"analyzer\": \"ik_max_word\" } } } } use text to do text analysis, use keyword to prevent analysis. #use copy_to to set up group index and search Store Restclient Search search all: a. match_all full text: analyze user input and search in inverted index (match_query, multi_match_query) # match 和 multi_match GET /hotel/_search { \"query\": { \"match\": { \"address\": \"如家外滩\" } } } GET /hotel/_search { \"query\": { \"multi_match\": { \"query\": \"外滩如家\", \"fields\": [\"brand\",\"name\",\"business\"] } } } keyword search: no analysis, search keywords(numbers, date,boolean): ids, range, term # 精确查询（term查询） GET /hotel/_search { \"query\": { \"term\": { \"city\": { \"value\": \"上海\" } } } } # 精确查询(范围range) GET /hotel/_search { \"query\": { \"range\": { \"price\": { \"gte\": 100, \"lte\": 300 } } } } # distance查询 GET /hotel/_search { \"query\": { \"geo_distance\":{ \"distance\": \"5km\", \"location\": \"31.21, 121.5\" } } } compound search boolean query combine multiple query must: \u0026\u0026 should: || must_not: 不算分 not filter: 不算分 GET /hotel/_search { \"query\": { \"bool\": { \"must\": [ {\"match\": { \"name\": \"如家\" }} ], \"must_not\": [ {\"range\": { \"price\": { \"gt\": 400 } }} ], \"filter\": [ { \"geo_distance\": { \"distance\": \"100km\", \"location\": { \"lat\": 31.21, \"lon\": 121.5 } } } ] } } } function score function score use BM25 algorithm GET /hotel/_search { \"query\": { \"function_score\": { \"query\": { \"match\": { \"address\": \"外滩\" } }, \"functions\": [ { \"filter\": { \"term\": { \"brand\": \"如家\" } }, \"weight\": 10 } ] } } } sort default: function score keyword, numbers, date, location Note: Elasticsearch won’t calculate scores when we are using sorting algorithm to optimize performance # sort排序 preice asc if scores are euqal GET /hotel/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"score\": \"desc\" }, { \"price\": \"asc\" } ]组合查询-function score 对应的Java RestClient代码： } //距离排序 distance to the location GET /hotel/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"_geo_distance\": { \"location\": { \"lat\": 31.03, \"lon\": 121.61 }, \"order\": \"asc\" , \"unit\": \"km\" } } ] } Page similar to limit in mysql GET /hotel/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"price\": \"asc\" } ], \"from\": 20 , \"size\": 5 } But Paging problems: the underlying layer of ES is inverted index, which is not conducive to paging, so paging query is a logical paging. For example, starting from 990, you need to intercept 10 pieces of data (10 pieces from 990 to 1000). For ES, you need to first query 0 to 1000 pieces of data, and then logically intercept 10 pieces of data by page. If it is a monomer, it is only a matter of efficiency at most, but if it is a cluster, it will be a bad thing. We need to get 1000 from each of the nodes and re-sort them. Then we get the 10 pieces from 990 to 1000. Too costly when the number is large. highlight highlight the search keyword By defau","date":"2022-08-10","objectID":"/elasticsearch/:0:4","series":null,"tags":null,"title":"Elasticssearch","uri":"/elasticsearch/#store"},{"categories":["Tech"],"content":"DSL Document operations Insert document # Insert a new document POST /test/_doc/1 { \"info\": \"黑马程序员java讲师\", \"email\": \"112837@qq.com\", \"name\":{ \"firstName\":\"云\", \"lastName\":\"赵\" } } select and delete document # 查询 GET /heima/_doc/1 # 删除 DELETE /heima/_doc/1 update document: # delete old document and create a new one PUT /heima/_doc/1 { \"info\": \"黑马程序员java讲师\", \"email\": \"112837@qq.com\", \"name\":{ \"firstName\":\"云\", \"lastName\":\"赵\" } } 如果id在索引库里面不存在，并不会报错，而是直接新增，如果索引库存在该记录，就会先删掉该记录，然后增加一个全新的。 # 局部修改文档字段 # there has to be a \"doc\" POST /heima/_update/1 { \"doc\": { \"email\":\"lbwnb@qq.com\" } } Write DSL statements and create index libraries (equivalent to creating tables in MySQL) # 酒店的mapping PUT /hotel { \"mappings\": { \"properties\": { \"id\":{ \"type\": \"keyword\" }, \"name\":{ \"type\": \"text\" , \"analyzer\": \"ik_max_word\", \"copy_to\": \"all\" }, \"address\":{ \"type\": \"keyword\" , \"index\": false }, \"price\":{ \"type\": \"integer\" }, \"score\":{ \"type\": \"integer\" }, \"brand\":{ \"type\": \"keyword\", \"copy_to\": \"all\" }, \"city\":{ \"type\": \"keyword\" }, \"starName\":{ \"type\": \"keyword\" }, \"business\":{ \"type\": \"keyword\", \"copy_to\": \"all\" }, \"location\":{ \"type\": \"geo_point\" }, \"pic\":{ \"type\": \"keyword\" , \"index\": false }, \"all\":{ \"type\": \"text\", \"analyzer\": \"ik_max_word\" } } } } use text to do text analysis, use keyword to prevent analysis. #use copy_to to set up group index and search Store Restclient Search search all: a. match_all full text: analyze user input and search in inverted index (match_query, multi_match_query) # match 和 multi_match GET /hotel/_search { \"query\": { \"match\": { \"address\": \"如家外滩\" } } } GET /hotel/_search { \"query\": { \"multi_match\": { \"query\": \"外滩如家\", \"fields\": [\"brand\",\"name\",\"business\"] } } } keyword search: no analysis, search keywords(numbers, date,boolean): ids, range, term # 精确查询（term查询） GET /hotel/_search { \"query\": { \"term\": { \"city\": { \"value\": \"上海\" } } } } # 精确查询(范围range) GET /hotel/_search { \"query\": { \"range\": { \"price\": { \"gte\": 100, \"lte\": 300 } } } } # distance查询 GET /hotel/_search { \"query\": { \"geo_distance\":{ \"distance\": \"5km\", \"location\": \"31.21, 121.5\" } } } compound search boolean query combine multiple query must: \u0026\u0026 should: || must_not: 不算分 not filter: 不算分 GET /hotel/_search { \"query\": { \"bool\": { \"must\": [ {\"match\": { \"name\": \"如家\" }} ], \"must_not\": [ {\"range\": { \"price\": { \"gt\": 400 } }} ], \"filter\": [ { \"geo_distance\": { \"distance\": \"100km\", \"location\": { \"lat\": 31.21, \"lon\": 121.5 } } } ] } } } function score function score use BM25 algorithm GET /hotel/_search { \"query\": { \"function_score\": { \"query\": { \"match\": { \"address\": \"外滩\" } }, \"functions\": [ { \"filter\": { \"term\": { \"brand\": \"如家\" } }, \"weight\": 10 } ] } } } sort default: function score keyword, numbers, date, location Note: Elasticsearch won’t calculate scores when we are using sorting algorithm to optimize performance # sort排序 preice asc if scores are euqal GET /hotel/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"score\": \"desc\" }, { \"price\": \"asc\" } ]组合查询-function score 对应的Java RestClient代码： } //距离排序 distance to the location GET /hotel/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"_geo_distance\": { \"location\": { \"lat\": 31.03, \"lon\": 121.61 }, \"order\": \"asc\" , \"unit\": \"km\" } } ] } Page similar to limit in mysql GET /hotel/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"price\": \"asc\" } ], \"from\": 20 , \"size\": 5 } But Paging problems: the underlying layer of ES is inverted index, which is not conducive to paging, so paging query is a logical paging. For example, starting from 990, you need to intercept 10 pieces of data (10 pieces from 990 to 1000). For ES, you need to first query 0 to 1000 pieces of data, and then logically intercept 10 pieces of data by page. If it is a monomer, it is only a matter of efficiency at most, but if it is a cluster, it will be a bad thing. We need to get 1000 from each of the nodes and re-sort them. Then we get the 10 pieces from 990 to 1000. Too costly when the number is large. highlight highlight the search keyword By defau","date":"2022-08-10","objectID":"/elasticsearch/:0:4","series":null,"tags":null,"title":"Elasticssearch","uri":"/elasticsearch/#search"},{"categories":["Tech"],"content":"DSL Document operations Insert document # Insert a new document POST /test/_doc/1 { \"info\": \"黑马程序员java讲师\", \"email\": \"112837@qq.com\", \"name\":{ \"firstName\":\"云\", \"lastName\":\"赵\" } } select and delete document # 查询 GET /heima/_doc/1 # 删除 DELETE /heima/_doc/1 update document: # delete old document and create a new one PUT /heima/_doc/1 { \"info\": \"黑马程序员java讲师\", \"email\": \"112837@qq.com\", \"name\":{ \"firstName\":\"云\", \"lastName\":\"赵\" } } 如果id在索引库里面不存在，并不会报错，而是直接新增，如果索引库存在该记录，就会先删掉该记录，然后增加一个全新的。 # 局部修改文档字段 # there has to be a \"doc\" POST /heima/_update/1 { \"doc\": { \"email\":\"lbwnb@qq.com\" } } Write DSL statements and create index libraries (equivalent to creating tables in MySQL) # 酒店的mapping PUT /hotel { \"mappings\": { \"properties\": { \"id\":{ \"type\": \"keyword\" }, \"name\":{ \"type\": \"text\" , \"analyzer\": \"ik_max_word\", \"copy_to\": \"all\" }, \"address\":{ \"type\": \"keyword\" , \"index\": false }, \"price\":{ \"type\": \"integer\" }, \"score\":{ \"type\": \"integer\" }, \"brand\":{ \"type\": \"keyword\", \"copy_to\": \"all\" }, \"city\":{ \"type\": \"keyword\" }, \"starName\":{ \"type\": \"keyword\" }, \"business\":{ \"type\": \"keyword\", \"copy_to\": \"all\" }, \"location\":{ \"type\": \"geo_point\" }, \"pic\":{ \"type\": \"keyword\" , \"index\": false }, \"all\":{ \"type\": \"text\", \"analyzer\": \"ik_max_word\" } } } } use text to do text analysis, use keyword to prevent analysis. #use copy_to to set up group index and search Store Restclient Search search all: a. match_all full text: analyze user input and search in inverted index (match_query, multi_match_query) # match 和 multi_match GET /hotel/_search { \"query\": { \"match\": { \"address\": \"如家外滩\" } } } GET /hotel/_search { \"query\": { \"multi_match\": { \"query\": \"外滩如家\", \"fields\": [\"brand\",\"name\",\"business\"] } } } keyword search: no analysis, search keywords(numbers, date,boolean): ids, range, term # 精确查询（term查询） GET /hotel/_search { \"query\": { \"term\": { \"city\": { \"value\": \"上海\" } } } } # 精确查询(范围range) GET /hotel/_search { \"query\": { \"range\": { \"price\": { \"gte\": 100, \"lte\": 300 } } } } # distance查询 GET /hotel/_search { \"query\": { \"geo_distance\":{ \"distance\": \"5km\", \"location\": \"31.21, 121.5\" } } } compound search boolean query combine multiple query must: \u0026\u0026 should: || must_not: 不算分 not filter: 不算分 GET /hotel/_search { \"query\": { \"bool\": { \"must\": [ {\"match\": { \"name\": \"如家\" }} ], \"must_not\": [ {\"range\": { \"price\": { \"gt\": 400 } }} ], \"filter\": [ { \"geo_distance\": { \"distance\": \"100km\", \"location\": { \"lat\": 31.21, \"lon\": 121.5 } } } ] } } } function score function score use BM25 algorithm GET /hotel/_search { \"query\": { \"function_score\": { \"query\": { \"match\": { \"address\": \"外滩\" } }, \"functions\": [ { \"filter\": { \"term\": { \"brand\": \"如家\" } }, \"weight\": 10 } ] } } } sort default: function score keyword, numbers, date, location Note: Elasticsearch won’t calculate scores when we are using sorting algorithm to optimize performance # sort排序 preice asc if scores are euqal GET /hotel/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"score\": \"desc\" }, { \"price\": \"asc\" } ]组合查询-function score 对应的Java RestClient代码： } //距离排序 distance to the location GET /hotel/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"_geo_distance\": { \"location\": { \"lat\": 31.03, \"lon\": 121.61 }, \"order\": \"asc\" , \"unit\": \"km\" } } ] } Page similar to limit in mysql GET /hotel/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"price\": \"asc\" } ], \"from\": 20 , \"size\": 5 } But Paging problems: the underlying layer of ES is inverted index, which is not conducive to paging, so paging query is a logical paging. For example, starting from 990, you need to intercept 10 pieces of data (10 pieces from 990 to 1000). For ES, you need to first query 0 to 1000 pieces of data, and then logically intercept 10 pieces of data by page. If it is a monomer, it is only a matter of efficiency at most, but if it is a cluster, it will be a bad thing. We need to get 1000 from each of the nodes and re-sort them. Then we get the 10 pieces from 990 to 1000. Too costly when the number is large. highlight highlight the search keyword By defau","date":"2022-08-10","objectID":"/elasticsearch/:0:4","series":null,"tags":null,"title":"Elasticssearch","uri":"/elasticsearch/#sort"},{"categories":["Tech"],"content":"DSL Document operations Insert document # Insert a new document POST /test/_doc/1 { \"info\": \"黑马程序员java讲师\", \"email\": \"112837@qq.com\", \"name\":{ \"firstName\":\"云\", \"lastName\":\"赵\" } } select and delete document # 查询 GET /heima/_doc/1 # 删除 DELETE /heima/_doc/1 update document: # delete old document and create a new one PUT /heima/_doc/1 { \"info\": \"黑马程序员java讲师\", \"email\": \"112837@qq.com\", \"name\":{ \"firstName\":\"云\", \"lastName\":\"赵\" } } 如果id在索引库里面不存在，并不会报错，而是直接新增，如果索引库存在该记录，就会先删掉该记录，然后增加一个全新的。 # 局部修改文档字段 # there has to be a \"doc\" POST /heima/_update/1 { \"doc\": { \"email\":\"lbwnb@qq.com\" } } Write DSL statements and create index libraries (equivalent to creating tables in MySQL) # 酒店的mapping PUT /hotel { \"mappings\": { \"properties\": { \"id\":{ \"type\": \"keyword\" }, \"name\":{ \"type\": \"text\" , \"analyzer\": \"ik_max_word\", \"copy_to\": \"all\" }, \"address\":{ \"type\": \"keyword\" , \"index\": false }, \"price\":{ \"type\": \"integer\" }, \"score\":{ \"type\": \"integer\" }, \"brand\":{ \"type\": \"keyword\", \"copy_to\": \"all\" }, \"city\":{ \"type\": \"keyword\" }, \"starName\":{ \"type\": \"keyword\" }, \"business\":{ \"type\": \"keyword\", \"copy_to\": \"all\" }, \"location\":{ \"type\": \"geo_point\" }, \"pic\":{ \"type\": \"keyword\" , \"index\": false }, \"all\":{ \"type\": \"text\", \"analyzer\": \"ik_max_word\" } } } } use text to do text analysis, use keyword to prevent analysis. #use copy_to to set up group index and search Store Restclient Search search all: a. match_all full text: analyze user input and search in inverted index (match_query, multi_match_query) # match 和 multi_match GET /hotel/_search { \"query\": { \"match\": { \"address\": \"如家外滩\" } } } GET /hotel/_search { \"query\": { \"multi_match\": { \"query\": \"外滩如家\", \"fields\": [\"brand\",\"name\",\"business\"] } } } keyword search: no analysis, search keywords(numbers, date,boolean): ids, range, term # 精确查询（term查询） GET /hotel/_search { \"query\": { \"term\": { \"city\": { \"value\": \"上海\" } } } } # 精确查询(范围range) GET /hotel/_search { \"query\": { \"range\": { \"price\": { \"gte\": 100, \"lte\": 300 } } } } # distance查询 GET /hotel/_search { \"query\": { \"geo_distance\":{ \"distance\": \"5km\", \"location\": \"31.21, 121.5\" } } } compound search boolean query combine multiple query must: \u0026\u0026 should: || must_not: 不算分 not filter: 不算分 GET /hotel/_search { \"query\": { \"bool\": { \"must\": [ {\"match\": { \"name\": \"如家\" }} ], \"must_not\": [ {\"range\": { \"price\": { \"gt\": 400 } }} ], \"filter\": [ { \"geo_distance\": { \"distance\": \"100km\", \"location\": { \"lat\": 31.21, \"lon\": 121.5 } } } ] } } } function score function score use BM25 algorithm GET /hotel/_search { \"query\": { \"function_score\": { \"query\": { \"match\": { \"address\": \"外滩\" } }, \"functions\": [ { \"filter\": { \"term\": { \"brand\": \"如家\" } }, \"weight\": 10 } ] } } } sort default: function score keyword, numbers, date, location Note: Elasticsearch won’t calculate scores when we are using sorting algorithm to optimize performance # sort排序 preice asc if scores are euqal GET /hotel/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"score\": \"desc\" }, { \"price\": \"asc\" } ]组合查询-function score 对应的Java RestClient代码： } //距离排序 distance to the location GET /hotel/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"_geo_distance\": { \"location\": { \"lat\": 31.03, \"lon\": 121.61 }, \"order\": \"asc\" , \"unit\": \"km\" } } ] } Page similar to limit in mysql GET /hotel/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"price\": \"asc\" } ], \"from\": 20 , \"size\": 5 } But Paging problems: the underlying layer of ES is inverted index, which is not conducive to paging, so paging query is a logical paging. For example, starting from 990, you need to intercept 10 pieces of data (10 pieces from 990 to 1000). For ES, you need to first query 0 to 1000 pieces of data, and then logically intercept 10 pieces of data by page. If it is a monomer, it is only a matter of efficiency at most, but if it is a cluster, it will be a bad thing. We need to get 1000 from each of the nodes and re-sort them. Then we get the 10 pieces from 990 to 1000. Too costly when the number is large. highlight highlight the search keyword By defau","date":"2022-08-10","objectID":"/elasticsearch/:0:4","series":null,"tags":null,"title":"Elasticssearch","uri":"/elasticsearch/#page"},{"categories":["Tech"],"content":"DSL Document operations Insert document # Insert a new document POST /test/_doc/1 { \"info\": \"黑马程序员java讲师\", \"email\": \"112837@qq.com\", \"name\":{ \"firstName\":\"云\", \"lastName\":\"赵\" } } select and delete document # 查询 GET /heima/_doc/1 # 删除 DELETE /heima/_doc/1 update document: # delete old document and create a new one PUT /heima/_doc/1 { \"info\": \"黑马程序员java讲师\", \"email\": \"112837@qq.com\", \"name\":{ \"firstName\":\"云\", \"lastName\":\"赵\" } } 如果id在索引库里面不存在，并不会报错，而是直接新增，如果索引库存在该记录，就会先删掉该记录，然后增加一个全新的。 # 局部修改文档字段 # there has to be a \"doc\" POST /heima/_update/1 { \"doc\": { \"email\":\"lbwnb@qq.com\" } } Write DSL statements and create index libraries (equivalent to creating tables in MySQL) # 酒店的mapping PUT /hotel { \"mappings\": { \"properties\": { \"id\":{ \"type\": \"keyword\" }, \"name\":{ \"type\": \"text\" , \"analyzer\": \"ik_max_word\", \"copy_to\": \"all\" }, \"address\":{ \"type\": \"keyword\" , \"index\": false }, \"price\":{ \"type\": \"integer\" }, \"score\":{ \"type\": \"integer\" }, \"brand\":{ \"type\": \"keyword\", \"copy_to\": \"all\" }, \"city\":{ \"type\": \"keyword\" }, \"starName\":{ \"type\": \"keyword\" }, \"business\":{ \"type\": \"keyword\", \"copy_to\": \"all\" }, \"location\":{ \"type\": \"geo_point\" }, \"pic\":{ \"type\": \"keyword\" , \"index\": false }, \"all\":{ \"type\": \"text\", \"analyzer\": \"ik_max_word\" } } } } use text to do text analysis, use keyword to prevent analysis. #use copy_to to set up group index and search Store Restclient Search search all: a. match_all full text: analyze user input and search in inverted index (match_query, multi_match_query) # match 和 multi_match GET /hotel/_search { \"query\": { \"match\": { \"address\": \"如家外滩\" } } } GET /hotel/_search { \"query\": { \"multi_match\": { \"query\": \"外滩如家\", \"fields\": [\"brand\",\"name\",\"business\"] } } } keyword search: no analysis, search keywords(numbers, date,boolean): ids, range, term # 精确查询（term查询） GET /hotel/_search { \"query\": { \"term\": { \"city\": { \"value\": \"上海\" } } } } # 精确查询(范围range) GET /hotel/_search { \"query\": { \"range\": { \"price\": { \"gte\": 100, \"lte\": 300 } } } } # distance查询 GET /hotel/_search { \"query\": { \"geo_distance\":{ \"distance\": \"5km\", \"location\": \"31.21, 121.5\" } } } compound search boolean query combine multiple query must: \u0026\u0026 should: || must_not: 不算分 not filter: 不算分 GET /hotel/_search { \"query\": { \"bool\": { \"must\": [ {\"match\": { \"name\": \"如家\" }} ], \"must_not\": [ {\"range\": { \"price\": { \"gt\": 400 } }} ], \"filter\": [ { \"geo_distance\": { \"distance\": \"100km\", \"location\": { \"lat\": 31.21, \"lon\": 121.5 } } } ] } } } function score function score use BM25 algorithm GET /hotel/_search { \"query\": { \"function_score\": { \"query\": { \"match\": { \"address\": \"外滩\" } }, \"functions\": [ { \"filter\": { \"term\": { \"brand\": \"如家\" } }, \"weight\": 10 } ] } } } sort default: function score keyword, numbers, date, location Note: Elasticsearch won’t calculate scores when we are using sorting algorithm to optimize performance # sort排序 preice asc if scores are euqal GET /hotel/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"score\": \"desc\" }, { \"price\": \"asc\" } ]组合查询-function score 对应的Java RestClient代码： } //距离排序 distance to the location GET /hotel/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"_geo_distance\": { \"location\": { \"lat\": 31.03, \"lon\": 121.61 }, \"order\": \"asc\" , \"unit\": \"km\" } } ] } Page similar to limit in mysql GET /hotel/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"price\": \"asc\" } ], \"from\": 20 , \"size\": 5 } But Paging problems: the underlying layer of ES is inverted index, which is not conducive to paging, so paging query is a logical paging. For example, starting from 990, you need to intercept 10 pieces of data (10 pieces from 990 to 1000). For ES, you need to first query 0 to 1000 pieces of data, and then logically intercept 10 pieces of data by page. If it is a monomer, it is only a matter of efficiency at most, but if it is a cluster, it will be a bad thing. We need to get 1000 from each of the nodes and re-sort them. Then we get the 10 pieces from 990 to 1000. Too costly when the number is large. highlight highlight the search keyword By defau","date":"2022-08-10","objectID":"/elasticsearch/:0:4","series":null,"tags":null,"title":"Elasticssearch","uri":"/elasticsearch/#highlight"},{"categories":["Tech"],"content":"[ ](https://blog.csdn.net/weixin_44757863/article/details/120959505)Aggregations Aggregations can be used to do analysis, computing and statistics analysis. Bucket: grouping TermAggregation By default, return a _count for field of the terms in desc order. We can use query to limit range of the TermAggregation. Metric: max, min, avg, stats based on the result of BucketAggregation Pipeline Aggregations of other aggregations(avg of a group) ","date":"2022-08-10","objectID":"/elasticsearch/:0:5","series":null,"tags":null,"title":"Elasticssearch","uri":"/elasticsearch/#heading"},{"categories":["Tech"],"content":"AutoSync between database and ES Use rabbitmq messages to decouple the service and update ","date":"2022-08-10","objectID":"/elasticsearch/:0:6","series":null,"tags":null,"title":"Elasticssearch","uri":"/elasticsearch/#autosync-between-database-and-es"},{"categories":["Tech"],"content":"ES Cluster An example of a robust ES cluster. split-brain problem: When the master is having network issues for a long time, other nodes notice the situation and decide to elect a node to be the master node, so there will be 2 master nodes when the old master node back online. Solving: setting the consensus vote to be (eligible nodes + 1) / 2.(In versions after Elasticsearch 7.0, the number of eligible nodes is automatically calculated, so there are unlikely to be split-brain problem) How to elect the master node ZenDiscovery, module is responsible for selecting the master node, which mainly includes Ping (the RPC is used to discover each other between nodes) and Unicast (the Unicast module contains a list of hosts to control which nodes need to ping); For all nodes that can become Masters ( node.master: true ) sort according to the Lexicographic order (字典序) of nodeId. Each node in each election sorts the nodes it knows in order, and then selects the first (0th) node. For the time being, it is considered as the master node. If the number of votes for a node reaches a certain value ( the number of eligible master nodes n/2+1) and the node itself elects itself, the node is the master. Otherwise, there will be reelection until the above conditions are met. Supplement: The master node is mainly responsible for managing clusters, nodes, and indexes. It is not responsible for document-level management. data nodes can disable http * ","date":"2022-08-10","objectID":"/elasticsearch/:0:7","series":null,"tags":null,"title":"Elasticssearch","uri":"/elasticsearch/#es-cluster"},{"categories":["Tech"],"content":"ES Cluster An example of a robust ES cluster. split-brain problem: When the master is having network issues for a long time, other nodes notice the situation and decide to elect a node to be the master node, so there will be 2 master nodes when the old master node back online. Solving: setting the consensus vote to be (eligible nodes + 1) / 2.(In versions after Elasticsearch 7.0, the number of eligible nodes is automatically calculated, so there are unlikely to be split-brain problem) How to elect the master node ZenDiscovery, module is responsible for selecting the master node, which mainly includes Ping (the RPC is used to discover each other between nodes) and Unicast (the Unicast module contains a list of hosts to control which nodes need to ping); For all nodes that can become Masters ( node.master: true ) sort according to the Lexicographic order (字典序) of nodeId. Each node in each election sorts the nodes it knows in order, and then selects the first (0th) node. For the time being, it is considered as the master node. If the number of votes for a node reaches a certain value ( the number of eligible master nodes n/2+1) and the node itself elects itself, the node is the master. Otherwise, there will be reelection until the above conditions are met. Supplement: The master node is mainly responsible for managing clusters, nodes, and indexes. It is not responsible for document-level management. data nodes can disable http * ","date":"2022-08-10","objectID":"/elasticsearch/:0:7","series":null,"tags":null,"title":"Elasticssearch","uri":"/elasticsearch/#split-brain-problem"},{"categories":["Tech"],"content":"ES Cluster An example of a robust ES cluster. split-brain problem: When the master is having network issues for a long time, other nodes notice the situation and decide to elect a node to be the master node, so there will be 2 master nodes when the old master node back online. Solving: setting the consensus vote to be (eligible nodes + 1) / 2.(In versions after Elasticsearch 7.0, the number of eligible nodes is automatically calculated, so there are unlikely to be split-brain problem) How to elect the master node ZenDiscovery, module is responsible for selecting the master node, which mainly includes Ping (the RPC is used to discover each other between nodes) and Unicast (the Unicast module contains a list of hosts to control which nodes need to ping); For all nodes that can become Masters ( node.master: true ) sort according to the Lexicographic order (字典序) of nodeId. Each node in each election sorts the nodes it knows in order, and then selects the first (0th) node. For the time being, it is considered as the master node. If the number of votes for a node reaches a certain value ( the number of eligible master nodes n/2+1) and the node itself elects itself, the node is the master. Otherwise, there will be reelection until the above conditions are met. Supplement: The master node is mainly responsible for managing clusters, nodes, and indexes. It is not responsible for document-level management. data nodes can disable http * ","date":"2022-08-10","objectID":"/elasticsearch/:0:7","series":null,"tags":null,"title":"Elasticssearch","uri":"/elasticsearch/#how-to-elect-the-master-node"},{"categories":["Tech"],"content":"The procedure of indexing When the user write index to the master node. Node 1 got the request and located the doc_id to be in Shard0, it will be directed to the node where the shard is located. Node 3 write on the shard. If succeeded, if will request and direct to the nodest hat store the copy of the shard and write the index. ","date":"2022-08-10","objectID":"/elasticsearch/:0:8","series":null,"tags":null,"title":"Elasticssearch","uri":"/elasticsearch/#the-procedure-of-indexing"},{"categories":["Tech"],"content":"Ways of optimising the index design: write: query optimization: Disable wildcard; Disable batch terms (hundreds of scenarios); Make full use of the inverted index mechanism and try to use the keyword type; When the amount of data is large, the index can be finalized based on time before retrieval; ","date":"2022-08-10","objectID":"/elasticsearch/:0:9","series":null,"tags":null,"title":"Elasticssearch","uri":"/elasticsearch/#ways-of-optimising-the-index"},{"categories":["Tech"],"content":"Ways of optimising the index design: write: query optimization: Disable wildcard; Disable batch terms (hundreds of scenarios); Make full use of the inverted index mechanism and try to use the keyword type; When the amount of data is large, the index can be finalized based on time before retrieval; ","date":"2022-08-10","objectID":"/elasticsearch/:0:9","series":null,"tags":null,"title":"Elasticssearch","uri":"/elasticsearch/#design"},{"categories":["Tech"],"content":"Ways of optimising the index design: write: query optimization: Disable wildcard; Disable batch terms (hundreds of scenarios); Make full use of the inverted index mechanism and try to use the keyword type; When the amount of data is large, the index can be finalized based on time before retrieval; ","date":"2022-08-10","objectID":"/elasticsearch/:0:9","series":null,"tags":null,"title":"Elasticssearch","uri":"/elasticsearch/#write"},{"categories":["Tech"],"content":"Ways of optimising the index design: write: query optimization: Disable wildcard; Disable batch terms (hundreds of scenarios); Make full use of the inverted index mechanism and try to use the keyword type; When the amount of data is large, the index can be finalized based on time before retrieval; ","date":"2022-08-10","objectID":"/elasticsearch/:0:9","series":null,"tags":null,"title":"Elasticssearch","uri":"/elasticsearch/#query-optimization"},{"categories":["Tech"],"content":" Decoupling Event-driven design better performance ","date":"2022-08-10","objectID":"/rabbitmq/:0:0","series":null,"tags":null,"title":"RabbbitMQ","uri":"/rabbitmq/#"},{"categories":["Tech"],"content":"Problems with Synchronous communication(Feign): 1.Coupling 2.can not handle huge amount of requests in a short time(high concurrency). 3.Waste of resources during waiting 4.Cascade failure: service failed, the cluster fails cascadingly ","date":"2022-08-10","objectID":"/rabbitmq/:0:1","series":null,"tags":null,"title":"RabbbitMQ","uri":"/rabbitmq/#problems-with-synchronous-communicationfeign"},{"categories":["Tech"],"content":"Asynchronous(Event driven design): Use Broker to control events. Service publish events and the others subscribed to the broker waiting for events. Better performance/No cascade failures Advantages of asynchronous communication: Low coupling Improve throughput Fault isolation Flow peaking Disadvantages of asynchronous communication: Rely on Broker’s reliability, security and throughput The architecture is complicated, the business does not have an obvious process line, which is not easy to track and manage. ","date":"2022-08-10","objectID":"/rabbitmq/:0:2","series":null,"tags":null,"title":"RabbbitMQ","uri":"/rabbitmq/#asynchronousevent-driven-design"},{"categories":["Tech"],"content":"Rabbitmq Message sending process for basic message queue: Establish a connection Create a channel Declare queues using created channels Use channel to send messages to queues Message receiving process for basic message queue: Establish a connection Create a channel Use the channel to declare the queue Declare consumer method handleDelivery() Use channel to bind consumers to queues ","date":"2022-08-10","objectID":"/rabbitmq/:0:3","series":null,"tags":null,"title":"RabbbitMQ","uri":"/rabbitmq/#rabbitmq"},{"categories":["Tech"],"content":"Spring amqp Use rabbitTemplate to send and get messages queues can use same binding key(which makes them fanout exchange) Describe the difference between Direct switch and Fanout switch? Fanout: routes messages to every queue bound to it. Direct: Determines Which Queue It Is Routed To According To RouteKey. • If multiple queues have the same RoutingKey, it is similar to Fanout function. The common annotations based on @RabbitListener annotation declaration queues and switches: @Queue @Exchange ","date":"2022-08-10","objectID":"/rabbitmq/:0:4","series":null,"tags":null,"title":"RabbbitMQ","uri":"/rabbitmq/#spring-amqp"},{"categories":["Tech"],"content":"TopicExchange In message queues, any object will be serialized. But the performance of the default serialization method(JDK serialization) is not really satisfying(takes too much space). We can use Jackson converter to improve its performance. Note: Some bugs may appear while using Jackson converter./ Note: consumer and publisher must be using the same message converter. ","date":"2022-08-10","objectID":"/rabbitmq/:0:5","series":null,"tags":null,"title":"RabbbitMQ","uri":"/rabbitmq/#topicexchange"},{"categories":["Tech"],"content":"Reliability publisher confirm: publisher-confirm-type = correlate Enabling publisher confirms calls back a confirm function to identify whether the process succeeds. (Producer to exchange) true: ack false: nack To define callback: rabbitTemplate.setConfirmCallback() confirm as an anonymous class method confirm(CorrelationData, ack, cause) ack: whether the exchange have received the message correlationData: info can be manually set (id) publisher-return: exchange to queue: if failed return “return Callback” publisher-returns: true template: mandatory: true When the message from the exchange did not get to the queue, the default mode is throwing the message away, but we can change the mode of returnCallback notify the publisher of the failure. rabbbitTemplate.returnsCallback() default mode: discard message, not returning the returnedMessage method deals with messages in the exchange but not sent to the queues. Consumer Ack: acknowledge = none As soon as the consumer receives the message, it send the ack message (No service successfulness confirmation) acknowledge = “manual” Manually use channel.basicAck() to confirm and channel.basicNack() to let the publisher resend the message. auto Use republishMessageRecover to bind error message and its routing key. How to guarantee reliability？ Enbale publisher confirms and publisher returns to ensure message is sent to the exchange and queues Enable durability to ensure messages are durable Use consumer ack to ensure the consumer gets messages from queues Set acknowlwdge-mode to auto or simply use republishMessageRecoverer to bind error message queue and exchange. flow control ： use manual ack mode listener-container prefetch = 1000 to control the flow. (The consumer get 1000 messages every time, need manual confirmation to continue) TTL: time to live: before the message got discarded. x-message-tt queue ttl can be set in the confiogurations.(Delete all messages in the queue) ms message ttl need to implement messagePostProcessor to setExpiration in the message properties(When the expired message is on top of the queue, it will be deleted.) Setting queue ttl and message ttl at the same time, it will use the shorter one. DLX dead letter exchange When a message is dead, it can be sent to another exchange. It is called DLX. Dead letter queue summary: There is no difference between dead letter switches and dead letter queues and Xitong. When the message becomes a dead letter, if the queue is bound to a dead letter switch, the message will be rerouted to the dead letter team by the dead letter switch. Messages become dead letter: exceeded limit of the queue consumer refused(basicNack) and requeue = false queue expired/message expired parameters: x-dead-letter-exchange and x-dead-letter-routing-key Dead letter queue use case In the shopping cart case, users place their order and need to pay within 15 min or so. In this case, the message TTL can be set to 15 min(in order.delay.queue). After that, the messages will be sent to dead letter queue(order.release.queue) and the overtime order cancel can be realized. @Bean public Queue orderDelayQueue(){ HashMap\u003cString, Object\u003e args = new HashMap\u003c\u003e(); args.put(“x-dead-letter-exchange”,“order-event-exchange”); args.put(“x-dead-letter-routing-key”,“order.release.order”); args.put(“x-message-ttl”,60000); return new Queue(“order.delay.queue”, true, false, false, args); } @Bean public Binding stockReleaseBinding(){ return new Binding(“order.release.order.queue”, Binding.DestinationType.QUEUE, “order-event-exchange”, “order.release.order”, null); } Use plugin to create delayqueue. Install: Use: In the listener, declare delayed = true in th exchange setting. In the publisher, set message property: .setHeader(“x-delay”, 5000) The message will be delayed, but the publisher returns will get errors for not able to send the message directly to the exchange. In the message, the ReceivedDelay property will be the x-delay we set. So one possible solution would be using","date":"2022-08-10","objectID":"/rabbitmq/:0:6","series":null,"tags":null,"title":"RabbbitMQ","uri":"/rabbitmq/#reliability"},{"categories":["Tech"],"content":"Reliability publisher confirm: publisher-confirm-type = correlate Enabling publisher confirms calls back a confirm function to identify whether the process succeeds. (Producer to exchange) true: ack false: nack To define callback: rabbitTemplate.setConfirmCallback() confirm as an anonymous class method confirm(CorrelationData, ack, cause) ack: whether the exchange have received the message correlationData: info can be manually set (id) publisher-return: exchange to queue: if failed return “return Callback” publisher-returns: true template: mandatory: true When the message from the exchange did not get to the queue, the default mode is throwing the message away, but we can change the mode of returnCallback notify the publisher of the failure. rabbbitTemplate.returnsCallback() default mode: discard message, not returning the returnedMessage method deals with messages in the exchange but not sent to the queues. Consumer Ack: acknowledge = none As soon as the consumer receives the message, it send the ack message (No service successfulness confirmation) acknowledge = “manual” Manually use channel.basicAck() to confirm and channel.basicNack() to let the publisher resend the message. auto Use republishMessageRecover to bind error message and its routing key. How to guarantee reliability？ Enbale publisher confirms and publisher returns to ensure message is sent to the exchange and queues Enable durability to ensure messages are durable Use consumer ack to ensure the consumer gets messages from queues Set acknowlwdge-mode to auto or simply use republishMessageRecoverer to bind error message queue and exchange. flow control ： use manual ack mode listener-container prefetch = 1000 to control the flow. (The consumer get 1000 messages every time, need manual confirmation to continue) TTL: time to live: before the message got discarded. x-message-tt queue ttl can be set in the confiogurations.(Delete all messages in the queue) ms message ttl need to implement messagePostProcessor to setExpiration in the message properties(When the expired message is on top of the queue, it will be deleted.) Setting queue ttl and message ttl at the same time, it will use the shorter one. DLX dead letter exchange When a message is dead, it can be sent to another exchange. It is called DLX. Dead letter queue summary: There is no difference between dead letter switches and dead letter queues and Xitong. When the message becomes a dead letter, if the queue is bound to a dead letter switch, the message will be rerouted to the dead letter team by the dead letter switch. Messages become dead letter: exceeded limit of the queue consumer refused(basicNack) and requeue = false queue expired/message expired parameters: x-dead-letter-exchange and x-dead-letter-routing-key Dead letter queue use case In the shopping cart case, users place their order and need to pay within 15 min or so. In this case, the message TTL can be set to 15 min(in order.delay.queue). After that, the messages will be sent to dead letter queue(order.release.queue) and the overtime order cancel can be realized. @Bean public Queue orderDelayQueue(){ HashMapargs = new HashMap(); args.put(“x-dead-letter-exchange”,“order-event-exchange”); args.put(“x-dead-letter-routing-key”,“order.release.order”); args.put(“x-message-ttl”,60000); return new Queue(“order.delay.queue”, true, false, false, args); } @Bean public Binding stockReleaseBinding(){ return new Binding(“order.release.order.queue”, Binding.DestinationType.QUEUE, “order-event-exchange”, “order.release.order”, null); } Use plugin to create delayqueue. Install: Use: In the listener, declare delayed = true in th exchange setting. In the publisher, set message property: .setHeader(“x-delay”, 5000) The message will be delayed, but the publisher returns will get errors for not able to send the message directly to the exchange. In the message, the ReceivedDelay property will be the x-delay we set. So one possible solution would be using","date":"2022-08-10","objectID":"/rabbitmq/:0:6","series":null,"tags":null,"title":"RabbbitMQ","uri":"/rabbitmq/#publisher-confirm"},{"categories":["Tech"],"content":"Reliability publisher confirm: publisher-confirm-type = correlate Enabling publisher confirms calls back a confirm function to identify whether the process succeeds. (Producer to exchange) true: ack false: nack To define callback: rabbitTemplate.setConfirmCallback() confirm as an anonymous class method confirm(CorrelationData, ack, cause) ack: whether the exchange have received the message correlationData: info can be manually set (id) publisher-return: exchange to queue: if failed return “return Callback” publisher-returns: true template: mandatory: true When the message from the exchange did not get to the queue, the default mode is throwing the message away, but we can change the mode of returnCallback notify the publisher of the failure. rabbbitTemplate.returnsCallback() default mode: discard message, not returning the returnedMessage method deals with messages in the exchange but not sent to the queues. Consumer Ack: acknowledge = none As soon as the consumer receives the message, it send the ack message (No service successfulness confirmation) acknowledge = “manual” Manually use channel.basicAck() to confirm and channel.basicNack() to let the publisher resend the message. auto Use republishMessageRecover to bind error message and its routing key. How to guarantee reliability？ Enbale publisher confirms and publisher returns to ensure message is sent to the exchange and queues Enable durability to ensure messages are durable Use consumer ack to ensure the consumer gets messages from queues Set acknowlwdge-mode to auto or simply use republishMessageRecoverer to bind error message queue and exchange. flow control ： use manual ack mode listener-container prefetch = 1000 to control the flow. (The consumer get 1000 messages every time, need manual confirmation to continue) TTL: time to live: before the message got discarded. x-message-tt queue ttl can be set in the confiogurations.(Delete all messages in the queue) ms message ttl need to implement messagePostProcessor to setExpiration in the message properties(When the expired message is on top of the queue, it will be deleted.) Setting queue ttl and message ttl at the same time, it will use the shorter one. DLX dead letter exchange When a message is dead, it can be sent to another exchange. It is called DLX. Dead letter queue summary: There is no difference between dead letter switches and dead letter queues and Xitong. When the message becomes a dead letter, if the queue is bound to a dead letter switch, the message will be rerouted to the dead letter team by the dead letter switch. Messages become dead letter: exceeded limit of the queue consumer refused(basicNack) and requeue = false queue expired/message expired parameters: x-dead-letter-exchange and x-dead-letter-routing-key Dead letter queue use case In the shopping cart case, users place their order and need to pay within 15 min or so. In this case, the message TTL can be set to 15 min(in order.delay.queue). After that, the messages will be sent to dead letter queue(order.release.queue) and the overtime order cancel can be realized. @Bean public Queue orderDelayQueue(){ HashMapargs = new HashMap(); args.put(“x-dead-letter-exchange”,“order-event-exchange”); args.put(“x-dead-letter-routing-key”,“order.release.order”); args.put(“x-message-ttl”,60000); return new Queue(“order.delay.queue”, true, false, false, args); } @Bean public Binding stockReleaseBinding(){ return new Binding(“order.release.order.queue”, Binding.DestinationType.QUEUE, “order-event-exchange”, “order.release.order”, null); } Use plugin to create delayqueue. Install: Use: In the listener, declare delayed = true in th exchange setting. In the publisher, set message property: .setHeader(“x-delay”, 5000) The message will be delayed, but the publisher returns will get errors for not able to send the message directly to the exchange. In the message, the ReceivedDelay property will be the x-delay we set. So one possible solution would be using","date":"2022-08-10","objectID":"/rabbitmq/:0:6","series":null,"tags":null,"title":"RabbbitMQ","uri":"/rabbitmq/#publisher-return"},{"categories":["Tech"],"content":"Reliability publisher confirm: publisher-confirm-type = correlate Enabling publisher confirms calls back a confirm function to identify whether the process succeeds. (Producer to exchange) true: ack false: nack To define callback: rabbitTemplate.setConfirmCallback() confirm as an anonymous class method confirm(CorrelationData, ack, cause) ack: whether the exchange have received the message correlationData: info can be manually set (id) publisher-return: exchange to queue: if failed return “return Callback” publisher-returns: true template: mandatory: true When the message from the exchange did not get to the queue, the default mode is throwing the message away, but we can change the mode of returnCallback notify the publisher of the failure. rabbbitTemplate.returnsCallback() default mode: discard message, not returning the returnedMessage method deals with messages in the exchange but not sent to the queues. Consumer Ack: acknowledge = none As soon as the consumer receives the message, it send the ack message (No service successfulness confirmation) acknowledge = “manual” Manually use channel.basicAck() to confirm and channel.basicNack() to let the publisher resend the message. auto Use republishMessageRecover to bind error message and its routing key. How to guarantee reliability？ Enbale publisher confirms and publisher returns to ensure message is sent to the exchange and queues Enable durability to ensure messages are durable Use consumer ack to ensure the consumer gets messages from queues Set acknowlwdge-mode to auto or simply use republishMessageRecoverer to bind error message queue and exchange. flow control ： use manual ack mode listener-container prefetch = 1000 to control the flow. (The consumer get 1000 messages every time, need manual confirmation to continue) TTL: time to live: before the message got discarded. x-message-tt queue ttl can be set in the confiogurations.(Delete all messages in the queue) ms message ttl need to implement messagePostProcessor to setExpiration in the message properties(When the expired message is on top of the queue, it will be deleted.) Setting queue ttl and message ttl at the same time, it will use the shorter one. DLX dead letter exchange When a message is dead, it can be sent to another exchange. It is called DLX. Dead letter queue summary: There is no difference between dead letter switches and dead letter queues and Xitong. When the message becomes a dead letter, if the queue is bound to a dead letter switch, the message will be rerouted to the dead letter team by the dead letter switch. Messages become dead letter: exceeded limit of the queue consumer refused(basicNack) and requeue = false queue expired/message expired parameters: x-dead-letter-exchange and x-dead-letter-routing-key Dead letter queue use case In the shopping cart case, users place their order and need to pay within 15 min or so. In this case, the message TTL can be set to 15 min(in order.delay.queue). After that, the messages will be sent to dead letter queue(order.release.queue) and the overtime order cancel can be realized. @Bean public Queue orderDelayQueue(){ HashMapargs = new HashMap(); args.put(“x-dead-letter-exchange”,“order-event-exchange”); args.put(“x-dead-letter-routing-key”,“order.release.order”); args.put(“x-message-ttl”,60000); return new Queue(“order.delay.queue”, true, false, false, args); } @Bean public Binding stockReleaseBinding(){ return new Binding(“order.release.order.queue”, Binding.DestinationType.QUEUE, “order-event-exchange”, “order.release.order”, null); } Use plugin to create delayqueue. Install: Use: In the listener, declare delayed = true in th exchange setting. In the publisher, set message property: .setHeader(“x-delay”, 5000) The message will be delayed, but the publisher returns will get errors for not able to send the message directly to the exchange. In the message, the ReceivedDelay property will be the x-delay we set. So one possible solution would be using","date":"2022-08-10","objectID":"/rabbitmq/:0:6","series":null,"tags":null,"title":"RabbbitMQ","uri":"/rabbitmq/#consumer-ack"},{"categories":["Tech"],"content":"Reliability publisher confirm: publisher-confirm-type = correlate Enabling publisher confirms calls back a confirm function to identify whether the process succeeds. (Producer to exchange) true: ack false: nack To define callback: rabbitTemplate.setConfirmCallback() confirm as an anonymous class method confirm(CorrelationData, ack, cause) ack: whether the exchange have received the message correlationData: info can be manually set (id) publisher-return: exchange to queue: if failed return “return Callback” publisher-returns: true template: mandatory: true When the message from the exchange did not get to the queue, the default mode is throwing the message away, but we can change the mode of returnCallback notify the publisher of the failure. rabbbitTemplate.returnsCallback() default mode: discard message, not returning the returnedMessage method deals with messages in the exchange but not sent to the queues. Consumer Ack: acknowledge = none As soon as the consumer receives the message, it send the ack message (No service successfulness confirmation) acknowledge = “manual” Manually use channel.basicAck() to confirm and channel.basicNack() to let the publisher resend the message. auto Use republishMessageRecover to bind error message and its routing key. How to guarantee reliability？ Enbale publisher confirms and publisher returns to ensure message is sent to the exchange and queues Enable durability to ensure messages are durable Use consumer ack to ensure the consumer gets messages from queues Set acknowlwdge-mode to auto or simply use republishMessageRecoverer to bind error message queue and exchange. flow control ： use manual ack mode listener-container prefetch = 1000 to control the flow. (The consumer get 1000 messages every time, need manual confirmation to continue) TTL: time to live: before the message got discarded. x-message-tt queue ttl can be set in the confiogurations.(Delete all messages in the queue) ms message ttl need to implement messagePostProcessor to setExpiration in the message properties(When the expired message is on top of the queue, it will be deleted.) Setting queue ttl and message ttl at the same time, it will use the shorter one. DLX dead letter exchange When a message is dead, it can be sent to another exchange. It is called DLX. Dead letter queue summary: There is no difference between dead letter switches and dead letter queues and Xitong. When the message becomes a dead letter, if the queue is bound to a dead letter switch, the message will be rerouted to the dead letter team by the dead letter switch. Messages become dead letter: exceeded limit of the queue consumer refused(basicNack) and requeue = false queue expired/message expired parameters: x-dead-letter-exchange and x-dead-letter-routing-key Dead letter queue use case In the shopping cart case, users place their order and need to pay within 15 min or so. In this case, the message TTL can be set to 15 min(in order.delay.queue). After that, the messages will be sent to dead letter queue(order.release.queue) and the overtime order cancel can be realized. @Bean public Queue orderDelayQueue(){ HashMapargs = new HashMap(); args.put(“x-dead-letter-exchange”,“order-event-exchange”); args.put(“x-dead-letter-routing-key”,“order.release.order”); args.put(“x-message-ttl”,60000); return new Queue(“order.delay.queue”, true, false, false, args); } @Bean public Binding stockReleaseBinding(){ return new Binding(“order.release.order.queue”, Binding.DestinationType.QUEUE, “order-event-exchange”, “order.release.order”, null); } Use plugin to create delayqueue. Install: Use: In the listener, declare delayed = true in th exchange setting. In the publisher, set message property: .setHeader(“x-delay”, 5000) The message will be delayed, but the publisher returns will get errors for not able to send the message directly to the exchange. In the message, the ReceivedDelay property will be the x-delay we set. So one possible solution would be using","date":"2022-08-10","objectID":"/rabbitmq/:0:6","series":null,"tags":null,"title":"RabbbitMQ","uri":"/rabbitmq/#ttl"},{"categories":["Tech"],"content":"Reliability publisher confirm: publisher-confirm-type = correlate Enabling publisher confirms calls back a confirm function to identify whether the process succeeds. (Producer to exchange) true: ack false: nack To define callback: rabbitTemplate.setConfirmCallback() confirm as an anonymous class method confirm(CorrelationData, ack, cause) ack: whether the exchange have received the message correlationData: info can be manually set (id) publisher-return: exchange to queue: if failed return “return Callback” publisher-returns: true template: mandatory: true When the message from the exchange did not get to the queue, the default mode is throwing the message away, but we can change the mode of returnCallback notify the publisher of the failure. rabbbitTemplate.returnsCallback() default mode: discard message, not returning the returnedMessage method deals with messages in the exchange but not sent to the queues. Consumer Ack: acknowledge = none As soon as the consumer receives the message, it send the ack message (No service successfulness confirmation) acknowledge = “manual” Manually use channel.basicAck() to confirm and channel.basicNack() to let the publisher resend the message. auto Use republishMessageRecover to bind error message and its routing key. How to guarantee reliability？ Enbale publisher confirms and publisher returns to ensure message is sent to the exchange and queues Enable durability to ensure messages are durable Use consumer ack to ensure the consumer gets messages from queues Set acknowlwdge-mode to auto or simply use republishMessageRecoverer to bind error message queue and exchange. flow control ： use manual ack mode listener-container prefetch = 1000 to control the flow. (The consumer get 1000 messages every time, need manual confirmation to continue) TTL: time to live: before the message got discarded. x-message-tt queue ttl can be set in the confiogurations.(Delete all messages in the queue) ms message ttl need to implement messagePostProcessor to setExpiration in the message properties(When the expired message is on top of the queue, it will be deleted.) Setting queue ttl and message ttl at the same time, it will use the shorter one. DLX dead letter exchange When a message is dead, it can be sent to another exchange. It is called DLX. Dead letter queue summary: There is no difference between dead letter switches and dead letter queues and Xitong. When the message becomes a dead letter, if the queue is bound to a dead letter switch, the message will be rerouted to the dead letter team by the dead letter switch. Messages become dead letter: exceeded limit of the queue consumer refused(basicNack) and requeue = false queue expired/message expired parameters: x-dead-letter-exchange and x-dead-letter-routing-key Dead letter queue use case In the shopping cart case, users place their order and need to pay within 15 min or so. In this case, the message TTL can be set to 15 min(in order.delay.queue). After that, the messages will be sent to dead letter queue(order.release.queue) and the overtime order cancel can be realized. @Bean public Queue orderDelayQueue(){ HashMapargs = new HashMap(); args.put(“x-dead-letter-exchange”,“order-event-exchange”); args.put(“x-dead-letter-routing-key”,“order.release.order”); args.put(“x-message-ttl”,60000); return new Queue(“order.delay.queue”, true, false, false, args); } @Bean public Binding stockReleaseBinding(){ return new Binding(“order.release.order.queue”, Binding.DestinationType.QUEUE, “order-event-exchange”, “order.release.order”, null); } Use plugin to create delayqueue. Install: Use: In the listener, declare delayed = true in th exchange setting. In the publisher, set message property: .setHeader(“x-delay”, 5000) The message will be delayed, but the publisher returns will get errors for not able to send the message directly to the exchange. In the message, the ReceivedDelay property will be the x-delay we set. So one possible solution would be using","date":"2022-08-10","objectID":"/rabbitmq/:0:6","series":null,"tags":null,"title":"RabbbitMQ","uri":"/rabbitmq/#dlx-dead-letter-exchange"},{"categories":["Tech"],"content":"Reliability publisher confirm: publisher-confirm-type = correlate Enabling publisher confirms calls back a confirm function to identify whether the process succeeds. (Producer to exchange) true: ack false: nack To define callback: rabbitTemplate.setConfirmCallback() confirm as an anonymous class method confirm(CorrelationData, ack, cause) ack: whether the exchange have received the message correlationData: info can be manually set (id) publisher-return: exchange to queue: if failed return “return Callback” publisher-returns: true template: mandatory: true When the message from the exchange did not get to the queue, the default mode is throwing the message away, but we can change the mode of returnCallback notify the publisher of the failure. rabbbitTemplate.returnsCallback() default mode: discard message, not returning the returnedMessage method deals with messages in the exchange but not sent to the queues. Consumer Ack: acknowledge = none As soon as the consumer receives the message, it send the ack message (No service successfulness confirmation) acknowledge = “manual” Manually use channel.basicAck() to confirm and channel.basicNack() to let the publisher resend the message. auto Use republishMessageRecover to bind error message and its routing key. How to guarantee reliability？ Enbale publisher confirms and publisher returns to ensure message is sent to the exchange and queues Enable durability to ensure messages are durable Use consumer ack to ensure the consumer gets messages from queues Set acknowlwdge-mode to auto or simply use republishMessageRecoverer to bind error message queue and exchange. flow control ： use manual ack mode listener-container prefetch = 1000 to control the flow. (The consumer get 1000 messages every time, need manual confirmation to continue) TTL: time to live: before the message got discarded. x-message-tt queue ttl can be set in the confiogurations.(Delete all messages in the queue) ms message ttl need to implement messagePostProcessor to setExpiration in the message properties(When the expired message is on top of the queue, it will be deleted.) Setting queue ttl and message ttl at the same time, it will use the shorter one. DLX dead letter exchange When a message is dead, it can be sent to another exchange. It is called DLX. Dead letter queue summary: There is no difference between dead letter switches and dead letter queues and Xitong. When the message becomes a dead letter, if the queue is bound to a dead letter switch, the message will be rerouted to the dead letter team by the dead letter switch. Messages become dead letter: exceeded limit of the queue consumer refused(basicNack) and requeue = false queue expired/message expired parameters: x-dead-letter-exchange and x-dead-letter-routing-key Dead letter queue use case In the shopping cart case, users place their order and need to pay within 15 min or so. In this case, the message TTL can be set to 15 min(in order.delay.queue). After that, the messages will be sent to dead letter queue(order.release.queue) and the overtime order cancel can be realized. @Bean public Queue orderDelayQueue(){ HashMapargs = new HashMap(); args.put(“x-dead-letter-exchange”,“order-event-exchange”); args.put(“x-dead-letter-routing-key”,“order.release.order”); args.put(“x-message-ttl”,60000); return new Queue(“order.delay.queue”, true, false, false, args); } @Bean public Binding stockReleaseBinding(){ return new Binding(“order.release.order.queue”, Binding.DestinationType.QUEUE, “order-event-exchange”, “order.release.order”, null); } Use plugin to create delayqueue. Install: Use: In the listener, declare delayed = true in th exchange setting. In the publisher, set message property: .setHeader(“x-delay”, 5000) The message will be delayed, but the publisher returns will get errors for not able to send the message directly to the exchange. In the message, the ReceivedDelay property will be the x-delay we set. So one possible solution would be using","date":"2022-08-10","objectID":"/rabbitmq/:0:6","series":null,"tags":null,"title":"RabbbitMQ","uri":"/rabbitmq/#dead-letter-queue-summary"},{"categories":["Tech"],"content":"Reliability publisher confirm: publisher-confirm-type = correlate Enabling publisher confirms calls back a confirm function to identify whether the process succeeds. (Producer to exchange) true: ack false: nack To define callback: rabbitTemplate.setConfirmCallback() confirm as an anonymous class method confirm(CorrelationData, ack, cause) ack: whether the exchange have received the message correlationData: info can be manually set (id) publisher-return: exchange to queue: if failed return “return Callback” publisher-returns: true template: mandatory: true When the message from the exchange did not get to the queue, the default mode is throwing the message away, but we can change the mode of returnCallback notify the publisher of the failure. rabbbitTemplate.returnsCallback() default mode: discard message, not returning the returnedMessage method deals with messages in the exchange but not sent to the queues. Consumer Ack: acknowledge = none As soon as the consumer receives the message, it send the ack message (No service successfulness confirmation) acknowledge = “manual” Manually use channel.basicAck() to confirm and channel.basicNack() to let the publisher resend the message. auto Use republishMessageRecover to bind error message and its routing key. How to guarantee reliability？ Enbale publisher confirms and publisher returns to ensure message is sent to the exchange and queues Enable durability to ensure messages are durable Use consumer ack to ensure the consumer gets messages from queues Set acknowlwdge-mode to auto or simply use republishMessageRecoverer to bind error message queue and exchange. flow control ： use manual ack mode listener-container prefetch = 1000 to control the flow. (The consumer get 1000 messages every time, need manual confirmation to continue) TTL: time to live: before the message got discarded. x-message-tt queue ttl can be set in the confiogurations.(Delete all messages in the queue) ms message ttl need to implement messagePostProcessor to setExpiration in the message properties(When the expired message is on top of the queue, it will be deleted.) Setting queue ttl and message ttl at the same time, it will use the shorter one. DLX dead letter exchange When a message is dead, it can be sent to another exchange. It is called DLX. Dead letter queue summary: There is no difference between dead letter switches and dead letter queues and Xitong. When the message becomes a dead letter, if the queue is bound to a dead letter switch, the message will be rerouted to the dead letter team by the dead letter switch. Messages become dead letter: exceeded limit of the queue consumer refused(basicNack) and requeue = false queue expired/message expired parameters: x-dead-letter-exchange and x-dead-letter-routing-key Dead letter queue use case In the shopping cart case, users place their order and need to pay within 15 min or so. In this case, the message TTL can be set to 15 min(in order.delay.queue). After that, the messages will be sent to dead letter queue(order.release.queue) and the overtime order cancel can be realized. @Bean public Queue orderDelayQueue(){ HashMapargs = new HashMap(); args.put(“x-dead-letter-exchange”,“order-event-exchange”); args.put(“x-dead-letter-routing-key”,“order.release.order”); args.put(“x-message-ttl”,60000); return new Queue(“order.delay.queue”, true, false, false, args); } @Bean public Binding stockReleaseBinding(){ return new Binding(“order.release.order.queue”, Binding.DestinationType.QUEUE, “order-event-exchange”, “order.release.order”, null); } Use plugin to create delayqueue. Install: Use: In the listener, declare delayed = true in th exchange setting. In the publisher, set message property: .setHeader(“x-delay”, 5000) The message will be delayed, but the publisher returns will get errors for not able to send the message directly to the exchange. In the message, the ReceivedDelay property will be the x-delay we set. So one possible solution would be using","date":"2022-08-10","objectID":"/rabbitmq/:0:6","series":null,"tags":null,"title":"RabbbitMQ","uri":"/rabbitmq/#dead-letter-queue-use-case"},{"categories":["Tech"],"content":"Reliability publisher confirm: publisher-confirm-type = correlate Enabling publisher confirms calls back a confirm function to identify whether the process succeeds. (Producer to exchange) true: ack false: nack To define callback: rabbitTemplate.setConfirmCallback() confirm as an anonymous class method confirm(CorrelationData, ack, cause) ack: whether the exchange have received the message correlationData: info can be manually set (id) publisher-return: exchange to queue: if failed return “return Callback” publisher-returns: true template: mandatory: true When the message from the exchange did not get to the queue, the default mode is throwing the message away, but we can change the mode of returnCallback notify the publisher of the failure. rabbbitTemplate.returnsCallback() default mode: discard message, not returning the returnedMessage method deals with messages in the exchange but not sent to the queues. Consumer Ack: acknowledge = none As soon as the consumer receives the message, it send the ack message (No service successfulness confirmation) acknowledge = “manual” Manually use channel.basicAck() to confirm and channel.basicNack() to let the publisher resend the message. auto Use republishMessageRecover to bind error message and its routing key. How to guarantee reliability？ Enbale publisher confirms and publisher returns to ensure message is sent to the exchange and queues Enable durability to ensure messages are durable Use consumer ack to ensure the consumer gets messages from queues Set acknowlwdge-mode to auto or simply use republishMessageRecoverer to bind error message queue and exchange. flow control ： use manual ack mode listener-container prefetch = 1000 to control the flow. (The consumer get 1000 messages every time, need manual confirmation to continue) TTL: time to live: before the message got discarded. x-message-tt queue ttl can be set in the confiogurations.(Delete all messages in the queue) ms message ttl need to implement messagePostProcessor to setExpiration in the message properties(When the expired message is on top of the queue, it will be deleted.) Setting queue ttl and message ttl at the same time, it will use the shorter one. DLX dead letter exchange When a message is dead, it can be sent to another exchange. It is called DLX. Dead letter queue summary: There is no difference between dead letter switches and dead letter queues and Xitong. When the message becomes a dead letter, if the queue is bound to a dead letter switch, the message will be rerouted to the dead letter team by the dead letter switch. Messages become dead letter: exceeded limit of the queue consumer refused(basicNack) and requeue = false queue expired/message expired parameters: x-dead-letter-exchange and x-dead-letter-routing-key Dead letter queue use case In the shopping cart case, users place their order and need to pay within 15 min or so. In this case, the message TTL can be set to 15 min(in order.delay.queue). After that, the messages will be sent to dead letter queue(order.release.queue) and the overtime order cancel can be realized. @Bean public Queue orderDelayQueue(){ HashMapargs = new HashMap(); args.put(“x-dead-letter-exchange”,“order-event-exchange”); args.put(“x-dead-letter-routing-key”,“order.release.order”); args.put(“x-message-ttl”,60000); return new Queue(“order.delay.queue”, true, false, false, args); } @Bean public Binding stockReleaseBinding(){ return new Binding(“order.release.order.queue”, Binding.DestinationType.QUEUE, “order-event-exchange”, “order.release.order”, null); } Use plugin to create delayqueue. Install: Use: In the listener, declare delayed = true in th exchange setting. In the publisher, set message property: .setHeader(“x-delay”, 5000) The message will be delayed, but the publisher returns will get errors for not able to send the message directly to the exchange. In the message, the ReceivedDelay property will be the x-delay we set. So one possible solution would be using","date":"2022-08-10","objectID":"/rabbitmq/:0:6","series":null,"tags":null,"title":"RabbbitMQ","uri":"/rabbitmq/#use-plugin-to-create-delayqueue"},{"categories":["Tech"],"content":"Reliability publisher confirm: publisher-confirm-type = correlate Enabling publisher confirms calls back a confirm function to identify whether the process succeeds. (Producer to exchange) true: ack false: nack To define callback: rabbitTemplate.setConfirmCallback() confirm as an anonymous class method confirm(CorrelationData, ack, cause) ack: whether the exchange have received the message correlationData: info can be manually set (id) publisher-return: exchange to queue: if failed return “return Callback” publisher-returns: true template: mandatory: true When the message from the exchange did not get to the queue, the default mode is throwing the message away, but we can change the mode of returnCallback notify the publisher of the failure. rabbbitTemplate.returnsCallback() default mode: discard message, not returning the returnedMessage method deals with messages in the exchange but not sent to the queues. Consumer Ack: acknowledge = none As soon as the consumer receives the message, it send the ack message (No service successfulness confirmation) acknowledge = “manual” Manually use channel.basicAck() to confirm and channel.basicNack() to let the publisher resend the message. auto Use republishMessageRecover to bind error message and its routing key. How to guarantee reliability？ Enbale publisher confirms and publisher returns to ensure message is sent to the exchange and queues Enable durability to ensure messages are durable Use consumer ack to ensure the consumer gets messages from queues Set acknowlwdge-mode to auto or simply use republishMessageRecoverer to bind error message queue and exchange. flow control ： use manual ack mode listener-container prefetch = 1000 to control the flow. (The consumer get 1000 messages every time, need manual confirmation to continue) TTL: time to live: before the message got discarded. x-message-tt queue ttl can be set in the confiogurations.(Delete all messages in the queue) ms message ttl need to implement messagePostProcessor to setExpiration in the message properties(When the expired message is on top of the queue, it will be deleted.) Setting queue ttl and message ttl at the same time, it will use the shorter one. DLX dead letter exchange When a message is dead, it can be sent to another exchange. It is called DLX. Dead letter queue summary: There is no difference between dead letter switches and dead letter queues and Xitong. When the message becomes a dead letter, if the queue is bound to a dead letter switch, the message will be rerouted to the dead letter team by the dead letter switch. Messages become dead letter: exceeded limit of the queue consumer refused(basicNack) and requeue = false queue expired/message expired parameters: x-dead-letter-exchange and x-dead-letter-routing-key Dead letter queue use case In the shopping cart case, users place their order and need to pay within 15 min or so. In this case, the message TTL can be set to 15 min(in order.delay.queue). After that, the messages will be sent to dead letter queue(order.release.queue) and the overtime order cancel can be realized. @Bean public Queue orderDelayQueue(){ HashMapargs = new HashMap(); args.put(“x-dead-letter-exchange”,“order-event-exchange”); args.put(“x-dead-letter-routing-key”,“order.release.order”); args.put(“x-message-ttl”,60000); return new Queue(“order.delay.queue”, true, false, false, args); } @Bean public Binding stockReleaseBinding(){ return new Binding(“order.release.order.queue”, Binding.DestinationType.QUEUE, “order-event-exchange”, “order.release.order”, null); } Use plugin to create delayqueue. Install: Use: In the listener, declare delayed = true in th exchange setting. In the publisher, set message property: .setHeader(“x-delay”, 5000) The message will be delayed, but the publisher returns will get errors for not able to send the message directly to the exchange. In the message, the ReceivedDelay property will be the x-delay we set. So one possible solution would be using","date":"2022-08-10","objectID":"/rabbitmq/:0:6","series":null,"tags":null,"title":"RabbbitMQ","uri":"/rabbitmq/#install"},{"categories":["Tech"],"content":"Reliability publisher confirm: publisher-confirm-type = correlate Enabling publisher confirms calls back a confirm function to identify whether the process succeeds. (Producer to exchange) true: ack false: nack To define callback: rabbitTemplate.setConfirmCallback() confirm as an anonymous class method confirm(CorrelationData, ack, cause) ack: whether the exchange have received the message correlationData: info can be manually set (id) publisher-return: exchange to queue: if failed return “return Callback” publisher-returns: true template: mandatory: true When the message from the exchange did not get to the queue, the default mode is throwing the message away, but we can change the mode of returnCallback notify the publisher of the failure. rabbbitTemplate.returnsCallback() default mode: discard message, not returning the returnedMessage method deals with messages in the exchange but not sent to the queues. Consumer Ack: acknowledge = none As soon as the consumer receives the message, it send the ack message (No service successfulness confirmation) acknowledge = “manual” Manually use channel.basicAck() to confirm and channel.basicNack() to let the publisher resend the message. auto Use republishMessageRecover to bind error message and its routing key. How to guarantee reliability？ Enbale publisher confirms and publisher returns to ensure message is sent to the exchange and queues Enable durability to ensure messages are durable Use consumer ack to ensure the consumer gets messages from queues Set acknowlwdge-mode to auto or simply use republishMessageRecoverer to bind error message queue and exchange. flow control ： use manual ack mode listener-container prefetch = 1000 to control the flow. (The consumer get 1000 messages every time, need manual confirmation to continue) TTL: time to live: before the message got discarded. x-message-tt queue ttl can be set in the confiogurations.(Delete all messages in the queue) ms message ttl need to implement messagePostProcessor to setExpiration in the message properties(When the expired message is on top of the queue, it will be deleted.) Setting queue ttl and message ttl at the same time, it will use the shorter one. DLX dead letter exchange When a message is dead, it can be sent to another exchange. It is called DLX. Dead letter queue summary: There is no difference between dead letter switches and dead letter queues and Xitong. When the message becomes a dead letter, if the queue is bound to a dead letter switch, the message will be rerouted to the dead letter team by the dead letter switch. Messages become dead letter: exceeded limit of the queue consumer refused(basicNack) and requeue = false queue expired/message expired parameters: x-dead-letter-exchange and x-dead-letter-routing-key Dead letter queue use case In the shopping cart case, users place their order and need to pay within 15 min or so. In this case, the message TTL can be set to 15 min(in order.delay.queue). After that, the messages will be sent to dead letter queue(order.release.queue) and the overtime order cancel can be realized. @Bean public Queue orderDelayQueue(){ HashMapargs = new HashMap(); args.put(“x-dead-letter-exchange”,“order-event-exchange”); args.put(“x-dead-letter-routing-key”,“order.release.order”); args.put(“x-message-ttl”,60000); return new Queue(“order.delay.queue”, true, false, false, args); } @Bean public Binding stockReleaseBinding(){ return new Binding(“order.release.order.queue”, Binding.DestinationType.QUEUE, “order-event-exchange”, “order.release.order”, null); } Use plugin to create delayqueue. Install: Use: In the listener, declare delayed = true in th exchange setting. In the publisher, set message property: .setHeader(“x-delay”, 5000) The message will be delayed, but the publisher returns will get errors for not able to send the message directly to the exchange. In the message, the ReceivedDelay property will be the x-delay we set. So one possible solution would be using","date":"2022-08-10","objectID":"/rabbitmq/:0:6","series":null,"tags":null,"title":"RabbbitMQ","uri":"/rabbitmq/#use"},{"categories":["Tech"],"content":"Reliability publisher confirm: publisher-confirm-type = correlate Enabling publisher confirms calls back a confirm function to identify whether the process succeeds. (Producer to exchange) true: ack false: nack To define callback: rabbitTemplate.setConfirmCallback() confirm as an anonymous class method confirm(CorrelationData, ack, cause) ack: whether the exchange have received the message correlationData: info can be manually set (id) publisher-return: exchange to queue: if failed return “return Callback” publisher-returns: true template: mandatory: true When the message from the exchange did not get to the queue, the default mode is throwing the message away, but we can change the mode of returnCallback notify the publisher of the failure. rabbbitTemplate.returnsCallback() default mode: discard message, not returning the returnedMessage method deals with messages in the exchange but not sent to the queues. Consumer Ack: acknowledge = none As soon as the consumer receives the message, it send the ack message (No service successfulness confirmation) acknowledge = “manual” Manually use channel.basicAck() to confirm and channel.basicNack() to let the publisher resend the message. auto Use republishMessageRecover to bind error message and its routing key. How to guarantee reliability？ Enbale publisher confirms and publisher returns to ensure message is sent to the exchange and queues Enable durability to ensure messages are durable Use consumer ack to ensure the consumer gets messages from queues Set acknowlwdge-mode to auto or simply use republishMessageRecoverer to bind error message queue and exchange. flow control ： use manual ack mode listener-container prefetch = 1000 to control the flow. (The consumer get 1000 messages every time, need manual confirmation to continue) TTL: time to live: before the message got discarded. x-message-tt queue ttl can be set in the confiogurations.(Delete all messages in the queue) ms message ttl need to implement messagePostProcessor to setExpiration in the message properties(When the expired message is on top of the queue, it will be deleted.) Setting queue ttl and message ttl at the same time, it will use the shorter one. DLX dead letter exchange When a message is dead, it can be sent to another exchange. It is called DLX. Dead letter queue summary: There is no difference between dead letter switches and dead letter queues and Xitong. When the message becomes a dead letter, if the queue is bound to a dead letter switch, the message will be rerouted to the dead letter team by the dead letter switch. Messages become dead letter: exceeded limit of the queue consumer refused(basicNack) and requeue = false queue expired/message expired parameters: x-dead-letter-exchange and x-dead-letter-routing-key Dead letter queue use case In the shopping cart case, users place their order and need to pay within 15 min or so. In this case, the message TTL can be set to 15 min(in order.delay.queue). After that, the messages will be sent to dead letter queue(order.release.queue) and the overtime order cancel can be realized. @Bean public Queue orderDelayQueue(){ HashMapargs = new HashMap(); args.put(“x-dead-letter-exchange”,“order-event-exchange”); args.put(“x-dead-letter-routing-key”,“order.release.order”); args.put(“x-message-ttl”,60000); return new Queue(“order.delay.queue”, true, false, false, args); } @Bean public Binding stockReleaseBinding(){ return new Binding(“order.release.order.queue”, Binding.DestinationType.QUEUE, “order-event-exchange”, “order.release.order”, null); } Use plugin to create delayqueue. Install: Use: In the listener, declare delayed = true in th exchange setting. In the publisher, set message property: .setHeader(“x-delay”, 5000) The message will be delayed, but the publisher returns will get errors for not able to send the message directly to the exchange. In the message, the ReceivedDelay property will be the x-delay we set. So one possible solution would be using","date":"2022-08-10","objectID":"/rabbitmq/:0:6","series":null,"tags":null,"title":"RabbbitMQ","uri":"/rabbitmq/#message-stuck"},{"categories":["Tech"],"content":"Reliability publisher confirm: publisher-confirm-type = correlate Enabling publisher confirms calls back a confirm function to identify whether the process succeeds. (Producer to exchange) true: ack false: nack To define callback: rabbitTemplate.setConfirmCallback() confirm as an anonymous class method confirm(CorrelationData, ack, cause) ack: whether the exchange have received the message correlationData: info can be manually set (id) publisher-return: exchange to queue: if failed return “return Callback” publisher-returns: true template: mandatory: true When the message from the exchange did not get to the queue, the default mode is throwing the message away, but we can change the mode of returnCallback notify the publisher of the failure. rabbbitTemplate.returnsCallback() default mode: discard message, not returning the returnedMessage method deals with messages in the exchange but not sent to the queues. Consumer Ack: acknowledge = none As soon as the consumer receives the message, it send the ack message (No service successfulness confirmation) acknowledge = “manual” Manually use channel.basicAck() to confirm and channel.basicNack() to let the publisher resend the message. auto Use republishMessageRecover to bind error message and its routing key. How to guarantee reliability？ Enbale publisher confirms and publisher returns to ensure message is sent to the exchange and queues Enable durability to ensure messages are durable Use consumer ack to ensure the consumer gets messages from queues Set acknowlwdge-mode to auto or simply use republishMessageRecoverer to bind error message queue and exchange. flow control ： use manual ack mode listener-container prefetch = 1000 to control the flow. (The consumer get 1000 messages every time, need manual confirmation to continue) TTL: time to live: before the message got discarded. x-message-tt queue ttl can be set in the confiogurations.(Delete all messages in the queue) ms message ttl need to implement messagePostProcessor to setExpiration in the message properties(When the expired message is on top of the queue, it will be deleted.) Setting queue ttl and message ttl at the same time, it will use the shorter one. DLX dead letter exchange When a message is dead, it can be sent to another exchange. It is called DLX. Dead letter queue summary: There is no difference between dead letter switches and dead letter queues and Xitong. When the message becomes a dead letter, if the queue is bound to a dead letter switch, the message will be rerouted to the dead letter team by the dead letter switch. Messages become dead letter: exceeded limit of the queue consumer refused(basicNack) and requeue = false queue expired/message expired parameters: x-dead-letter-exchange and x-dead-letter-routing-key Dead letter queue use case In the shopping cart case, users place their order and need to pay within 15 min or so. In this case, the message TTL can be set to 15 min(in order.delay.queue). After that, the messages will be sent to dead letter queue(order.release.queue) and the overtime order cancel can be realized. @Bean public Queue orderDelayQueue(){ HashMapargs = new HashMap(); args.put(“x-dead-letter-exchange”,“order-event-exchange”); args.put(“x-dead-letter-routing-key”,“order.release.order”); args.put(“x-message-ttl”,60000); return new Queue(“order.delay.queue”, true, false, false, args); } @Bean public Binding stockReleaseBinding(){ return new Binding(“order.release.order.queue”, Binding.DestinationType.QUEUE, “order-event-exchange”, “order.release.order”, null); } Use plugin to create delayqueue. Install: Use: In the listener, declare delayed = true in th exchange setting. In the publisher, set message property: .setHeader(“x-delay”, 5000) The message will be delayed, but the publisher returns will get errors for not able to send the message directly to the exchange. In the message, the ReceivedDelay property will be the x-delay we set. So one possible solution would be using","date":"2022-08-10","objectID":"/rabbitmq/:0:6","series":null,"tags":null,"title":"RabbbitMQ","uri":"/rabbitmq/#lazy-queues"},{"categories":["Tech"],"content":"MQ cluster classic cluster Nodes in the cluster share some info like exchange queue but not messages in the queues. If one node is offline, the messages are lost. When the consumer are requesting info from one node but the visited node doesn’t have the queue. It will get the queue and its info from the node that has it. Image cluster The image queue structure is one master and multiple slave (slave is an image) all operations are performed on the primary node and then synchronized to the image node. After the master node is down, the image node is replaced by a new master node (if the master node is down before the master-slave synchronization is completed, data loss may occur) the server load balancer function is not available because all operations are completed by the master node (but the master node of different queues can be different, which can be used to improve throughput) higher reliability Quorum Queues Sane as image queues but easier to use.RabbitMQ部署指南.md ","date":"2022-08-10","objectID":"/rabbitmq/:0:7","series":null,"tags":null,"title":"RabbbitMQ","uri":"/rabbitmq/#mq-cluster"},{"categories":["Tech"],"content":"MQ cluster classic cluster Nodes in the cluster share some info like exchange queue but not messages in the queues. If one node is offline, the messages are lost. When the consumer are requesting info from one node but the visited node doesn’t have the queue. It will get the queue and its info from the node that has it. Image cluster The image queue structure is one master and multiple slave (slave is an image) all operations are performed on the primary node and then synchronized to the image node. After the master node is down, the image node is replaced by a new master node (if the master node is down before the master-slave synchronization is completed, data loss may occur) the server load balancer function is not available because all operations are completed by the master node (but the master node of different queues can be different, which can be used to improve throughput) higher reliability Quorum Queues Sane as image queues but easier to use.RabbitMQ部署指南.md ","date":"2022-08-10","objectID":"/rabbitmq/:0:7","series":null,"tags":null,"title":"RabbbitMQ","uri":"/rabbitmq/#classic-cluster"},{"categories":["Tech"],"content":"MQ cluster classic cluster Nodes in the cluster share some info like exchange queue but not messages in the queues. If one node is offline, the messages are lost. When the consumer are requesting info from one node but the visited node doesn’t have the queue. It will get the queue and its info from the node that has it. Image cluster The image queue structure is one master and multiple slave (slave is an image) all operations are performed on the primary node and then synchronized to the image node. After the master node is down, the image node is replaced by a new master node (if the master node is down before the master-slave synchronization is completed, data loss may occur) the server load balancer function is not available because all operations are completed by the master node (but the master node of different queues can be different, which can be used to improve throughput) higher reliability Quorum Queues Sane as image queues but easier to use.RabbitMQ部署指南.md ","date":"2022-08-10","objectID":"/rabbitmq/:0:7","series":null,"tags":null,"title":"RabbbitMQ","uri":"/rabbitmq/#image-cluster"},{"categories":["Tech"],"content":"MQ cluster classic cluster Nodes in the cluster share some info like exchange queue but not messages in the queues. If one node is offline, the messages are lost. When the consumer are requesting info from one node but the visited node doesn’t have the queue. It will get the queue and its info from the node that has it. Image cluster The image queue structure is one master and multiple slave (slave is an image) all operations are performed on the primary node and then synchronized to the image node. After the master node is down, the image node is replaced by a new master node (if the master node is down before the master-slave synchronization is completed, data loss may occur) the server load balancer function is not available because all operations are completed by the master node (but the master node of different queues can be different, which can be used to improve throughput) higher reliability Quorum Queues Sane as image queues but easier to use.RabbitMQ部署指南.md ","date":"2022-08-10","objectID":"/rabbitmq/:0:7","series":null,"tags":null,"title":"RabbbitMQ","uri":"/rabbitmq/#quorum-queues"},{"categories":["Tech"],"content":"Typical problems of RabbitMQ Duplicate consumption of messages To solve this problem, first we need to make sure there’s an unique identifier for each of the messages. Make sure the consumer only executes it once Use redis to store unique identifier, when sending the message, determine whether the identifier already exists in Redis. Boolean flag = stringRedisTemplate.opsForValue().setIfAbsent(“orderNo+couponId”); Check whether the message has already been consumed if (! Boolean.TRUE.equals(flag)) { return; } // Execute business… / / consumer identity stored in Redis, expired stringRedisTemplate for 10 SEC opsForValue (). The set (\" orderNo + couponId “, “1”, Duration, ofSeconds (10 l)); Use the unique key in database We can also use an unique key in the database to ensure its consistency when executed multiple times. ","date":"2022-08-10","objectID":"/rabbitmq/:0:8","series":null,"tags":null,"title":"RabbbitMQ","uri":"/rabbitmq/#typical-problems-of-rabbitmq"},{"categories":["Tech"],"content":"Typical problems of RabbitMQ Duplicate consumption of messages To solve this problem, first we need to make sure there’s an unique identifier for each of the messages. Make sure the consumer only executes it once Use redis to store unique identifier, when sending the message, determine whether the identifier already exists in Redis. Boolean flag = stringRedisTemplate.opsForValue().setIfAbsent(“orderNo+couponId”); Check whether the message has already been consumed if (! Boolean.TRUE.equals(flag)) { return; } // Execute business… / / consumer identity stored in Redis, expired stringRedisTemplate for 10 SEC opsForValue (). The set (\" orderNo + couponId “, “1”, Duration, ofSeconds (10 l)); Use the unique key in database We can also use an unique key in the database to ensure its consistency when executed multiple times. ","date":"2022-08-10","objectID":"/rabbitmq/:0:8","series":null,"tags":null,"title":"RabbbitMQ","uri":"/rabbitmq/#duplicate-consumption-of-messages"},{"categories":["Tech"],"content":"Typical problems of RabbitMQ Duplicate consumption of messages To solve this problem, first we need to make sure there’s an unique identifier for each of the messages. Make sure the consumer only executes it once Use redis to store unique identifier, when sending the message, determine whether the identifier already exists in Redis. Boolean flag = stringRedisTemplate.opsForValue().setIfAbsent(“orderNo+couponId”); Check whether the message has already been consumed if (! Boolean.TRUE.equals(flag)) { return; } // Execute business… / / consumer identity stored in Redis, expired stringRedisTemplate for 10 SEC opsForValue (). The set (\" orderNo + couponId “, “1”, Duration, ofSeconds (10 l)); Use the unique key in database We can also use an unique key in the database to ensure its consistency when executed multiple times. ","date":"2022-08-10","objectID":"/rabbitmq/:0:8","series":null,"tags":null,"title":"RabbbitMQ","uri":"/rabbitmq/#make-sure-the-consumer-only-executes-it-once"},{"categories":["Tech"],"content":"Typical problems of RabbitMQ Duplicate consumption of messages To solve this problem, first we need to make sure there’s an unique identifier for each of the messages. Make sure the consumer only executes it once Use redis to store unique identifier, when sending the message, determine whether the identifier already exists in Redis. Boolean flag = stringRedisTemplate.opsForValue().setIfAbsent(“orderNo+couponId”); Check whether the message has already been consumed if (! Boolean.TRUE.equals(flag)) { return; } // Execute business… / / consumer identity stored in Redis, expired stringRedisTemplate for 10 SEC opsForValue (). The set (\" orderNo + couponId “, “1”, Duration, ofSeconds (10 l)); Use the unique key in database We can also use an unique key in the database to ensure its consistency when executed multiple times. ","date":"2022-08-10","objectID":"/rabbitmq/:0:8","series":null,"tags":null,"title":"RabbbitMQ","uri":"/rabbitmq/#use-the-unique-key-in-database"},{"categories":["Tech"],"content":"Publish faster than consume(Stuck in the queue) Throttling the producer’s message sending interface (not recommended, affecting user experience) deploy more consumer instances (recommended) increase the number of prefetch to allow the consumer to receive more messages at a time (recommended, can be used together with the second solution) ","date":"2022-08-10","objectID":"/rabbitmq/:0:9","series":null,"tags":null,"title":"RabbbitMQ","uri":"/rabbitmq/#publish-faster-than-consumestuck-in-the-queue"},{"categories":["Tech"],"content":"Distributed lock Under highly concurrent environment, the distributed lock is used to ensure consistency in the database. Here’s an example of this. Using simple lock: stringRedisTemplate.opsForValue().setIfAbsent(\"lockKey\", \"ljy\"); // This is the simplest realization of lock in redis // But this lock will trigger deadlock when the service is down // adding delete won't help stringTemplate.delete(\"lockKey\"); // setting expiration time have problem as well stringRedisTemplate.opsForValue().setIfAbsent(\"lockKey\", \"ljy\", 10, TimeUnit.SECONDS); // we don't know when to expire the key (the request may haven't finished but the lock is expired) Let’s say we have a lock expires in 10 seconds, but the requests need 15 seconds to be processed. In that way the second thread will get the lock after 10 seconds and the get deleted after 20 seconds. This may trigger the distributed lock to be ineffect permanently. We can add an if sentence to ","date":"2022-05-18","objectID":"/redis-distributed-lock/:0:0","series":null,"tags":["Redis"],"title":"Redis distributed lock","uri":"/redis-distributed-lock/#distributed-lock"},{"categories":["Tech"],"content":"Redisson Redis Java client which has features of distributed locks. It considers all the problem above and get it done with simple command. Map\u003cString, List\u003cCatalog2Vo\u003e\u003e categoryMap=null; RLock lock = redissonClient.getLock(\"CatalogJson-Lock\"); lock.lock(10,TimeUnit.SECONDS); try { categoryMap = getCategoryMap(); } catch (InterruptedException e) { e.printStackTrace(); }finally { lock.unlock(); return categoryMap; } ","date":"2022-05-18","objectID":"/redis-distributed-lock/:1:0","series":null,"tags":["Redis"],"title":"Redis distributed lock","uri":"/redis-distributed-lock/#redisson"},{"categories":["Leetcode"],"content":"Redis ","date":"2022-05-17","objectID":"/best-time-to-buy-and-sell-stock/:0:0","series":null,"tags":["java"],"title":"Best Time to Buy and Sell Stock","uri":"/best-time-to-buy-and-sell-stock/#redis"},{"categories":["Leetcode"],"content":"Redis with pyrhon Simple example of how to use redis as Cache in python: If we want to store some info in cache(logo, name), we first can convert them into JSON format and assign a key, and store the info in cache. logo = get_logo() #maybe get logo from api json.dumps(logo) #convert to JSON logo_key = f\"{symbol}_logo\" #naming convention When we need to use the info, we can first get it from the cache. redis_client.get(logo_key) If it returns null, that means the key is not in cache. We need to store it first. redis_client.set(logo_key, json.dumps(logo)) The standard procedure would be: logo_key = f\"{symbol}_logo\" logo = redis_client.get(logo_key) if logo is None: logo = get_logo() redis_client.set(logo_key, json.dumps(logo)) else: redis_client.get(logo_key) ","date":"2022-05-17","objectID":"/best-time-to-buy-and-sell-stock/:1:0","series":null,"tags":["java"],"title":"Best Time to Buy and Sell Stock","uri":"/best-time-to-buy-and-sell-stock/#redis-with-pyrhon"},{"categories":["Leetcode"],"content":"Leetcode 93 https://leetcode-cn.com/problems/restore-ip-addresses/ A valid IP address consists of exactly four integers separated by single dots. Each integer is between 0 and 255 (inclusive) and cannot have leading zeros. For example, “0.1.2.201” and “192.168.1.1” are valid IP addresses, but “0.011.255.245”, “192.168.1.312” and “192.168@1.1” are invalid IP addresses. Given a string s containing only digits, return all possible valid IP addresses that can be formed by inserting dots into s. You are not allowed to reorder or remove any digits in s. You may return the valid IP addresses in any order. Example 1: Input: s = “25525511135” Output: [“255.255.11.135”,“255.255.111.35”] Example 2: Input: s = “0000” Output: [“0.0.0.0”] Example 3: Input: s = “101023” Output: [“1.0.10.23”,“1.0.102.3”,“10.1.0.23”,“10.10.2.3”,“101.0.2.3”] Constraints: 1 \u003c= s.length \u003c= 20 s consists of digits only. class Solution { List\u003cString\u003e result = new ArrayList\u003c\u003e(); public List\u003cString\u003e restoreIpAddresses(String s) { if (s.length() \u003e 12) return result; // 算是剪枝了 backTrack(s, 0, 0); return result; } // startIndex: 搜索的起始位置， pointNum:添加逗点的数量 private void backTrack(String s, int startIndex, int pointNum) { if (pointNum == 3) {// 逗点数量为3时，分隔结束 // 判断第四段⼦字符串是否合法，如果合法就放进result中 if (isValid(s,startIndex,s.length()-1)) { result.add(s); } return; } for (int i = startIndex; i \u003c s.length() \u0026\u0026 i \u003c index + 3 ; i++) { if (isValid(s, startIndex, i)) { s = s.substring(0, i + 1) + \".\" + s.substring(i + 1); //在str的后⾯插⼊⼀个逗点 pointNum++; backTrack(s, i + 2, pointNum);// 插⼊逗点之后下⼀个⼦串的起始位置为i+2 pointNum--;// 回溯 s = s.substring(0, i + 1) + s.substring(i + 2);// 回溯删掉逗点 } else { break; } } } // 判断字符串s在左闭⼜闭区间[start, end]所组成的数字是否合法 private Boolean isValid(String s, int start, int end) { if (start \u003e end) { return false; } if (s.charAt(start) == '0' \u0026\u0026 start != end) { // 0开头的数字不合法 return false; } int num = 0; for (int i = start; i \u003c= end; i++) { if (s.charAt(i) \u003e '9' || s.charAt(i) \u003c '0') { // 遇到⾮数字字符不合法 return false; } num = num * 10 + (s.charAt(i) - '0'); if (num \u003e 255) { // 如果⼤于255了不合法 return false; } } return true; } } //Still room for improvement ","date":"2022-05-07","objectID":"/restore-ip-address/:0:0","series":null,"tags":["java"],"title":"Restore IP address","uri":"/restore-ip-address/#"},{"categories":["Leetcode"],"content":"回溯法，一般可以解决如下几种问题： 组合问题：N个数里面按一定规则找出k个数的集合 切割问题：一个字符串按一定规则有几种切割方式 子集问题：一个N个数的集合里有多少符合条件的子集 排列问题：N个数按一定规则全排列，有几种排列方式 棋盘问题：N皇后，解数独等等 模板： void backtracking(参数) { if (终止条件) { 存放结果; return; } for (选择：本层集合中元素（树中节点孩子的数量就是集合的大小）) { 处理节点; backtracking(路径，选择列表); // 递归 回溯，撤销处理结果 } } //for循环横向遍历 //递归纵向遍历 for循环横向遍历，递归纵向遍历，回溯不断调整结果集 ","date":"2022-05-06","objectID":"/%E5%9B%9E%E6%BA%AF%E6%80%BB%E7%BB%93/:0:0","series":null,"tags":["java"],"title":"backtracking summary","uri":"/%E5%9B%9E%E6%BA%AF%E6%80%BB%E7%BB%93/#"},{"categories":["Leetcode"],"content":"组合 组合中使用 startindex 控制遍历顺序 回溯算法：求组合问题！(opens new window)剪枝精髓是：for循环在寻找起点的时候要有一个范围，如果这个起点到集合终止之间的元素已经不够题目要求的k个元素了，就没有必要搜索了。 关键：去重! 在candidates[i] == candidates[i - 1]相同的情况下： used[i - 1] == true，说明同一树枝candidates[i - 1]使用过 used[i - 1] == false，说明同一树层candidates[i - 1]使用过 利用used[i - 1] == false 效率更高。 ","date":"2022-05-06","objectID":"/%E5%9B%9E%E6%BA%AF%E6%80%BB%E7%BB%93/:0:1","series":null,"tags":["java"],"title":"backtracking summary","uri":"/%E5%9B%9E%E6%BA%AF%E6%80%BB%E7%BB%93/#组合"},{"categories":["Leetcode"],"content":"子集问题 注意：子集问题中数组要排序 递增子序列中不能直接按照默认升序进行解答，需要used数组进行去重 回溯算法：求子集问题（二）(opens new window)也可以使用set针对同一父节点本层去重，但子集问题一定要排序，为什么呢？ 我用没有排序的集合{2,1,2,2}来举个例子画一个图，如下： ","date":"2022-05-06","objectID":"/%E5%9B%9E%E6%BA%AF%E6%80%BB%E7%BB%93/:0:2","series":null,"tags":["java"],"title":"backtracking summary","uri":"/%E5%9B%9E%E6%BA%AF%E6%80%BB%E7%BB%93/#子集问题"},{"categories":["Leetcode"],"content":"排列问题 排列问题的不同： 每层都是从0开始搜索而不是startIndex 需要used数组记录path里都放了哪些元素了 去重时使用used数组，效率较高，优于使用set ","date":"2022-05-06","objectID":"/%E5%9B%9E%E6%BA%AF%E6%80%BB%E7%BB%93/:0:3","series":null,"tags":["java"],"title":"backtracking summary","uri":"/%E5%9B%9E%E6%BA%AF%E6%80%BB%E7%BB%93/#排列问题"},{"categories":["Leetcode"],"content":"棋盘问题 N皇后 ","date":"2022-05-06","objectID":"/%E5%9B%9E%E6%BA%AF%E6%80%BB%E7%BB%93/:0:4","series":null,"tags":["java"],"title":"backtracking summary","uri":"/%E5%9B%9E%E6%BA%AF%E6%80%BB%E7%BB%93/#棋盘问题"},{"categories":["Leetcode"],"content":" 棋盘的宽度就是for循环的长度，递归的深度就是棋盘的高度 数独 二维递归： 返回值为bool，在找到正确答案时直接递归返回true。 ","date":"2022-05-06","objectID":"/%E5%9B%9E%E6%BA%AF%E6%80%BB%E7%BB%93/:1:0","series":null,"tags":["java"],"title":"backtracking summary","uri":"/%E5%9B%9E%E6%BA%AF%E6%80%BB%E7%BB%93/#20201118225433127pnghttpscdnnlarkcomyuque02022png266796611651916827858-63619790-956a-4e52-be7b-fe93eddd10e7pngclientidu2146c5ac-cbaf-4crop0crop0crop1crop1fromdropidu2a0659d4margin5bobject20object5dname20201118225433127pngoriginheight1092originwidth1274originaltypebinaryratio1rotation0showtitlefalsesize205235statusdonestylenonetaskidu2e1ef327-8321-4fd0-8283-49529937822title"},{"categories":["Leetcode"],"content":"性能分析 子集问题分析： 时间复杂度：O(2^n)，因为每一个元素的状态无外乎取与不取，所以时间复杂度为O(2^n) 空间复杂度：O(n)，递归深度为n，所以系统栈所用空间为O(n)，每一层递归所用的空间都是常数级别，注意代码里的result和path都是全局变量，就算是放在参数里，传的也是引用，并不会新申请内存空间，最终空间复杂度为O(n) 排列问题分析： 时间复杂度：O(n!)，这个可以从排列的树形图中很明显发现，每一层节点为n，第二层每一个分支都延伸了n-1个分支，再往下又是n-2个分支，所以一直到叶子节点一共就是 n * n-1 * n-2 * ….. 1 = n!。 空间复杂度：O(n)，和子集问题同理。 组合问题分析： 时间复杂度：O(2^n)，组合问题其实就是一种子集的问题，所以组合问题最坏的情况，也不会超过子集问题的时间复杂度。 空间复杂度：O(n)，和子集问题同理。 N皇后问题分析： 时间复杂度：O(n!) ，其实如果看树形图的话，直觉上是O(n^n)，但皇后之间不能见面所以在搜索的过程中是有剪枝的，最差也就是O（n!），n!表示n * (n-1) * …. * 1。 空间复杂度：O(n)，和子集问题同理。 解数独问题分析： 时间复杂度：O(9^m) , m是’.‘的数目。 空间复杂度：O(n^2)，递归的深度是n^2 ","date":"2022-05-06","objectID":"/%E5%9B%9E%E6%BA%AF%E6%80%BB%E7%BB%93/:1:1","series":null,"tags":["java"],"title":"backtracking summary","uri":"/%E5%9B%9E%E6%BA%AF%E6%80%BB%E7%BB%93/#性能分析"},{"categories":["backTracking"],"content":"Leetcode 491 https://leetcode-cn.com/problems/increasing-subsequences/ Given an integer array nums, return all the different possible increasing subsequences of the given array with at least two elements. You may return the answer in any order. The given array may contain duplicates, and two equal integers should also be considered a special case of increasing sequence. Example 1: Input: nums = [4,6,7,7] Output: [[4,6],[4,6,7],[4,6,7,7],[4,7],[4,7,7],[6,7],[6,7,7],[7,7]] Example 2: Input: nums = [4,4,3,2,1] Output: [[4,4]] Constraints: 1 \u003c= nums.length \u003c= 15 -100 \u003c= nums[i] \u003c= 100 class Solution { List\u003cList\u003cInteger\u003e\u003e result = new ArrayList\u003c\u003e(); LinkedList\u003cInteger\u003e list = new LinkedList\u003c\u003e(); public List\u003cList\u003cInteger\u003e\u003e findSubsequences(int[] nums) { backTracking(nums, 0); return result; } public void backTracking(int nums[], int index){ if(list.size() \u003e 1){ result.add(new ArrayList\u003c\u003e(list)); } HashMap\u003cInteger, Integer\u003e map = new HashMap\u003c\u003e(); //使用map进行去重 for(int i = index; i \u003c nums.length; i++){ if(list.size() \u003e 0 \u0026\u0026 nums[i] \u003c list.getLast()){ continue; } //change from nums[i] i-1 to map if(map.getOrDefault(nums[i], 0) \u003e= 1){ continue; } map.put(nums[i], map.getOrDefault(nums[i], 0) + 1); list.add(nums[i]); backTracking(nums, i + 1); list.removeLast(); } } } ","date":"2022-05-06","objectID":"/increasing-subsequences/:0:0","series":null,"tags":["java"],"title":"Increasing Subsequence","uri":"/increasing-subsequences/#"},{"categories":["backTracking"],"content":"Leetcode 17 https://leetcode-cn.com/problems/letter-combinations-of-a-phone-number/ Given a string containing digits from 2-9 inclusive, return all possible letter combinations that the number could represent. Return the answer in any order. A mapping of digit to letters (just like on the telephone buttons) is given below. Note that 1 does not map to any letters. Example 1: Input: digits = “23” Output: [“ad”,“ae”,“af”,“bd”,“be”,“bf”,“cd”,“ce”,“cf”] Example 2: Input: digits = \"\" Output: [] Example 3: Input: digits = “2” Output: [“a”,“b”,“c”] Constraints: 0 \u003c= digits.length \u003c= 4 digits[i] is a digit in the range [‘2’, ‘9’]. class Solution { List\u003cString\u003e result = new LinkedList\u003c\u003e(); HashMap\u003cInteger, String\u003e map = new HashMap\u003c\u003e(); public List\u003cString\u003e letterCombinations(String digits) { if(digits.length() == 0){ return result; } map.put(2, \"abc\"); map.put(3, \"def\"); map.put(4, \"ghi\"); map.put(5, \"jkl\"); map.put(6, \"mno\"); map.put(7, \"pqrs\"); map.put(8, \"tuv\"); map.put(9, \"wxyz\"); letterCombinationHelper(digits, new StringBuilder(), 0); return result; } public void letterCombinationHelper(String digits, StringBuilder sb, int index){ if(sb.toString().length() == digits.length()){ result.add(sb.toString()); return; } if(index \u003e= digits.length()){ return; } int startIndex = digits.charAt(index) - '0'; // for loop(horizontal:a b c) for(int i = 0; i \u003c map.get(startIndex).length(); i++){ sb.append(map.get(startIndex).charAt(i)); letterCombinationHelper(digits, sb, index + 1); // vertical 2 3 sb.deleteCharAt(sb.length() - 1); } } } ","date":"2022-05-06","objectID":"/letter-combinations-of-a-phone-number/:0:0","series":null,"tags":["backTracking"],"title":"Letter combination of a phone number","uri":"/letter-combinations-of-a-phone-number/#"},{"categories":["Leetcode"],"content":"Leetcode 47 https://leetcode-cn.com/problems/permutations-ii/ Given a collection of numbers, nums, that might contain duplicates, return all possible unique permutations in any order. Example 1: Input: nums = [1,1,2] Output: [[1,1,2], [1,2,1], [2,1,1]] Example 2: Input: nums = [1,2,3] Output: [[1,2,3],[1,3,2],[2,1,3],[2,3,1],[3,1,2],[3,2,1]] Constraints: 1 \u003c= nums.length \u003c= 8 -10 \u003c= nums[i] \u003c= 10 class Solution { List\u003cList\u003cInteger\u003e\u003e result = new ArrayList\u003c\u003e(); LinkedList\u003cInteger\u003e list = new LinkedList\u003c\u003e(); public List\u003cList\u003cInteger\u003e\u003e permuteUnique(int[] nums) { Arrays.sort(nums); int [] used = new int[nums.length]; backTracking(nums, used); return result; } public void backTracking(int nums[], int[] used){ if(list.size() == nums.length){ result.add(new ArrayList\u003c\u003e(list)); return; } for(int i = 0; i \u003c nums.length; i++){ if(i \u003e 0 \u0026\u0026 nums[i] == nums[i - 1] \u0026\u0026 used[i - 1] == 0){ // cut continue; } if(used[i] == 0){ list.add(nums[i]); used[i] = 1; backTracking(nums, used); list.removeLast(); used[i] = 0; } } } } 假设当前到达了元素 b ，那么前一个元素 a 一定是已经处理过了的，它的 used[a]==true 理应成立。但如果 used[a]==false 说明什么？？说明 nums[a] 已经被从当前组合集合中撤销掉了！！ 既然现在 nums[a] 被撤销，而 nums[b] 被选择，也就是说：现在 nums[b] 被顶替在了之前 nums[a] 的位置上。如果 nums[a]==nums[b] ，那么当前组合结果和之前不撤销nums[a]的那个组合有什么区别？是没有区别的，这样就导致两个组合是重复的！这也就是我们需要剪枝去重的情况！ ","date":"2022-05-06","objectID":"/permutations-ii/:0:0","series":null,"tags":["java"],"title":"Permutations II","uri":"/permutations-ii/#"},{"categories":["Leetcode"],"content":"Leetcode 98 https://leetcode-cn.com/problems/validate-binary-search-tree/ Given the root of a binary tree, determine if it is a valid binary search tree (BST). A valid BST is defined as follows: The left subtree of a node contains only nodes with keys less than the node’s key. The right subtree of a node contains only nodes with keys greater than the node’s key. Both the left and right subtrees must also be binary search trees. Example 1: Input: root = [2,1,3] Output: true Example 2: Input: root = [5,1,4,null,null,3,6] Output: false Explanation: The root node’s value is 5 but its right child’s value is 4. Constraints: The number of nodes in the tree is in the range [1, 104]. -231 \u003c= Node.val \u003c= 231 - 1 class Solution { // 递归 TreeNode max; public boolean isValidBST(TreeNode root) { if (root == null) { return true; } // 左 boolean left = isValidBST(root.left); if (!left) { return false; } // 中 if (max != null \u0026\u0026 root.val \u003c= max.val) { return false; } max = root; // 右 boolean right = isValidBST(root.right); return right; } } class Solution { // 迭代 public boolean isValidBST(TreeNode root) { if (root == null) { return true; } Stack\u003cTreeNode\u003e stack = new Stack\u003c\u003e(); TreeNode pre = null; while (root != null || !stack.isEmpty()) { while (root != null) { stack.push(root); root = root.left;// 左 } // 中，处理 TreeNode pop = stack.pop(); if (pre != null \u0026\u0026 pop.val \u003c= pre.val) { return false; } pre = pop; root = pop.right;// 右 } return true; } } ","date":"2022-05-03","objectID":"/validate-binary-search-tree/:0:0","series":null,"tags":["java"],"title":"Validate Binary Search Tree","uri":"/validate-binary-search-tree/#"},{"categories":["Leetcode"],"content":"Leetcode 106 105 https://leetcode-cn.com/problems/construct-binary-tree-from-inorder-and-postorder-traversal/ Example 1: Input: inorder = [9,3,15,20,7], postorder = [9,15,7,20,3] Output: [3,9,20,null,null,15,7] Example 2: Input: inorder = [-1], postorder = [-1] Output: [-1] //Recursion 106 class Solution { public TreeNode buildTree(int[] inorder, int[] postorder) { return Traverse(inorder, 0, inorder.length, postorder, 0, postorder.length); } public TreeNode Traverse(int[] inorder, int inLeft, int inRight, int[] postorder, int postLeft, int postRight){ if(inRight - inLeft \u003c 1){ return null; } if(inRight - inLeft == 1){ return new TreeNode(inorder[inLeft]); } //postorder last node as root int rootVal = postorder[postRight - 1]; TreeNode root = new TreeNode(rootVal); int rootIndex = 0; for(int i = inLeft; i \u003c inRight; i++){ if(inorder[i] == rootVal){ rootIndex = i; break; } } root.left = Traverse(inorder, inLeft, rootIndex, postorder, postLeft, postLeft + rootIndex - inLeft); root.right = Traverse(inorder, rootIndex + 1, inRight, postorder, postLeft + rootIndex - inLeft, postRight - 1); return root; } } //105 class Solution { public TreeNode buildTree(int[] preorder, int[] inorder) { return helper(preorder, 0, preorder.length - 1, inorder, 0, inorder.length - 1); } public TreeNode helper(int[] preorder, int preLeft, int preRight, int[] inorder, int inLeft, int inRight) { // 递归终止条件 if (inLeft \u003e inRight || preLeft \u003e preRight) return null; // val 为前序遍历第一个的值，也即是根节点的值 // idx 为根据根节点的值来找中序遍历的下标 int idx = inLeft, val = preorder[preLeft]; TreeNode root = new TreeNode(val); for (int i = inLeft; i \u003c= inRight; i++) { if (inorder[i] == val) { idx = i; break; } } // 根据 idx 来递归找左右子树 root.left = helper(preorder, preLeft + 1, preLeft + (idx - inLeft), inorder, inLeft, idx - 1); root.right = helper(preorder, preLeft + (idx - inLeft) + 1, preRight, inorder, idx + 1, inRight); return root; } } ","date":"2022-05-02","objectID":"/construct-tree/:0:0","series":null,"tags":["java"],"title":"Construct Tree","uri":"/construct-tree/#"},{"categories":["Leetcode"],"content":"https://leetcode-cn.com/problems/invert-binary-tree/ Given the root of a binary tree, invert the tree, and return its root. Example 1: Input: root = [4,2,7,1,3,6,9] Output: [4,7,2,9,6,3,1] Example 2: Input: root = [2,1,3] Output: [2,3,1] Example 3: Input: root = [] Output: [] Constraints: The number of nodes in the tree is in the range [0, 100]. -100 \u003c= Node.val \u003c= 100 //Iteration /** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode() {} * TreeNode(int val) { this.val = val; } * TreeNode(int val, TreeNode left, TreeNode right) { * this.val = val; * this.left = left; * this.right = right; * } * } */ class Solution { public TreeNode invertTree(TreeNode root) { Deque\u003cTreeNode\u003e stack = new LinkedList\u003c\u003e(); if(root != null){ stack.offerLast(root); } TreeNode node = root; while(!stack.isEmpty() ){ node = stack.pollLast(); TreeNode temp = node.left; node.left = node.right; node.right = temp; if(node.left != null){ stack.offerLast(node.left); } if(node.right != null){ stack.offerLast(node.right); } } return root; } } //Recursion class Solution { public TreeNode invertTree(TreeNode root) { InvertNode(root); return root; } public void InvertNode(TreeNode root){ if(root == null){ return; } TreeNode temp = root.left; root.left = root.right; root.right = temp; InvertNode(root.left); InvertNode(root.right); } } ","date":"2022-05-02","objectID":"/invert-binary-tree/:0:0","series":null,"tags":["java"],"title":"Invert Binary Tree","uri":"/invert-binary-tree/#"},{"categories":null,"content":" Second-hand Trading system A simple but comprehensive trading system. Read more... Distributed Backend cart system Implementation of cart system under distributed environment. Read more... Personal Blog System Discover my personal blogs. Read more... ","date":"2022-05-02","objectID":"/projects/:0:0","series":null,"tags":null,"title":"Projects","uri":"/projects/#"},{"categories":["Leetcode"],"content":"Leetcode 102 https://leetcode-cn.com/problems/binary-tree-level-order-traversal/ Given the root of a binary tree, return the level order traversal of its nodes’ values. (i.e., from left to right, level by level). Example 1: Input: root = [3,9,20,null,null,15,7] Output: [[3],[9,20],[15,7]] Example 2: Input: root = [1] Output: [[1]] Example 3: Input: root = [] Output: [] Constraints: The number of nodes in the tree is in the range [0, 2000]. -1000 \u003c= Node.val \u003c= 1000 //DFS method /** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode() {} * TreeNode(int val) { this.val = val; } * TreeNode(int val, TreeNode left, TreeNode right) { * this.val = val; * this.left = left; * this.right = right; * } * } */ class Solution { List\u003cList\u003cInteger\u003e\u003e result = new LinkedList\u003c\u003e(); public List\u003cList\u003cInteger\u003e\u003e levelOrder(TreeNode root) { int depth = 0; DFS(root,0); return result; } public void DFS(TreeNode node, int depth){ if(node == null){ return; } depth++; if(depth \u003e result.size()){ List\u003cInteger\u003e layer = new LinkedList\u003c\u003e(); result.add(layer); } result.get(depth - 1).add(node.val); DFS(node.left, depth); DFS(node.right, depth); } } // Queue class Solution { public List\u003cList\u003cInteger\u003e\u003e levelOrder(TreeNode root) { Deque\u003cTreeNode\u003e queue = new LinkedList\u003c\u003e(); if(root != null){ queue.add(root); } List\u003cList\u003cInteger\u003e\u003e result = new LinkedList\u003c\u003e(); while(queue.size() != 0){ int size = queue.size(); List\u003cInteger\u003e layers = new LinkedList\u003c\u003e(); for(int i = 0; i \u003c size; i++){ TreeNode node = queue.pollFirst(); layers.add(node.val); if(node.left != null){ queue.offerLast(node.left); } if(node.right != null){ queue.offerLast(node.right); } } result.add(layers); } return result; } } Similar problems: Leetcode 107 class Solution { public List\u003cList\u003cInteger\u003e\u003e levelOrderBottom(TreeNode root) { LinkedList\u003cList\u003cInteger\u003e\u003e result = new LinkedList\u003c\u003e(); Deque\u003cTreeNode\u003e queue = new LinkedList\u003c\u003e(); if(root != null){ queue.offerLast(root); } while(!queue.isEmpty()){ int size = queue.size(); List\u003cInteger\u003e list = new LinkedList\u003c\u003e(); for(int i =0; i \u003c size; i++){ TreeNode node = queue.pollFirst(); if(node.left != null){ queue.offerLast(node.left); } if(node.right != null){ queue.offerLast(node.right); } list.add(node.val); } result.addFirst(list); } return result; } } //DFS class Solution { LinkedList\u003cList\u003cInteger\u003e\u003e result = new LinkedList\u003c\u003e(); public List\u003cList\u003cInteger\u003e\u003e levelOrderBottom(TreeNode root) { level(root,0); Collections.reverse(result); return result; } public void level(TreeNode node, int depth){ if(node == null){ return; } depth++; if(result.size() \u003c depth){ List\u003cInteger\u003e list = new LinkedList\u003c\u003e(); result.add(list); } result.get(depth-1).add(node.val); level(node.left, depth); level(node.right, depth); } } Leetcode 199 //DFS class Solution { List\u003cInteger\u003e result = new LinkedList\u003c\u003e(); public List\u003cInteger\u003e rightSideView(TreeNode root) { level(root,0); return result; } public void level(TreeNode node, int depth){ if(node == null){ return; } depth++; if(result.size() \u003c depth){ result.add(0); } result.set(depth-1, node.val); level(node.left, depth); level(node.right, depth); } } //BFS class Solution { public List\u003cInteger\u003e rightSideView(TreeNode root) { Deque\u003cTreeNode\u003e queue = new LinkedList\u003c\u003e(); List\u003cInteger\u003e list = new LinkedList\u003c\u003e(); if(root != null){ queue.offerLast(root); } while(!queue.isEmpty()){ int size = queue.size(); for(int i =0; i \u003c size; i++){ TreeNode node = queue.pollFirst(); if(node.left != null){ queue.offerLast(node.left); } if(node.right != null){ queue.offerLast(node.right); } if(i == size - 1){ list.add(node.val); } } } return list; } } ","date":"2022-05-01","objectID":"/bfs/:0:0","series":null,"tags":["java"],"title":"BFS","uri":"/bfs/#"},{"categories":["Leetcode"],"content":"leetcode 100 https://leetcode-cn.com/problems/same-tree/ //DFS /** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode() {} * TreeNode(int val) { this.val = val; } * TreeNode(int val, TreeNode left, TreeNode right) { * this.val = val; * this.left = left; * this.right = right; * } * } */ class Solution { public boolean isSameTree(TreeNode p, TreeNode q) { return compare(p, q); } public boolean compare(TreeNode p, TreeNode q){ if(p == null \u0026\u0026 q != null){ return false; } else if(p != null \u0026\u0026 q == null){ return false; } else if(p == null \u0026\u0026 q == null){ return true; } else if(p.val != q.val){ return false; } boolean left = compare(p.left, q.left); boolean right = compare(p.right, q.right); return left \u0026 right; } } ","date":"2022-05-01","objectID":"/compare-tree/:0:0","series":null,"tags":["java"],"title":"Compare Tree","uri":"/compare-tree/#"},{"categories":["SpringBoot"],"content":"SpringBoot ","date":"2021-12-16","objectID":"/springboot/:0:0","series":null,"tags":["java"],"title":"SpringBoot","uri":"/springboot/#springboot"},{"categories":["SpringBoot"],"content":"@Configuration ","date":"2021-12-16","objectID":"/springboot/:1:0","series":null,"tags":["java"],"title":"SpringBoot","uri":"/springboot/#configuration"},{"categories":["SpringBoot"],"content":"组件添加 源码重点： postProcessBeanDefinitionRegistry 每个registryId唯一 调用processConfigBeanDefinitions将registry中Bean进行加载 processConfigBeanDefinitions中 ConfigurationClassUtils判断full或者lite 有@configuration则为完整的配置类 对于代理Bean的方法中 分为Full和Lite Full: @configuration(procyBeanMethods=true) 保证每个@Bean方法被调用返回的组件是单实例的(默认方式) Lite: @configuration(procyBeanMethods=false) 每个@Bean方法调用的组件都是新创建的 Lite模式难以声明Bean之间的依赖 重要： 如果存在不同@Bean之间的组件依赖 必须使用Full模式 其他使用Lite模式启动更加迅速 总结： @Configuration的注解类标识这个类可以使用Spring IoC容器作为bean定义的来源。 @Bean注解告诉Spring，一个带有@Bean的注解方法将返回一个对象，该对象应该被注册为在Spring应用程序上下文中的bean。 @ComponentScan的功能其实就是自动扫描并加载符合条件的组件（比如@Component和@Repository等）或者bean定义，最终将这些bean定义加载到IoC容器中。 @EnableAutoConfiguration会根据类路径中的jar依赖为项目进行自动配置，如：添加了spring-boot-starter-web依赖，会自动添加Tomcat和Spring MVC的依赖，Spring Boot会对Tomcat和Spring MVC进行自动配置。 @ImportResource(classpath:) 若存在较老的组件或使用xml配置的组件，使用此方式引入依赖 ","date":"2021-12-16","objectID":"/springboot/:1:1","series":null,"tags":["java"],"title":"SpringBoot","uri":"/springboot/#组件添加"},{"categories":["SpringBoot"],"content":"组件添加 源码重点： postProcessBeanDefinitionRegistry 每个registryId唯一 调用processConfigBeanDefinitions将registry中Bean进行加载 processConfigBeanDefinitions中 ConfigurationClassUtils判断full或者lite 有@configuration则为完整的配置类 对于代理Bean的方法中 分为Full和Lite Full: @configuration(procyBeanMethods=true) 保证每个@Bean方法被调用返回的组件是单实例的(默认方式) Lite: @configuration(procyBeanMethods=false) 每个@Bean方法调用的组件都是新创建的 Lite模式难以声明Bean之间的依赖 重要： 如果存在不同@Bean之间的组件依赖 必须使用Full模式 其他使用Lite模式启动更加迅速 总结： @Configuration的注解类标识这个类可以使用Spring IoC容器作为bean定义的来源。 @Bean注解告诉Spring，一个带有@Bean的注解方法将返回一个对象，该对象应该被注册为在Spring应用程序上下文中的bean。 @ComponentScan的功能其实就是自动扫描并加载符合条件的组件（比如@Component和@Repository等）或者bean定义，最终将这些bean定义加载到IoC容器中。 @EnableAutoConfiguration会根据类路径中的jar依赖为项目进行自动配置，如：添加了spring-boot-starter-web依赖，会自动添加Tomcat和Spring MVC的依赖，Spring Boot会对Tomcat和Spring MVC进行自动配置。 @ImportResource(classpath:) 若存在较老的组件或使用xml配置的组件，使用此方式引入依赖 ","date":"2021-12-16","objectID":"/springboot/:1:1","series":null,"tags":["java"],"title":"SpringBoot","uri":"/springboot/#重要"},{"categories":["SpringBoot"],"content":"组件添加 源码重点： postProcessBeanDefinitionRegistry 每个registryId唯一 调用processConfigBeanDefinitions将registry中Bean进行加载 processConfigBeanDefinitions中 ConfigurationClassUtils判断full或者lite 有@configuration则为完整的配置类 对于代理Bean的方法中 分为Full和Lite Full: @configuration(procyBeanMethods=true) 保证每个@Bean方法被调用返回的组件是单实例的(默认方式) Lite: @configuration(procyBeanMethods=false) 每个@Bean方法调用的组件都是新创建的 Lite模式难以声明Bean之间的依赖 重要： 如果存在不同@Bean之间的组件依赖 必须使用Full模式 其他使用Lite模式启动更加迅速 总结： @Configuration的注解类标识这个类可以使用Spring IoC容器作为bean定义的来源。 @Bean注解告诉Spring，一个带有@Bean的注解方法将返回一个对象，该对象应该被注册为在Spring应用程序上下文中的bean。 @ComponentScan的功能其实就是自动扫描并加载符合条件的组件（比如@Component和@Repository等）或者bean定义，最终将这些bean定义加载到IoC容器中。 @EnableAutoConfiguration会根据类路径中的jar依赖为项目进行自动配置，如：添加了spring-boot-starter-web依赖，会自动添加Tomcat和Spring MVC的依赖，Spring Boot会对Tomcat和Spring MVC进行自动配置。 @ImportResource(classpath:) 若存在较老的组件或使用xml配置的组件，使用此方式引入依赖 ","date":"2021-12-16","objectID":"/springboot/:1:1","series":null,"tags":["java"],"title":"SpringBoot","uri":"/springboot/#总结"},{"categories":["SpringBoot"],"content":"组件添加 源码重点： postProcessBeanDefinitionRegistry 每个registryId唯一 调用processConfigBeanDefinitions将registry中Bean进行加载 processConfigBeanDefinitions中 ConfigurationClassUtils判断full或者lite 有@configuration则为完整的配置类 对于代理Bean的方法中 分为Full和Lite Full: @configuration(procyBeanMethods=true) 保证每个@Bean方法被调用返回的组件是单实例的(默认方式) Lite: @configuration(procyBeanMethods=false) 每个@Bean方法调用的组件都是新创建的 Lite模式难以声明Bean之间的依赖 重要： 如果存在不同@Bean之间的组件依赖 必须使用Full模式 其他使用Lite模式启动更加迅速 总结： @Configuration的注解类标识这个类可以使用Spring IoC容器作为bean定义的来源。 @Bean注解告诉Spring，一个带有@Bean的注解方法将返回一个对象，该对象应该被注册为在Spring应用程序上下文中的bean。 @ComponentScan的功能其实就是自动扫描并加载符合条件的组件（比如@Component和@Repository等）或者bean定义，最终将这些bean定义加载到IoC容器中。 @EnableAutoConfiguration会根据类路径中的jar依赖为项目进行自动配置，如：添加了spring-boot-starter-web依赖，会自动添加Tomcat和Spring MVC的依赖，Spring Boot会对Tomcat和Spring MVC进行自动配置。 @ImportResource(classpath:) 若存在较老的组件或使用xml配置的组件，使用此方式引入依赖 ","date":"2021-12-16","objectID":"/springboot/:1:1","series":null,"tags":["java"],"title":"SpringBoot","uri":"/springboot/#importresourceclasspath"},{"categories":["SpringBoot"],"content":"配置绑定 当需要将properties文件中的配置（例：数据库）解析至Bean中 使用以下注解 @Component+@ConfigurationProperties(prefix=\"\") 只有在容器中的组件才有需要用的功能 记得利用component注解将其加入容器中 @EnableConfigurationProperties(xx.class) 开启xx的配置绑定功能 并且将xx组件注册到容器中 重点： @EnableAutoConfiguration @AutoConfigurationPackage 利用register 将一个包中的所有组件注册（默认MainApplication所在的包） Import(AutoConfigurationEntry) 每次springboot启动时会导入所有场景的自动配置文件 xxxAutoConfiguration 基础配置文件总共127个 但并不是每次导入的配置都会生效，源码内部配合@Conditional注解进行限制加载（按需加载） ","date":"2021-12-16","objectID":"/springboot/:2:0","series":null,"tags":["java"],"title":"SpringBoot","uri":"/springboot/#配置绑定"},{"categories":["SpringBoot"],"content":"配置绑定 当需要将properties文件中的配置（例：数据库）解析至Bean中 使用以下注解 @Component+@ConfigurationProperties(prefix=\"\") 只有在容器中的组件才有需要用的功能 记得利用component注解将其加入容器中 @EnableConfigurationProperties(xx.class) 开启xx的配置绑定功能 并且将xx组件注册到容器中 重点： @EnableAutoConfiguration @AutoConfigurationPackage 利用register 将一个包中的所有组件注册（默认MainApplication所在的包） Import(AutoConfigurationEntry) 每次springboot启动时会导入所有场景的自动配置文件 xxxAutoConfiguration 基础配置文件总共127个 但并不是每次导入的配置都会生效，源码内部配合@Conditional注解进行限制加载（按需加载） ","date":"2021-12-16","objectID":"/springboot/:2:0","series":null,"tags":["java"],"title":"SpringBoot","uri":"/springboot/#enableautoconfiguration"},{"categories":["SpringBoot"],"content":"配置绑定 当需要将properties文件中的配置（例：数据库）解析至Bean中 使用以下注解 @Component+@ConfigurationProperties(prefix=\"\") 只有在容器中的组件才有需要用的功能 记得利用component注解将其加入容器中 @EnableConfigurationProperties(xx.class) 开启xx的配置绑定功能 并且将xx组件注册到容器中 重点： @EnableAutoConfiguration @AutoConfigurationPackage 利用register 将一个包中的所有组件注册（默认MainApplication所在的包） Import(AutoConfigurationEntry) 每次springboot启动时会导入所有场景的自动配置文件 xxxAutoConfiguration 基础配置文件总共127个 但并不是每次导入的配置都会生效，源码内部配合@Conditional注解进行限制加载（按需加载） ","date":"2021-12-16","objectID":"/springboot/:2:0","series":null,"tags":["java"],"title":"SpringBoot","uri":"/springboot/#autoconfigurationpackage"},{"categories":["SpringBoot"],"content":"配置绑定 当需要将properties文件中的配置（例：数据库）解析至Bean中 使用以下注解 @Component+@ConfigurationProperties(prefix=\"\") 只有在容器中的组件才有需要用的功能 记得利用component注解将其加入容器中 @EnableConfigurationProperties(xx.class) 开启xx的配置绑定功能 并且将xx组件注册到容器中 重点： @EnableAutoConfiguration @AutoConfigurationPackage 利用register 将一个包中的所有组件注册（默认MainApplication所在的包） Import(AutoConfigurationEntry) 每次springboot启动时会导入所有场景的自动配置文件 xxxAutoConfiguration 基础配置文件总共127个 但并不是每次导入的配置都会生效，源码内部配合@Conditional注解进行限制加载（按需加载） ","date":"2021-12-16","objectID":"/springboot/:2:0","series":null,"tags":["java"],"title":"SpringBoot","uri":"/springboot/#importautoconfigurationentry"},{"categories":["SpringBoot"],"content":"默认配置 springboot首先加载所有自动配置类 自动配置类按照条件生效 默认配置类都绑定了文件的值，xxxProperties里面拿 和配置文件进行了绑定 生效的配置类给容器装配置组件 因而实现功能 xxxAuto Configuration –\u003e 组件 –\u003e xxxProperties中拿值 –\u003e applicatio.properties 在默认配置中，使用了大量@conditional 以用户配置优先，用户没有进行配置则使用springboot装配的组件 用户想要定制化进行配置的方法主要有两种：1用户直接自己@Bean替换底层组件 2用户去看这个组件货去的配置文件值并且进行修改 ","date":"2021-12-16","objectID":"/springboot/:3:0","series":null,"tags":["java"],"title":"SpringBoot","uri":"/springboot/#默认配置"},{"categories":["SpringBoot"],"content":"Web开发 静态资源的访问首先找Controller内部能否处理，如果不能则交给静态资源处理器（在静态资源文件夹下寻找） ","date":"2021-12-16","objectID":"/springboot/:4:0","series":null,"tags":["java"],"title":"SpringBoot","uri":"/springboot/#web开发"},{"categories":["Tech"],"content":"Using spring cloud gateway can quickly build an API gateway, but before that, let’s introduce some special concepts involved in using spring cloud gateway framework, so as to deepen the understanding of spring cloud gateway and facilitate the later use. Routing: it is a basic component in spring cloud gateway. It is usually composed of an ID, a target URI, and a series of predicates and filters. Predicate: it is the predicate object of Java 8 function library. The specific type isPredicateIt is used to match data information on HTTP request, such as request header information and request body information. If the assertion for a request is true, then the route it is associated with is successful, and then the request is processed by the route. Filter: a component used to modify the request or response of a route. In spring cloud gateway, the gatewayfilter interface should be implemented, and it should be constructed by a specific implementation class based on gatewayfilterfactory. The filters of spring cloud gateway are executed in an orderly manner, and the execution order is determined by the size of the order value. The smaller the value is, the higher the priority is, and the earlier the execution is. Order number of Gateway filter and default filter are default numbers starting from 1. When numbers of different gateway are the same, the order will be Defaultfilter-\u003egateway filter-\u003eglobal filter ","date":"2021-04-06","objectID":"/spring-gateway/:0:0","series":null,"tags":["Spring Cloud, java"],"title":"Spring Cloud Gateway","uri":"/spring-gateway/#"},{"categories":["Tech"],"content":"Docker and MySQL 开源应用容器引擎： 将软件编译成镜像，做好配置后发布，保证启动快速。 启动快 占用资源少 体积小 docker主机(Host)：安装了Docker程序的机器（Docker直接安装在操作系统之上）； docker客户端(Client)：连接docker主机进行操作； docker仓库(Registry)：用来保存各种打包好的软件镜像； docker镜像(Images)：软件打包好的镜像；放在docker仓库中； docker容器(Container)：镜像启动后的实例称为一个容器；容器是独立运行的一个或一组应用 ","date":"2021-02-06","objectID":"/docker/:0:0","series":null,"tags":["Docker"],"title":"Docker","uri":"/docker/#docker-and-mysql"},{"categories":["Tech"],"content":"命令总结： docker pull 下载 `Docker run =docker create+start 创建容器后立即运行 例：docker run -p 3306:3306 --name mysql01 -e MYSQL_ROOT_PASSWORD=mysql -d mysql` -d：后台运行 -p: 将主机的端口映射到容器的一个端口 主机端口:容器内部的端口 docker ps 查看现有所有容器 docker rm containerID ","date":"2021-02-06","objectID":"/docker/:1:0","series":null,"tags":["Docker"],"title":"Docker","uri":"/docker/#命令总结"},{"categories":["Tech"],"content":"docker connect to MySQL docker exec -it mysql01 bash #进入mysql命令行 mysql01为container名字 mysql -h localhost -u root -p mysql#连接 ","date":"2021-02-06","objectID":"/docker/:2:0","series":null,"tags":["Docker"],"title":"Docker","uri":"/docker/#docker-connect-to-mysql"},{"categories":["Tech"],"content":"MySQL常用命令总结： showdatabses;#显示数据库列表usexxx;#切换至xxx数据库showtables;#显示数据库表describetablename;#显示数据表结构createdatabasesdatabasename;createtabletablename(字段列表);#例：createtablemyTable(idint(5)notnullprimarykeyauto_increment,namechar(20)notnull,sexint(4)notnulldefault'0');dropdatabasedatabasename;droptabletablename;select*fromtablename;select*fromtablenamelimit0,2;#仅查询前几行insertintotablenamevalues();renametabletablenametonewtablename;#表名更改updatetablenamesetstrname=newcoontent;#更新updatemysql.usersetpassword=PASSWORD('xxxxx')whereuser='root';flushprivileges;#修改root密码selectuser();#显示当前用户grantselect,insert...ondatabasename.(*ortablename)newuusername@localhostidentifiedby\"password\";#新增用户并限制其登登录权限和地址deletefromuserwhereuser='username'andhost='localhost';flushprivileges;#删除用户 ","date":"2021-02-06","objectID":"/docker/:0:0","series":null,"tags":["Docker"],"title":"Docker","uri":"/docker/#mysql常用命令总结"},{"categories":null,"content":"Hi this is the first post of mine. From now on, my personal working and studying process will be posted here. ","date":"2021-02-01","objectID":"/first_post/:0:0","series":null,"tags":null,"title":"First_post","uri":"/first_post/#"}]