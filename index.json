[{"categories":["Tech"],"content":"A powerful search engine for large scale data search. Also benefit for log analysis and monitoring. Elasticsearch: store, compute, search data Inverted Index inverted index (also referred to as a postings list, postings file, or inverted file) is a database index storing a mapping from content, such as words or numbers, to its locations in a table, or in a document or a set of documents (named in contrast to a forward index, which maps from documents to content). The purpose of an inverted index is to allow fast full-text searches, at a cost of increased processing when a document is added to the database. “it is what it is” “what is it” “it is a banana” We can get the following reverse file index: “a”: {2} “banana”: {2} “is”: {0, 1, 2} “it”: {0, 1, 2} “what”: {0, 1} Search criteria “what” , “is” and “it” will correspond to this collection :. For the same text, we get the following completely reverse indexes, document the number and the word result of the current query. Data . Similarly, both the number of documents and the word results Currently queried start from zero. So, “banana”: {(2, 3)} that is to say, “banana” is in the third document (), and the position in the third document is the fourth word (address is 3). “a”: {(2, 2)} “banana”: {(2, 3)} “is”: {(0, 1), (0, 4), (1, 1), (2, 1)} “it”: {(0, 0), (0, 3), (1, 2), (2, 0)} “what”: {(0, 2), (1, 0)} Mysql is suitable for writing and transactions Elasticsearch is suitable for largescale data search analysis and computing ","date":"2022-07-06","objectID":"/elasticsearch/:0:0","series":null,"tags":["Elasticsearch, java"],"title":"Elasticsearch","uri":"/elasticsearch/#"},{"categories":["Tech"],"content":"A powerful search engine for large scale data search. Also benefit for log analysis and monitoring. Elasticsearch: store, compute, search data Inverted Index inverted index (also referred to as a postings list, postings file, or inverted file) is a database index storing a mapping from content, such as words or numbers, to its locations in a table, or in a document or a set of documents (named in contrast to a forward index, which maps from documents to content). The purpose of an inverted index is to allow fast full-text searches, at a cost of increased processing when a document is added to the database. “it is what it is” “what is it” “it is a banana” We can get the following reverse file index: “a”: {2} “banana”: {2} “is”: {0, 1, 2} “it”: {0, 1, 2} “what”: {0, 1} Search criteria “what” , “is” and “it” will correspond to this collection :. For the same text, we get the following completely reverse indexes, document the number and the word result of the current query. Data . Similarly, both the number of documents and the word results Currently queried start from zero. So, “banana”: {(2, 3)} that is to say, “banana” is in the third document (), and the position in the third document is the fourth word (address is 3). “a”: {(2, 2)} “banana”: {(2, 3)} “is”: {(0, 1), (0, 4), (1, 1), (2, 1)} “it”: {(0, 0), (0, 3), (1, 2), (2, 0)} “what”: {(0, 2), (1, 0)} Mysql is suitable for writing and transactions Elasticsearch is suitable for largescale data search analysis and computing ","date":"2022-07-06","objectID":"/elasticsearch/:0:0","series":null,"tags":["Elasticsearch, java"],"title":"Elasticsearch","uri":"/elasticsearch/#inverted-index"},{"categories":["Tech"],"content":"Index type: text(words that can be seperated), keyword(Should not be seperated: country names) Numbers: long, integer, short, byte, double, float boolean date object index: Specifies whether to create an index. The default value is true. analyzer: Specifies which analyzer to use. properties: To specify sub strings ","date":"2022-07-06","objectID":"/elasticsearch/:0:1","series":null,"tags":["Elasticsearch, java"],"title":"Elasticsearch","uri":"/elasticsearch/#index"},{"categories":["Tech"],"content":"Index type: text(words that can be seperated), keyword(Should not be seperated: country names) Numbers: long, integer, short, byte, double, float boolean date object index: Specifies whether to create an index. The default value is true. analyzer: Specifies which analyzer to use. properties: To specify sub strings ","date":"2022-07-06","objectID":"/elasticsearch/:0:1","series":null,"tags":["Elasticsearch, java"],"title":"Elasticsearch","uri":"/elasticsearch/#type"},{"categories":["Tech"],"content":"Index type: text(words that can be seperated), keyword(Should not be seperated: country names) Numbers: long, integer, short, byte, double, float boolean date object index: Specifies whether to create an index. The default value is true. analyzer: Specifies which analyzer to use. properties: To specify sub strings ","date":"2022-07-06","objectID":"/elasticsearch/:0:1","series":null,"tags":["Elasticsearch, java"],"title":"Elasticsearch","uri":"/elasticsearch/#index-1"},{"categories":["Tech"],"content":"Index type: text(words that can be seperated), keyword(Should not be seperated: country names) Numbers: long, integer, short, byte, double, float boolean date object index: Specifies whether to create an index. The default value is true. analyzer: Specifies which analyzer to use. properties: To specify sub strings ","date":"2022-07-06","objectID":"/elasticsearch/:0:1","series":null,"tags":["Elasticsearch, java"],"title":"Elasticsearch","uri":"/elasticsearch/#analyzer"},{"categories":["Tech"],"content":"Index type: text(words that can be seperated), keyword(Should not be seperated: country names) Numbers: long, integer, short, byte, double, float boolean date object index: Specifies whether to create an index. The default value is true. analyzer: Specifies which analyzer to use. properties: To specify sub strings ","date":"2022-07-06","objectID":"/elasticsearch/:0:1","series":null,"tags":["Elasticsearch, java"],"title":"Elasticsearch","uri":"/elasticsearch/#properties"},{"categories":["Tech"],"content":"DSL index operations Create index and mapping using DSL: # Create index PUT /test { \"mappings\": { \"properties\": { \"info\": { \"type\": \"text\", \"analyzer\": \"standard\" }, \"email\": { \"type\": \"keyword\", \"index\": false }, \"name\": { \"type\": \"object\", \"properties\": { \"firstName\": { \"type\": \"keyword\" }, \"lastName\": { \"type\": \"keyword\" } } } } } } inspect, update and delete index # search GET /test # update(cannot be updated, so we need a brand new index) PUT /test/_mapping { \"properties\":{ \"age\":{ \"type\": \"integer\" } } } # delete DELETE /test ","date":"2022-07-06","objectID":"/elasticsearch/:0:2","series":null,"tags":["Elasticsearch, java"],"title":"Elasticsearch","uri":"/elasticsearch/#dsl-index-operations"},{"categories":["Tech"],"content":"DSL Document operations Insert document # Insert a new document POST /test/_doc/1 { \"info\": \"黑马程序员java讲师\", \"email\": \"112837@qq.com\", \"name\":{ \"firstName\":\"云\", \"lastName\":\"赵\" } } select and delete document # 查询 GET /heima/_doc/1 # 删除 DELETE /heima/_doc/1 update document: # delete old document and create a new one PUT /heima/_doc/1 { \"info\": \"黑马程序员java讲师\", \"email\": \"112837@qq.com\", \"name\":{ \"firstName\":\"云\", \"lastName\":\"赵\" } } 如果id在索引库里面不存在，并不会报错，而是直接新增，如果索引库存在该记录，就会先删掉该记录，然后增加一个全新的。 # 局部修改文档字段 # there has to be a \"doc\" POST /heima/_update/1 { \"doc\": { \"email\":\"lbwnb@qq.com\" } } Write DSL statements and create index libraries (equivalent to creating tables in MySQL) # 酒店的mapping PUT /hotel { \"mappings\": { \"properties\": { \"id\":{ \"type\": \"keyword\" }, \"name\":{ \"type\": \"text\" , \"analyzer\": \"ik_max_word\", \"copy_to\": \"all\" }, \"address\":{ \"type\": \"keyword\" , \"index\": false }, \"price\":{ \"type\": \"integer\" }, \"score\":{ \"type\": \"integer\" }, \"brand\":{ \"type\": \"keyword\", \"copy_to\": \"all\" }, \"city\":{ \"type\": \"keyword\" }, \"starName\":{ \"type\": \"keyword\" }, \"business\":{ \"type\": \"keyword\", \"copy_to\": \"all\" }, \"location\":{ \"type\": \"geo_point\" }, \"pic\":{ \"type\": \"keyword\" , \"index\": false }, \"all\":{ \"type\": \"text\", \"analyzer\": \"ik_max_word\" } } } } use text to do text analysis, use keyword to prevent analysis. #use copy_to to set up group index and search Store Restclient Search search all: a. match_all full text: analyze user input and search in inverted index (match_query, multi_match_query) # match 和 multi_match GET /hotel/_search { \"query\": { \"match\": { \"address\": \"如家外滩\" } } } GET /hotel/_search { \"query\": { \"multi_match\": { \"query\": \"外滩如家\", \"fields\": [\"brand\",\"name\",\"business\"] } } } keyword search: no analysis, search keywords(numbers, date,boolean): ids, range, term # 精确查询（term查询） GET /hotel/_search { \"query\": { \"term\": { \"city\": { \"value\": \"上海\" } } } } # 精确查询(范围range) GET /hotel/_search { \"query\": { \"range\": { \"price\": { \"gte\": 100, \"lte\": 300 } } } } # distance查询 GET /hotel/_search { \"query\": { \"geo_distance\":{ \"distance\": \"5km\", \"location\": \"31.21, 121.5\" } } } compound search boolean query combine multiple query must: \u0026\u0026 should: || must_not: 不算分 not filter: 不算分 GET /hotel/_search { \"query\": { \"bool\": { \"must\": [ {\"match\": { \"name\": \"如家\" }} ], \"must_not\": [ {\"range\": { \"price\": { \"gt\": 400 } }} ], \"filter\": [ { \"geo_distance\": { \"distance\": \"100km\", \"location\": { \"lat\": 31.21, \"lon\": 121.5 } } } ] } } } function score function score use BM25 algorithm GET /hotel/_search { \"query\": { \"function_score\": { \"query\": { \"match\": { \"address\": \"外滩\" } }, \"functions\": [ { \"filter\": { \"term\": { \"brand\": \"如家\" } }, \"weight\": 10 } ] } } } sort default: function score keyword, numbers, date, location Note: Elasticsearch won’t calculate scores when we are using sorting algorithm to optimize performance # sort排序 preice asc if scores are euqal GET /hotel/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"score\": \"desc\" }, { \"price\": \"asc\" } ]组合查询-function score 对应的Java RestClient代码： } //距离排序 distance to the location GET /hotel/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"_geo_distance\": { \"location\": { \"lat\": 31.03, \"lon\": 121.61 }, \"order\": \"asc\" , \"unit\": \"km\" } } ] } Page similar to limit in mysql GET /hotel/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"price\": \"asc\" } ], \"from\": 20 , \"size\": 5 } But Paging problems: the underlying layer of ES is inverted index, which is not conducive to paging, so paging query is a logical paging. For example, starting from 990, you need to intercept 10 pieces of data (10 pieces from 990 to 1000). For ES, you need to first query 0 to 1000 pieces of data, and then logically intercept 10 pieces of data by page. If it is a monomer, it is only a matter of efficiency at most, but if it is a cluster, it will be a bad thing. We need to get 1000 from each of the nodes and re-sort them. Then we get the 10 pieces from 990 to 1000. Too costly when the number is large. highlight highlight the search keyword By defau","date":"2022-07-06","objectID":"/elasticsearch/:0:3","series":null,"tags":["Elasticsearch, java"],"title":"Elasticsearch","uri":"/elasticsearch/#dsl-document-operations"},{"categories":["Tech"],"content":"DSL Document operations Insert document # Insert a new document POST /test/_doc/1 { \"info\": \"黑马程序员java讲师\", \"email\": \"112837@qq.com\", \"name\":{ \"firstName\":\"云\", \"lastName\":\"赵\" } } select and delete document # 查询 GET /heima/_doc/1 # 删除 DELETE /heima/_doc/1 update document: # delete old document and create a new one PUT /heima/_doc/1 { \"info\": \"黑马程序员java讲师\", \"email\": \"112837@qq.com\", \"name\":{ \"firstName\":\"云\", \"lastName\":\"赵\" } } 如果id在索引库里面不存在，并不会报错，而是直接新增，如果索引库存在该记录，就会先删掉该记录，然后增加一个全新的。 # 局部修改文档字段 # there has to be a \"doc\" POST /heima/_update/1 { \"doc\": { \"email\":\"lbwnb@qq.com\" } } Write DSL statements and create index libraries (equivalent to creating tables in MySQL) # 酒店的mapping PUT /hotel { \"mappings\": { \"properties\": { \"id\":{ \"type\": \"keyword\" }, \"name\":{ \"type\": \"text\" , \"analyzer\": \"ik_max_word\", \"copy_to\": \"all\" }, \"address\":{ \"type\": \"keyword\" , \"index\": false }, \"price\":{ \"type\": \"integer\" }, \"score\":{ \"type\": \"integer\" }, \"brand\":{ \"type\": \"keyword\", \"copy_to\": \"all\" }, \"city\":{ \"type\": \"keyword\" }, \"starName\":{ \"type\": \"keyword\" }, \"business\":{ \"type\": \"keyword\", \"copy_to\": \"all\" }, \"location\":{ \"type\": \"geo_point\" }, \"pic\":{ \"type\": \"keyword\" , \"index\": false }, \"all\":{ \"type\": \"text\", \"analyzer\": \"ik_max_word\" } } } } use text to do text analysis, use keyword to prevent analysis. #use copy_to to set up group index and search Store Restclient Search search all: a. match_all full text: analyze user input and search in inverted index (match_query, multi_match_query) # match 和 multi_match GET /hotel/_search { \"query\": { \"match\": { \"address\": \"如家外滩\" } } } GET /hotel/_search { \"query\": { \"multi_match\": { \"query\": \"外滩如家\", \"fields\": [\"brand\",\"name\",\"business\"] } } } keyword search: no analysis, search keywords(numbers, date,boolean): ids, range, term # 精确查询（term查询） GET /hotel/_search { \"query\": { \"term\": { \"city\": { \"value\": \"上海\" } } } } # 精确查询(范围range) GET /hotel/_search { \"query\": { \"range\": { \"price\": { \"gte\": 100, \"lte\": 300 } } } } # distance查询 GET /hotel/_search { \"query\": { \"geo_distance\":{ \"distance\": \"5km\", \"location\": \"31.21, 121.5\" } } } compound search boolean query combine multiple query must: \u0026\u0026 should: || must_not: 不算分 not filter: 不算分 GET /hotel/_search { \"query\": { \"bool\": { \"must\": [ {\"match\": { \"name\": \"如家\" }} ], \"must_not\": [ {\"range\": { \"price\": { \"gt\": 400 } }} ], \"filter\": [ { \"geo_distance\": { \"distance\": \"100km\", \"location\": { \"lat\": 31.21, \"lon\": 121.5 } } } ] } } } function score function score use BM25 algorithm GET /hotel/_search { \"query\": { \"function_score\": { \"query\": { \"match\": { \"address\": \"外滩\" } }, \"functions\": [ { \"filter\": { \"term\": { \"brand\": \"如家\" } }, \"weight\": 10 } ] } } } sort default: function score keyword, numbers, date, location Note: Elasticsearch won’t calculate scores when we are using sorting algorithm to optimize performance # sort排序 preice asc if scores are euqal GET /hotel/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"score\": \"desc\" }, { \"price\": \"asc\" } ]组合查询-function score 对应的Java RestClient代码： } //距离排序 distance to the location GET /hotel/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"_geo_distance\": { \"location\": { \"lat\": 31.03, \"lon\": 121.61 }, \"order\": \"asc\" , \"unit\": \"km\" } } ] } Page similar to limit in mysql GET /hotel/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"price\": \"asc\" } ], \"from\": 20 , \"size\": 5 } But Paging problems: the underlying layer of ES is inverted index, which is not conducive to paging, so paging query is a logical paging. For example, starting from 990, you need to intercept 10 pieces of data (10 pieces from 990 to 1000). For ES, you need to first query 0 to 1000 pieces of data, and then logically intercept 10 pieces of data by page. If it is a monomer, it is only a matter of efficiency at most, but if it is a cluster, it will be a bad thing. We need to get 1000 from each of the nodes and re-sort them. Then we get the 10 pieces from 990 to 1000. Too costly when the number is large. highlight highlight the search keyword By defau","date":"2022-07-06","objectID":"/elasticsearch/:0:3","series":null,"tags":["Elasticsearch, java"],"title":"Elasticsearch","uri":"/elasticsearch/#store"},{"categories":["Tech"],"content":"DSL Document operations Insert document # Insert a new document POST /test/_doc/1 { \"info\": \"黑马程序员java讲师\", \"email\": \"112837@qq.com\", \"name\":{ \"firstName\":\"云\", \"lastName\":\"赵\" } } select and delete document # 查询 GET /heima/_doc/1 # 删除 DELETE /heima/_doc/1 update document: # delete old document and create a new one PUT /heima/_doc/1 { \"info\": \"黑马程序员java讲师\", \"email\": \"112837@qq.com\", \"name\":{ \"firstName\":\"云\", \"lastName\":\"赵\" } } 如果id在索引库里面不存在，并不会报错，而是直接新增，如果索引库存在该记录，就会先删掉该记录，然后增加一个全新的。 # 局部修改文档字段 # there has to be a \"doc\" POST /heima/_update/1 { \"doc\": { \"email\":\"lbwnb@qq.com\" } } Write DSL statements and create index libraries (equivalent to creating tables in MySQL) # 酒店的mapping PUT /hotel { \"mappings\": { \"properties\": { \"id\":{ \"type\": \"keyword\" }, \"name\":{ \"type\": \"text\" , \"analyzer\": \"ik_max_word\", \"copy_to\": \"all\" }, \"address\":{ \"type\": \"keyword\" , \"index\": false }, \"price\":{ \"type\": \"integer\" }, \"score\":{ \"type\": \"integer\" }, \"brand\":{ \"type\": \"keyword\", \"copy_to\": \"all\" }, \"city\":{ \"type\": \"keyword\" }, \"starName\":{ \"type\": \"keyword\" }, \"business\":{ \"type\": \"keyword\", \"copy_to\": \"all\" }, \"location\":{ \"type\": \"geo_point\" }, \"pic\":{ \"type\": \"keyword\" , \"index\": false }, \"all\":{ \"type\": \"text\", \"analyzer\": \"ik_max_word\" } } } } use text to do text analysis, use keyword to prevent analysis. #use copy_to to set up group index and search Store Restclient Search search all: a. match_all full text: analyze user input and search in inverted index (match_query, multi_match_query) # match 和 multi_match GET /hotel/_search { \"query\": { \"match\": { \"address\": \"如家外滩\" } } } GET /hotel/_search { \"query\": { \"multi_match\": { \"query\": \"外滩如家\", \"fields\": [\"brand\",\"name\",\"business\"] } } } keyword search: no analysis, search keywords(numbers, date,boolean): ids, range, term # 精确查询（term查询） GET /hotel/_search { \"query\": { \"term\": { \"city\": { \"value\": \"上海\" } } } } # 精确查询(范围range) GET /hotel/_search { \"query\": { \"range\": { \"price\": { \"gte\": 100, \"lte\": 300 } } } } # distance查询 GET /hotel/_search { \"query\": { \"geo_distance\":{ \"distance\": \"5km\", \"location\": \"31.21, 121.5\" } } } compound search boolean query combine multiple query must: \u0026\u0026 should: || must_not: 不算分 not filter: 不算分 GET /hotel/_search { \"query\": { \"bool\": { \"must\": [ {\"match\": { \"name\": \"如家\" }} ], \"must_not\": [ {\"range\": { \"price\": { \"gt\": 400 } }} ], \"filter\": [ { \"geo_distance\": { \"distance\": \"100km\", \"location\": { \"lat\": 31.21, \"lon\": 121.5 } } } ] } } } function score function score use BM25 algorithm GET /hotel/_search { \"query\": { \"function_score\": { \"query\": { \"match\": { \"address\": \"外滩\" } }, \"functions\": [ { \"filter\": { \"term\": { \"brand\": \"如家\" } }, \"weight\": 10 } ] } } } sort default: function score keyword, numbers, date, location Note: Elasticsearch won’t calculate scores when we are using sorting algorithm to optimize performance # sort排序 preice asc if scores are euqal GET /hotel/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"score\": \"desc\" }, { \"price\": \"asc\" } ]组合查询-function score 对应的Java RestClient代码： } //距离排序 distance to the location GET /hotel/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"_geo_distance\": { \"location\": { \"lat\": 31.03, \"lon\": 121.61 }, \"order\": \"asc\" , \"unit\": \"km\" } } ] } Page similar to limit in mysql GET /hotel/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"price\": \"asc\" } ], \"from\": 20 , \"size\": 5 } But Paging problems: the underlying layer of ES is inverted index, which is not conducive to paging, so paging query is a logical paging. For example, starting from 990, you need to intercept 10 pieces of data (10 pieces from 990 to 1000). For ES, you need to first query 0 to 1000 pieces of data, and then logically intercept 10 pieces of data by page. If it is a monomer, it is only a matter of efficiency at most, but if it is a cluster, it will be a bad thing. We need to get 1000 from each of the nodes and re-sort them. Then we get the 10 pieces from 990 to 1000. Too costly when the number is large. highlight highlight the search keyword By defau","date":"2022-07-06","objectID":"/elasticsearch/:0:3","series":null,"tags":["Elasticsearch, java"],"title":"Elasticsearch","uri":"/elasticsearch/#search"},{"categories":["Tech"],"content":"DSL Document operations Insert document # Insert a new document POST /test/_doc/1 { \"info\": \"黑马程序员java讲师\", \"email\": \"112837@qq.com\", \"name\":{ \"firstName\":\"云\", \"lastName\":\"赵\" } } select and delete document # 查询 GET /heima/_doc/1 # 删除 DELETE /heima/_doc/1 update document: # delete old document and create a new one PUT /heima/_doc/1 { \"info\": \"黑马程序员java讲师\", \"email\": \"112837@qq.com\", \"name\":{ \"firstName\":\"云\", \"lastName\":\"赵\" } } 如果id在索引库里面不存在，并不会报错，而是直接新增，如果索引库存在该记录，就会先删掉该记录，然后增加一个全新的。 # 局部修改文档字段 # there has to be a \"doc\" POST /heima/_update/1 { \"doc\": { \"email\":\"lbwnb@qq.com\" } } Write DSL statements and create index libraries (equivalent to creating tables in MySQL) # 酒店的mapping PUT /hotel { \"mappings\": { \"properties\": { \"id\":{ \"type\": \"keyword\" }, \"name\":{ \"type\": \"text\" , \"analyzer\": \"ik_max_word\", \"copy_to\": \"all\" }, \"address\":{ \"type\": \"keyword\" , \"index\": false }, \"price\":{ \"type\": \"integer\" }, \"score\":{ \"type\": \"integer\" }, \"brand\":{ \"type\": \"keyword\", \"copy_to\": \"all\" }, \"city\":{ \"type\": \"keyword\" }, \"starName\":{ \"type\": \"keyword\" }, \"business\":{ \"type\": \"keyword\", \"copy_to\": \"all\" }, \"location\":{ \"type\": \"geo_point\" }, \"pic\":{ \"type\": \"keyword\" , \"index\": false }, \"all\":{ \"type\": \"text\", \"analyzer\": \"ik_max_word\" } } } } use text to do text analysis, use keyword to prevent analysis. #use copy_to to set up group index and search Store Restclient Search search all: a. match_all full text: analyze user input and search in inverted index (match_query, multi_match_query) # match 和 multi_match GET /hotel/_search { \"query\": { \"match\": { \"address\": \"如家外滩\" } } } GET /hotel/_search { \"query\": { \"multi_match\": { \"query\": \"外滩如家\", \"fields\": [\"brand\",\"name\",\"business\"] } } } keyword search: no analysis, search keywords(numbers, date,boolean): ids, range, term # 精确查询（term查询） GET /hotel/_search { \"query\": { \"term\": { \"city\": { \"value\": \"上海\" } } } } # 精确查询(范围range) GET /hotel/_search { \"query\": { \"range\": { \"price\": { \"gte\": 100, \"lte\": 300 } } } } # distance查询 GET /hotel/_search { \"query\": { \"geo_distance\":{ \"distance\": \"5km\", \"location\": \"31.21, 121.5\" } } } compound search boolean query combine multiple query must: \u0026\u0026 should: || must_not: 不算分 not filter: 不算分 GET /hotel/_search { \"query\": { \"bool\": { \"must\": [ {\"match\": { \"name\": \"如家\" }} ], \"must_not\": [ {\"range\": { \"price\": { \"gt\": 400 } }} ], \"filter\": [ { \"geo_distance\": { \"distance\": \"100km\", \"location\": { \"lat\": 31.21, \"lon\": 121.5 } } } ] } } } function score function score use BM25 algorithm GET /hotel/_search { \"query\": { \"function_score\": { \"query\": { \"match\": { \"address\": \"外滩\" } }, \"functions\": [ { \"filter\": { \"term\": { \"brand\": \"如家\" } }, \"weight\": 10 } ] } } } sort default: function score keyword, numbers, date, location Note: Elasticsearch won’t calculate scores when we are using sorting algorithm to optimize performance # sort排序 preice asc if scores are euqal GET /hotel/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"score\": \"desc\" }, { \"price\": \"asc\" } ]组合查询-function score 对应的Java RestClient代码： } //距离排序 distance to the location GET /hotel/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"_geo_distance\": { \"location\": { \"lat\": 31.03, \"lon\": 121.61 }, \"order\": \"asc\" , \"unit\": \"km\" } } ] } Page similar to limit in mysql GET /hotel/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"price\": \"asc\" } ], \"from\": 20 , \"size\": 5 } But Paging problems: the underlying layer of ES is inverted index, which is not conducive to paging, so paging query is a logical paging. For example, starting from 990, you need to intercept 10 pieces of data (10 pieces from 990 to 1000). For ES, you need to first query 0 to 1000 pieces of data, and then logically intercept 10 pieces of data by page. If it is a monomer, it is only a matter of efficiency at most, but if it is a cluster, it will be a bad thing. We need to get 1000 from each of the nodes and re-sort them. Then we get the 10 pieces from 990 to 1000. Too costly when the number is large. highlight highlight the search keyword By defau","date":"2022-07-06","objectID":"/elasticsearch/:0:3","series":null,"tags":["Elasticsearch, java"],"title":"Elasticsearch","uri":"/elasticsearch/#sort"},{"categories":["Tech"],"content":"DSL Document operations Insert document # Insert a new document POST /test/_doc/1 { \"info\": \"黑马程序员java讲师\", \"email\": \"112837@qq.com\", \"name\":{ \"firstName\":\"云\", \"lastName\":\"赵\" } } select and delete document # 查询 GET /heima/_doc/1 # 删除 DELETE /heima/_doc/1 update document: # delete old document and create a new one PUT /heima/_doc/1 { \"info\": \"黑马程序员java讲师\", \"email\": \"112837@qq.com\", \"name\":{ \"firstName\":\"云\", \"lastName\":\"赵\" } } 如果id在索引库里面不存在，并不会报错，而是直接新增，如果索引库存在该记录，就会先删掉该记录，然后增加一个全新的。 # 局部修改文档字段 # there has to be a \"doc\" POST /heima/_update/1 { \"doc\": { \"email\":\"lbwnb@qq.com\" } } Write DSL statements and create index libraries (equivalent to creating tables in MySQL) # 酒店的mapping PUT /hotel { \"mappings\": { \"properties\": { \"id\":{ \"type\": \"keyword\" }, \"name\":{ \"type\": \"text\" , \"analyzer\": \"ik_max_word\", \"copy_to\": \"all\" }, \"address\":{ \"type\": \"keyword\" , \"index\": false }, \"price\":{ \"type\": \"integer\" }, \"score\":{ \"type\": \"integer\" }, \"brand\":{ \"type\": \"keyword\", \"copy_to\": \"all\" }, \"city\":{ \"type\": \"keyword\" }, \"starName\":{ \"type\": \"keyword\" }, \"business\":{ \"type\": \"keyword\", \"copy_to\": \"all\" }, \"location\":{ \"type\": \"geo_point\" }, \"pic\":{ \"type\": \"keyword\" , \"index\": false }, \"all\":{ \"type\": \"text\", \"analyzer\": \"ik_max_word\" } } } } use text to do text analysis, use keyword to prevent analysis. #use copy_to to set up group index and search Store Restclient Search search all: a. match_all full text: analyze user input and search in inverted index (match_query, multi_match_query) # match 和 multi_match GET /hotel/_search { \"query\": { \"match\": { \"address\": \"如家外滩\" } } } GET /hotel/_search { \"query\": { \"multi_match\": { \"query\": \"外滩如家\", \"fields\": [\"brand\",\"name\",\"business\"] } } } keyword search: no analysis, search keywords(numbers, date,boolean): ids, range, term # 精确查询（term查询） GET /hotel/_search { \"query\": { \"term\": { \"city\": { \"value\": \"上海\" } } } } # 精确查询(范围range) GET /hotel/_search { \"query\": { \"range\": { \"price\": { \"gte\": 100, \"lte\": 300 } } } } # distance查询 GET /hotel/_search { \"query\": { \"geo_distance\":{ \"distance\": \"5km\", \"location\": \"31.21, 121.5\" } } } compound search boolean query combine multiple query must: \u0026\u0026 should: || must_not: 不算分 not filter: 不算分 GET /hotel/_search { \"query\": { \"bool\": { \"must\": [ {\"match\": { \"name\": \"如家\" }} ], \"must_not\": [ {\"range\": { \"price\": { \"gt\": 400 } }} ], \"filter\": [ { \"geo_distance\": { \"distance\": \"100km\", \"location\": { \"lat\": 31.21, \"lon\": 121.5 } } } ] } } } function score function score use BM25 algorithm GET /hotel/_search { \"query\": { \"function_score\": { \"query\": { \"match\": { \"address\": \"外滩\" } }, \"functions\": [ { \"filter\": { \"term\": { \"brand\": \"如家\" } }, \"weight\": 10 } ] } } } sort default: function score keyword, numbers, date, location Note: Elasticsearch won’t calculate scores when we are using sorting algorithm to optimize performance # sort排序 preice asc if scores are euqal GET /hotel/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"score\": \"desc\" }, { \"price\": \"asc\" } ]组合查询-function score 对应的Java RestClient代码： } //距离排序 distance to the location GET /hotel/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"_geo_distance\": { \"location\": { \"lat\": 31.03, \"lon\": 121.61 }, \"order\": \"asc\" , \"unit\": \"km\" } } ] } Page similar to limit in mysql GET /hotel/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"price\": \"asc\" } ], \"from\": 20 , \"size\": 5 } But Paging problems: the underlying layer of ES is inverted index, which is not conducive to paging, so paging query is a logical paging. For example, starting from 990, you need to intercept 10 pieces of data (10 pieces from 990 to 1000). For ES, you need to first query 0 to 1000 pieces of data, and then logically intercept 10 pieces of data by page. If it is a monomer, it is only a matter of efficiency at most, but if it is a cluster, it will be a bad thing. We need to get 1000 from each of the nodes and re-sort them. Then we get the 10 pieces from 990 to 1000. Too costly when the number is large. highlight highlight the search keyword By defau","date":"2022-07-06","objectID":"/elasticsearch/:0:3","series":null,"tags":["Elasticsearch, java"],"title":"Elasticsearch","uri":"/elasticsearch/#page"},{"categories":["Tech"],"content":"DSL Document operations Insert document # Insert a new document POST /test/_doc/1 { \"info\": \"黑马程序员java讲师\", \"email\": \"112837@qq.com\", \"name\":{ \"firstName\":\"云\", \"lastName\":\"赵\" } } select and delete document # 查询 GET /heima/_doc/1 # 删除 DELETE /heima/_doc/1 update document: # delete old document and create a new one PUT /heima/_doc/1 { \"info\": \"黑马程序员java讲师\", \"email\": \"112837@qq.com\", \"name\":{ \"firstName\":\"云\", \"lastName\":\"赵\" } } 如果id在索引库里面不存在，并不会报错，而是直接新增，如果索引库存在该记录，就会先删掉该记录，然后增加一个全新的。 # 局部修改文档字段 # there has to be a \"doc\" POST /heima/_update/1 { \"doc\": { \"email\":\"lbwnb@qq.com\" } } Write DSL statements and create index libraries (equivalent to creating tables in MySQL) # 酒店的mapping PUT /hotel { \"mappings\": { \"properties\": { \"id\":{ \"type\": \"keyword\" }, \"name\":{ \"type\": \"text\" , \"analyzer\": \"ik_max_word\", \"copy_to\": \"all\" }, \"address\":{ \"type\": \"keyword\" , \"index\": false }, \"price\":{ \"type\": \"integer\" }, \"score\":{ \"type\": \"integer\" }, \"brand\":{ \"type\": \"keyword\", \"copy_to\": \"all\" }, \"city\":{ \"type\": \"keyword\" }, \"starName\":{ \"type\": \"keyword\" }, \"business\":{ \"type\": \"keyword\", \"copy_to\": \"all\" }, \"location\":{ \"type\": \"geo_point\" }, \"pic\":{ \"type\": \"keyword\" , \"index\": false }, \"all\":{ \"type\": \"text\", \"analyzer\": \"ik_max_word\" } } } } use text to do text analysis, use keyword to prevent analysis. #use copy_to to set up group index and search Store Restclient Search search all: a. match_all full text: analyze user input and search in inverted index (match_query, multi_match_query) # match 和 multi_match GET /hotel/_search { \"query\": { \"match\": { \"address\": \"如家外滩\" } } } GET /hotel/_search { \"query\": { \"multi_match\": { \"query\": \"外滩如家\", \"fields\": [\"brand\",\"name\",\"business\"] } } } keyword search: no analysis, search keywords(numbers, date,boolean): ids, range, term # 精确查询（term查询） GET /hotel/_search { \"query\": { \"term\": { \"city\": { \"value\": \"上海\" } } } } # 精确查询(范围range) GET /hotel/_search { \"query\": { \"range\": { \"price\": { \"gte\": 100, \"lte\": 300 } } } } # distance查询 GET /hotel/_search { \"query\": { \"geo_distance\":{ \"distance\": \"5km\", \"location\": \"31.21, 121.5\" } } } compound search boolean query combine multiple query must: \u0026\u0026 should: || must_not: 不算分 not filter: 不算分 GET /hotel/_search { \"query\": { \"bool\": { \"must\": [ {\"match\": { \"name\": \"如家\" }} ], \"must_not\": [ {\"range\": { \"price\": { \"gt\": 400 } }} ], \"filter\": [ { \"geo_distance\": { \"distance\": \"100km\", \"location\": { \"lat\": 31.21, \"lon\": 121.5 } } } ] } } } function score function score use BM25 algorithm GET /hotel/_search { \"query\": { \"function_score\": { \"query\": { \"match\": { \"address\": \"外滩\" } }, \"functions\": [ { \"filter\": { \"term\": { \"brand\": \"如家\" } }, \"weight\": 10 } ] } } } sort default: function score keyword, numbers, date, location Note: Elasticsearch won’t calculate scores when we are using sorting algorithm to optimize performance # sort排序 preice asc if scores are euqal GET /hotel/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"score\": \"desc\" }, { \"price\": \"asc\" } ]组合查询-function score 对应的Java RestClient代码： } //距离排序 distance to the location GET /hotel/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"_geo_distance\": { \"location\": { \"lat\": 31.03, \"lon\": 121.61 }, \"order\": \"asc\" , \"unit\": \"km\" } } ] } Page similar to limit in mysql GET /hotel/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"price\": \"asc\" } ], \"from\": 20 , \"size\": 5 } But Paging problems: the underlying layer of ES is inverted index, which is not conducive to paging, so paging query is a logical paging. For example, starting from 990, you need to intercept 10 pieces of data (10 pieces from 990 to 1000). For ES, you need to first query 0 to 1000 pieces of data, and then logically intercept 10 pieces of data by page. If it is a monomer, it is only a matter of efficiency at most, but if it is a cluster, it will be a bad thing. We need to get 1000 from each of the nodes and re-sort them. Then we get the 10 pieces from 990 to 1000. Too costly when the number is large. highlight highlight the search keyword By defau","date":"2022-07-06","objectID":"/elasticsearch/:0:3","series":null,"tags":["Elasticsearch, java"],"title":"Elasticsearch","uri":"/elasticsearch/#highlight"},{"categories":["Tech"],"content":"[ ](https://blog.csdn.net/weixin_44757863/article/details/120959505)Aggregations Aggregations can be used to do analysis, computing and statistics analysis. Bucket: grouping TermAggregation By default, return a _count for field of the terms in desc order. We can use query to limit range of the TermAggregation. Metric: max, min, avg, stats based on the result of BucketAggregation Pipeline Aggregations of other aggregations(avg of a group) ","date":"2022-07-06","objectID":"/elasticsearch/:0:4","series":null,"tags":["Elasticsearch, java"],"title":"Elasticsearch","uri":"/elasticsearch/#heading"},{"categories":["Tech"],"content":"AutoSync between database and ES Use rabbitmq messages to decouple the service and update ","date":"2022-07-06","objectID":"/elasticsearch/:0:5","series":null,"tags":["Elasticsearch, java"],"title":"Elasticsearch","uri":"/elasticsearch/#autosync-between-database-and-es"},{"categories":["Tech"],"content":"ES Cluster An example of a robust ES cluster. split-brain problem: When the master is having network issues for a long time, other nodes notice the situation and decide to elect a node to be the master node, so there will be 2 master nodes when the old master node back online. Solving: setting the consensus vote to be (eligible nodes + 1) / 2.(In versions after Elasticsearch 7.0, the number of eligible nodes is automatically calculated, so there are unlikely to be split-brain problem) ","date":"2022-07-06","objectID":"/elasticsearch/:0:6","series":null,"tags":["Elasticsearch, java"],"title":"Elasticsearch","uri":"/elasticsearch/#es-cluster"},{"categories":["Tech"],"content":" Decoupling Event-driven design better performance ","date":"2022-06-18","objectID":"/rabbitmq/:0:0","series":null,"tags":["RabbitMQ"],"title":"RabbitMQ","uri":"/rabbitmq/#"},{"categories":["Tech"],"content":"Problems with Synchronous communication(Feign): 1.Coupling 2.can not handle huge amount of requests in a short time(high concurrency). 3.Waste of resources during waiting 4.Cascade failure: service failed, the cluster fails cascadingly ","date":"2022-06-18","objectID":"/rabbitmq/:0:1","series":null,"tags":["RabbitMQ"],"title":"RabbitMQ","uri":"/rabbitmq/#problems-with-synchronous-communicationfeign"},{"categories":["Tech"],"content":"Asynchronous(Event driven design): Use Broker to control events. Service publish events and the others subscribed to the broker waiting for events. Better performance/No cascade failures Advantages of asynchronous communication: Low coupling Improve throughput Fault isolation Flow peaking Disadvantages of asynchronous communication: Rely on Broker’s reliability, security and throughput The architecture is complicated, the business does not have an obvious process line, which is not easy to track and manage. ","date":"2022-06-18","objectID":"/rabbitmq/:0:2","series":null,"tags":["RabbitMQ"],"title":"RabbitMQ","uri":"/rabbitmq/#asynchronousevent-driven-design"},{"categories":["Tech"],"content":"Rabbitmq Message sending process for basic message queue: Establish a connection Create a channel Declare queues using created channels Use channel to send messages to queues Message receiving process for basic message queue: Establish a connection Create a channel Use the channel to declare the queue Declare consumer method handleDelivery() Use channel to bind consumers to queues ","date":"2022-06-18","objectID":"/rabbitmq/:0:3","series":null,"tags":["RabbitMQ"],"title":"RabbitMQ","uri":"/rabbitmq/#rabbitmq"},{"categories":["Tech"],"content":"Spring amqp Use rabbitTemplate to send and get messages queues can use same binding key(which makes them fanout exchange) Describe the difference between Direct switch and Fanout switch? Fanout: routes messages to every queue bound to it. Direct: Determines Which Queue It Is Routed To According To RouteKey. • If multiple queues have the same RoutingKey, it is similar to Fanout function. The common annotations based on @RabbitListener annotation declaration queues and switches: @Queue @Exchange ","date":"2022-06-18","objectID":"/rabbitmq/:0:4","series":null,"tags":["RabbitMQ"],"title":"RabbitMQ","uri":"/rabbitmq/#spring-amqp"},{"categories":["Tech"],"content":"TopicExchange In message queues, any object will be serialized. But the performance of the default serialization method(JDK serialization) is not really satisfying(takes too much space). We can use Jackson converter to improve its performance. Note: Some bugs may appear while using Jackson converter./ Note: consumer and publisher must be using the same message converter. ","date":"2022-06-18","objectID":"/rabbitmq/:0:5","series":null,"tags":["RabbitMQ"],"title":"RabbitMQ","uri":"/rabbitmq/#topicexchange"},{"categories":["Tech"],"content":"Reliability publisher confirm: publisher-confirm-type = correlate Enabling publisher confirms calls back a confirm function to identify whether the process succeeds. (Producer to exchange) true: ack false: nack To define callback: rabbitTemplate.setConfirmCallback() confirm as an anonymous class method confirm(CorrelationData, ack, cause) ack: whether the exchange have received the message correlationData: info can be manually set (id) publisher-return: exchange to queue: if failed return “return Callback” publisher-returns: true template: mandatory: true When the message from the exchange did not get to the queue, the default mode is throwing the message away, but we can change the mode of returnCallback notify the publisher of the failure. rabbbitTemplate.returnsCallback() default mode: discard message, not returning the returnedMessage method deals with messages in the exchange but not sent to the queues. Consumer Ack: acknowledge = none As soon as the consumer receives the message, it send the ack message (No service successfulness confirmation) acknowledge = “manual” Manually use channel.basicAck() to confirm and channel.basicNack() to let the publisher resend the message. auto Use republishMessageRecover to bind error message and its routing key. How to guarantee reliability？ Enbale publisher confirms and publisher returns to ensure message is sent to the exchange and queues Enable durability to ensure messages are durable Use consumer ack to ensure the consumer gets messages from queues Set acknowlwdge-mode to auto or simply use republishMessageRecoverer to bind error message queue and exchange. flow control ： use manual ack mode listener-container prefetch = 1000 to control the flow. (The consumer get 1000 messages every time, need manual confirmation to continue) TTL: time to live: before the message got discarded. x-message-tt queue ttl can be set in the confiogurations.(Delete all messages in the queue) ms message ttl need to implement messagePostProcessor to setExpiration in the message properties(When the expired message is on top of the queue, it will be deleted.) Setting queue ttl and message ttl at the same time, it will use the shorter one. DLX dead letter exchange When a message is dead, it can be sent to another exchange. It is called DLX. Dead letter queue summary: There is no difference between dead letter switches and dead letter queues and Xitong. When the message becomes a dead letter, if the queue is bound to a dead letter switch, the message will be rerouted to the dead letter team by the dead letter switch. Messages become dead letter: exceeded limit of the queue consumer refused(basicNack) and requeue = false queue expired/message expired parameters: x-dead-letter-exchange and x-dead-letter-routing-key Use plugin to create delayqueue. Install: Use: In the listener, declare delayed = true in th exchange setting. In the publisher, set message property: .setHeader(“x-delay”, 5000) The message will be delayed, but the publisher returns will get errors for not able to send the message directly to the exchange. In the message, the ReceivedDelay property will be the x-delay we set. So one possible solution would be using conditional statement to skip the publisher return if (message.getMessageProperties().getReceivedDelay() \u003e 0) { return; } Message stuck solutions: more consumers thread pool increase volume of the queue Lazy Queues Note that normal rabbitmq queues are stored in the memory. But the lazy queues directly store the messages in the disk(This will cause some performance issues). When the consumers are consuming messages, the messages are read from the disk into the memory.(This supports millions of messages storing.) To set one queue to be lazy queue, simply set x-queue-mode to “lazy” or @arguments = @Argument(name = “x-queue-mode”, value = “lazy”) Pro: higher upper limit(disk storage) no intermittent page-out more stable Con: longer delay Subject to the disk IO performance ","date":"2022-06-18","objectID":"/rabbitmq/:0:6","series":null,"tags":["RabbitMQ"],"title":"RabbitMQ","uri":"/rabbitmq/#reliability"},{"categories":["Tech"],"content":"Reliability publisher confirm: publisher-confirm-type = correlate Enabling publisher confirms calls back a confirm function to identify whether the process succeeds. (Producer to exchange) true: ack false: nack To define callback: rabbitTemplate.setConfirmCallback() confirm as an anonymous class method confirm(CorrelationData, ack, cause) ack: whether the exchange have received the message correlationData: info can be manually set (id) publisher-return: exchange to queue: if failed return “return Callback” publisher-returns: true template: mandatory: true When the message from the exchange did not get to the queue, the default mode is throwing the message away, but we can change the mode of returnCallback notify the publisher of the failure. rabbbitTemplate.returnsCallback() default mode: discard message, not returning the returnedMessage method deals with messages in the exchange but not sent to the queues. Consumer Ack: acknowledge = none As soon as the consumer receives the message, it send the ack message (No service successfulness confirmation) acknowledge = “manual” Manually use channel.basicAck() to confirm and channel.basicNack() to let the publisher resend the message. auto Use republishMessageRecover to bind error message and its routing key. How to guarantee reliability？ Enbale publisher confirms and publisher returns to ensure message is sent to the exchange and queues Enable durability to ensure messages are durable Use consumer ack to ensure the consumer gets messages from queues Set acknowlwdge-mode to auto or simply use republishMessageRecoverer to bind error message queue and exchange. flow control ： use manual ack mode listener-container prefetch = 1000 to control the flow. (The consumer get 1000 messages every time, need manual confirmation to continue) TTL: time to live: before the message got discarded. x-message-tt queue ttl can be set in the confiogurations.(Delete all messages in the queue) ms message ttl need to implement messagePostProcessor to setExpiration in the message properties(When the expired message is on top of the queue, it will be deleted.) Setting queue ttl and message ttl at the same time, it will use the shorter one. DLX dead letter exchange When a message is dead, it can be sent to another exchange. It is called DLX. Dead letter queue summary: There is no difference between dead letter switches and dead letter queues and Xitong. When the message becomes a dead letter, if the queue is bound to a dead letter switch, the message will be rerouted to the dead letter team by the dead letter switch. Messages become dead letter: exceeded limit of the queue consumer refused(basicNack) and requeue = false queue expired/message expired parameters: x-dead-letter-exchange and x-dead-letter-routing-key Use plugin to create delayqueue. Install: Use: In the listener, declare delayed = true in th exchange setting. In the publisher, set message property: .setHeader(“x-delay”, 5000) The message will be delayed, but the publisher returns will get errors for not able to send the message directly to the exchange. In the message, the ReceivedDelay property will be the x-delay we set. So one possible solution would be using conditional statement to skip the publisher return if (message.getMessageProperties().getReceivedDelay() 0) { return; } Message stuck solutions: more consumers thread pool increase volume of the queue Lazy Queues Note that normal rabbitmq queues are stored in the memory. But the lazy queues directly store the messages in the disk(This will cause some performance issues). When the consumers are consuming messages, the messages are read from the disk into the memory.(This supports millions of messages storing.) To set one queue to be lazy queue, simply set x-queue-mode to “lazy” or @arguments = @Argument(name = “x-queue-mode”, value = “lazy”) Pro: higher upper limit(disk storage) no intermittent page-out more stable Con: longer delay Subject to the disk IO performance ","date":"2022-06-18","objectID":"/rabbitmq/:0:6","series":null,"tags":["RabbitMQ"],"title":"RabbitMQ","uri":"/rabbitmq/#publisher-confirm"},{"categories":["Tech"],"content":"Reliability publisher confirm: publisher-confirm-type = correlate Enabling publisher confirms calls back a confirm function to identify whether the process succeeds. (Producer to exchange) true: ack false: nack To define callback: rabbitTemplate.setConfirmCallback() confirm as an anonymous class method confirm(CorrelationData, ack, cause) ack: whether the exchange have received the message correlationData: info can be manually set (id) publisher-return: exchange to queue: if failed return “return Callback” publisher-returns: true template: mandatory: true When the message from the exchange did not get to the queue, the default mode is throwing the message away, but we can change the mode of returnCallback notify the publisher of the failure. rabbbitTemplate.returnsCallback() default mode: discard message, not returning the returnedMessage method deals with messages in the exchange but not sent to the queues. Consumer Ack: acknowledge = none As soon as the consumer receives the message, it send the ack message (No service successfulness confirmation) acknowledge = “manual” Manually use channel.basicAck() to confirm and channel.basicNack() to let the publisher resend the message. auto Use republishMessageRecover to bind error message and its routing key. How to guarantee reliability？ Enbale publisher confirms and publisher returns to ensure message is sent to the exchange and queues Enable durability to ensure messages are durable Use consumer ack to ensure the consumer gets messages from queues Set acknowlwdge-mode to auto or simply use republishMessageRecoverer to bind error message queue and exchange. flow control ： use manual ack mode listener-container prefetch = 1000 to control the flow. (The consumer get 1000 messages every time, need manual confirmation to continue) TTL: time to live: before the message got discarded. x-message-tt queue ttl can be set in the confiogurations.(Delete all messages in the queue) ms message ttl need to implement messagePostProcessor to setExpiration in the message properties(When the expired message is on top of the queue, it will be deleted.) Setting queue ttl and message ttl at the same time, it will use the shorter one. DLX dead letter exchange When a message is dead, it can be sent to another exchange. It is called DLX. Dead letter queue summary: There is no difference between dead letter switches and dead letter queues and Xitong. When the message becomes a dead letter, if the queue is bound to a dead letter switch, the message will be rerouted to the dead letter team by the dead letter switch. Messages become dead letter: exceeded limit of the queue consumer refused(basicNack) and requeue = false queue expired/message expired parameters: x-dead-letter-exchange and x-dead-letter-routing-key Use plugin to create delayqueue. Install: Use: In the listener, declare delayed = true in th exchange setting. In the publisher, set message property: .setHeader(“x-delay”, 5000) The message will be delayed, but the publisher returns will get errors for not able to send the message directly to the exchange. In the message, the ReceivedDelay property will be the x-delay we set. So one possible solution would be using conditional statement to skip the publisher return if (message.getMessageProperties().getReceivedDelay() 0) { return; } Message stuck solutions: more consumers thread pool increase volume of the queue Lazy Queues Note that normal rabbitmq queues are stored in the memory. But the lazy queues directly store the messages in the disk(This will cause some performance issues). When the consumers are consuming messages, the messages are read from the disk into the memory.(This supports millions of messages storing.) To set one queue to be lazy queue, simply set x-queue-mode to “lazy” or @arguments = @Argument(name = “x-queue-mode”, value = “lazy”) Pro: higher upper limit(disk storage) no intermittent page-out more stable Con: longer delay Subject to the disk IO performance ","date":"2022-06-18","objectID":"/rabbitmq/:0:6","series":null,"tags":["RabbitMQ"],"title":"RabbitMQ","uri":"/rabbitmq/#publisher-return"},{"categories":["Tech"],"content":"Reliability publisher confirm: publisher-confirm-type = correlate Enabling publisher confirms calls back a confirm function to identify whether the process succeeds. (Producer to exchange) true: ack false: nack To define callback: rabbitTemplate.setConfirmCallback() confirm as an anonymous class method confirm(CorrelationData, ack, cause) ack: whether the exchange have received the message correlationData: info can be manually set (id) publisher-return: exchange to queue: if failed return “return Callback” publisher-returns: true template: mandatory: true When the message from the exchange did not get to the queue, the default mode is throwing the message away, but we can change the mode of returnCallback notify the publisher of the failure. rabbbitTemplate.returnsCallback() default mode: discard message, not returning the returnedMessage method deals with messages in the exchange but not sent to the queues. Consumer Ack: acknowledge = none As soon as the consumer receives the message, it send the ack message (No service successfulness confirmation) acknowledge = “manual” Manually use channel.basicAck() to confirm and channel.basicNack() to let the publisher resend the message. auto Use republishMessageRecover to bind error message and its routing key. How to guarantee reliability？ Enbale publisher confirms and publisher returns to ensure message is sent to the exchange and queues Enable durability to ensure messages are durable Use consumer ack to ensure the consumer gets messages from queues Set acknowlwdge-mode to auto or simply use republishMessageRecoverer to bind error message queue and exchange. flow control ： use manual ack mode listener-container prefetch = 1000 to control the flow. (The consumer get 1000 messages every time, need manual confirmation to continue) TTL: time to live: before the message got discarded. x-message-tt queue ttl can be set in the confiogurations.(Delete all messages in the queue) ms message ttl need to implement messagePostProcessor to setExpiration in the message properties(When the expired message is on top of the queue, it will be deleted.) Setting queue ttl and message ttl at the same time, it will use the shorter one. DLX dead letter exchange When a message is dead, it can be sent to another exchange. It is called DLX. Dead letter queue summary: There is no difference between dead letter switches and dead letter queues and Xitong. When the message becomes a dead letter, if the queue is bound to a dead letter switch, the message will be rerouted to the dead letter team by the dead letter switch. Messages become dead letter: exceeded limit of the queue consumer refused(basicNack) and requeue = false queue expired/message expired parameters: x-dead-letter-exchange and x-dead-letter-routing-key Use plugin to create delayqueue. Install: Use: In the listener, declare delayed = true in th exchange setting. In the publisher, set message property: .setHeader(“x-delay”, 5000) The message will be delayed, but the publisher returns will get errors for not able to send the message directly to the exchange. In the message, the ReceivedDelay property will be the x-delay we set. So one possible solution would be using conditional statement to skip the publisher return if (message.getMessageProperties().getReceivedDelay() 0) { return; } Message stuck solutions: more consumers thread pool increase volume of the queue Lazy Queues Note that normal rabbitmq queues are stored in the memory. But the lazy queues directly store the messages in the disk(This will cause some performance issues). When the consumers are consuming messages, the messages are read from the disk into the memory.(This supports millions of messages storing.) To set one queue to be lazy queue, simply set x-queue-mode to “lazy” or @arguments = @Argument(name = “x-queue-mode”, value = “lazy”) Pro: higher upper limit(disk storage) no intermittent page-out more stable Con: longer delay Subject to the disk IO performance ","date":"2022-06-18","objectID":"/rabbitmq/:0:6","series":null,"tags":["RabbitMQ"],"title":"RabbitMQ","uri":"/rabbitmq/#consumer-ack"},{"categories":["Tech"],"content":"Reliability publisher confirm: publisher-confirm-type = correlate Enabling publisher confirms calls back a confirm function to identify whether the process succeeds. (Producer to exchange) true: ack false: nack To define callback: rabbitTemplate.setConfirmCallback() confirm as an anonymous class method confirm(CorrelationData, ack, cause) ack: whether the exchange have received the message correlationData: info can be manually set (id) publisher-return: exchange to queue: if failed return “return Callback” publisher-returns: true template: mandatory: true When the message from the exchange did not get to the queue, the default mode is throwing the message away, but we can change the mode of returnCallback notify the publisher of the failure. rabbbitTemplate.returnsCallback() default mode: discard message, not returning the returnedMessage method deals with messages in the exchange but not sent to the queues. Consumer Ack: acknowledge = none As soon as the consumer receives the message, it send the ack message (No service successfulness confirmation) acknowledge = “manual” Manually use channel.basicAck() to confirm and channel.basicNack() to let the publisher resend the message. auto Use republishMessageRecover to bind error message and its routing key. How to guarantee reliability？ Enbale publisher confirms and publisher returns to ensure message is sent to the exchange and queues Enable durability to ensure messages are durable Use consumer ack to ensure the consumer gets messages from queues Set acknowlwdge-mode to auto or simply use republishMessageRecoverer to bind error message queue and exchange. flow control ： use manual ack mode listener-container prefetch = 1000 to control the flow. (The consumer get 1000 messages every time, need manual confirmation to continue) TTL: time to live: before the message got discarded. x-message-tt queue ttl can be set in the confiogurations.(Delete all messages in the queue) ms message ttl need to implement messagePostProcessor to setExpiration in the message properties(When the expired message is on top of the queue, it will be deleted.) Setting queue ttl and message ttl at the same time, it will use the shorter one. DLX dead letter exchange When a message is dead, it can be sent to another exchange. It is called DLX. Dead letter queue summary: There is no difference between dead letter switches and dead letter queues and Xitong. When the message becomes a dead letter, if the queue is bound to a dead letter switch, the message will be rerouted to the dead letter team by the dead letter switch. Messages become dead letter: exceeded limit of the queue consumer refused(basicNack) and requeue = false queue expired/message expired parameters: x-dead-letter-exchange and x-dead-letter-routing-key Use plugin to create delayqueue. Install: Use: In the listener, declare delayed = true in th exchange setting. In the publisher, set message property: .setHeader(“x-delay”, 5000) The message will be delayed, but the publisher returns will get errors for not able to send the message directly to the exchange. In the message, the ReceivedDelay property will be the x-delay we set. So one possible solution would be using conditional statement to skip the publisher return if (message.getMessageProperties().getReceivedDelay() 0) { return; } Message stuck solutions: more consumers thread pool increase volume of the queue Lazy Queues Note that normal rabbitmq queues are stored in the memory. But the lazy queues directly store the messages in the disk(This will cause some performance issues). When the consumers are consuming messages, the messages are read from the disk into the memory.(This supports millions of messages storing.) To set one queue to be lazy queue, simply set x-queue-mode to “lazy” or @arguments = @Argument(name = “x-queue-mode”, value = “lazy”) Pro: higher upper limit(disk storage) no intermittent page-out more stable Con: longer delay Subject to the disk IO performance ","date":"2022-06-18","objectID":"/rabbitmq/:0:6","series":null,"tags":["RabbitMQ"],"title":"RabbitMQ","uri":"/rabbitmq/#ttl"},{"categories":["Tech"],"content":"Reliability publisher confirm: publisher-confirm-type = correlate Enabling publisher confirms calls back a confirm function to identify whether the process succeeds. (Producer to exchange) true: ack false: nack To define callback: rabbitTemplate.setConfirmCallback() confirm as an anonymous class method confirm(CorrelationData, ack, cause) ack: whether the exchange have received the message correlationData: info can be manually set (id) publisher-return: exchange to queue: if failed return “return Callback” publisher-returns: true template: mandatory: true When the message from the exchange did not get to the queue, the default mode is throwing the message away, but we can change the mode of returnCallback notify the publisher of the failure. rabbbitTemplate.returnsCallback() default mode: discard message, not returning the returnedMessage method deals with messages in the exchange but not sent to the queues. Consumer Ack: acknowledge = none As soon as the consumer receives the message, it send the ack message (No service successfulness confirmation) acknowledge = “manual” Manually use channel.basicAck() to confirm and channel.basicNack() to let the publisher resend the message. auto Use republishMessageRecover to bind error message and its routing key. How to guarantee reliability？ Enbale publisher confirms and publisher returns to ensure message is sent to the exchange and queues Enable durability to ensure messages are durable Use consumer ack to ensure the consumer gets messages from queues Set acknowlwdge-mode to auto or simply use republishMessageRecoverer to bind error message queue and exchange. flow control ： use manual ack mode listener-container prefetch = 1000 to control the flow. (The consumer get 1000 messages every time, need manual confirmation to continue) TTL: time to live: before the message got discarded. x-message-tt queue ttl can be set in the confiogurations.(Delete all messages in the queue) ms message ttl need to implement messagePostProcessor to setExpiration in the message properties(When the expired message is on top of the queue, it will be deleted.) Setting queue ttl and message ttl at the same time, it will use the shorter one. DLX dead letter exchange When a message is dead, it can be sent to another exchange. It is called DLX. Dead letter queue summary: There is no difference between dead letter switches and dead letter queues and Xitong. When the message becomes a dead letter, if the queue is bound to a dead letter switch, the message will be rerouted to the dead letter team by the dead letter switch. Messages become dead letter: exceeded limit of the queue consumer refused(basicNack) and requeue = false queue expired/message expired parameters: x-dead-letter-exchange and x-dead-letter-routing-key Use plugin to create delayqueue. Install: Use: In the listener, declare delayed = true in th exchange setting. In the publisher, set message property: .setHeader(“x-delay”, 5000) The message will be delayed, but the publisher returns will get errors for not able to send the message directly to the exchange. In the message, the ReceivedDelay property will be the x-delay we set. So one possible solution would be using conditional statement to skip the publisher return if (message.getMessageProperties().getReceivedDelay() 0) { return; } Message stuck solutions: more consumers thread pool increase volume of the queue Lazy Queues Note that normal rabbitmq queues are stored in the memory. But the lazy queues directly store the messages in the disk(This will cause some performance issues). When the consumers are consuming messages, the messages are read from the disk into the memory.(This supports millions of messages storing.) To set one queue to be lazy queue, simply set x-queue-mode to “lazy” or @arguments = @Argument(name = “x-queue-mode”, value = “lazy”) Pro: higher upper limit(disk storage) no intermittent page-out more stable Con: longer delay Subject to the disk IO performance ","date":"2022-06-18","objectID":"/rabbitmq/:0:6","series":null,"tags":["RabbitMQ"],"title":"RabbitMQ","uri":"/rabbitmq/#dlx-dead-letter-exchange"},{"categories":["Tech"],"content":"Reliability publisher confirm: publisher-confirm-type = correlate Enabling publisher confirms calls back a confirm function to identify whether the process succeeds. (Producer to exchange) true: ack false: nack To define callback: rabbitTemplate.setConfirmCallback() confirm as an anonymous class method confirm(CorrelationData, ack, cause) ack: whether the exchange have received the message correlationData: info can be manually set (id) publisher-return: exchange to queue: if failed return “return Callback” publisher-returns: true template: mandatory: true When the message from the exchange did not get to the queue, the default mode is throwing the message away, but we can change the mode of returnCallback notify the publisher of the failure. rabbbitTemplate.returnsCallback() default mode: discard message, not returning the returnedMessage method deals with messages in the exchange but not sent to the queues. Consumer Ack: acknowledge = none As soon as the consumer receives the message, it send the ack message (No service successfulness confirmation) acknowledge = “manual” Manually use channel.basicAck() to confirm and channel.basicNack() to let the publisher resend the message. auto Use republishMessageRecover to bind error message and its routing key. How to guarantee reliability？ Enbale publisher confirms and publisher returns to ensure message is sent to the exchange and queues Enable durability to ensure messages are durable Use consumer ack to ensure the consumer gets messages from queues Set acknowlwdge-mode to auto or simply use republishMessageRecoverer to bind error message queue and exchange. flow control ： use manual ack mode listener-container prefetch = 1000 to control the flow. (The consumer get 1000 messages every time, need manual confirmation to continue) TTL: time to live: before the message got discarded. x-message-tt queue ttl can be set in the confiogurations.(Delete all messages in the queue) ms message ttl need to implement messagePostProcessor to setExpiration in the message properties(When the expired message is on top of the queue, it will be deleted.) Setting queue ttl and message ttl at the same time, it will use the shorter one. DLX dead letter exchange When a message is dead, it can be sent to another exchange. It is called DLX. Dead letter queue summary: There is no difference between dead letter switches and dead letter queues and Xitong. When the message becomes a dead letter, if the queue is bound to a dead letter switch, the message will be rerouted to the dead letter team by the dead letter switch. Messages become dead letter: exceeded limit of the queue consumer refused(basicNack) and requeue = false queue expired/message expired parameters: x-dead-letter-exchange and x-dead-letter-routing-key Use plugin to create delayqueue. Install: Use: In the listener, declare delayed = true in th exchange setting. In the publisher, set message property: .setHeader(“x-delay”, 5000) The message will be delayed, but the publisher returns will get errors for not able to send the message directly to the exchange. In the message, the ReceivedDelay property will be the x-delay we set. So one possible solution would be using conditional statement to skip the publisher return if (message.getMessageProperties().getReceivedDelay() 0) { return; } Message stuck solutions: more consumers thread pool increase volume of the queue Lazy Queues Note that normal rabbitmq queues are stored in the memory. But the lazy queues directly store the messages in the disk(This will cause some performance issues). When the consumers are consuming messages, the messages are read from the disk into the memory.(This supports millions of messages storing.) To set one queue to be lazy queue, simply set x-queue-mode to “lazy” or @arguments = @Argument(name = “x-queue-mode”, value = “lazy”) Pro: higher upper limit(disk storage) no intermittent page-out more stable Con: longer delay Subject to the disk IO performance ","date":"2022-06-18","objectID":"/rabbitmq/:0:6","series":null,"tags":["RabbitMQ"],"title":"RabbitMQ","uri":"/rabbitmq/#dead-letter-queue-summary"},{"categories":["Tech"],"content":"Reliability publisher confirm: publisher-confirm-type = correlate Enabling publisher confirms calls back a confirm function to identify whether the process succeeds. (Producer to exchange) true: ack false: nack To define callback: rabbitTemplate.setConfirmCallback() confirm as an anonymous class method confirm(CorrelationData, ack, cause) ack: whether the exchange have received the message correlationData: info can be manually set (id) publisher-return: exchange to queue: if failed return “return Callback” publisher-returns: true template: mandatory: true When the message from the exchange did not get to the queue, the default mode is throwing the message away, but we can change the mode of returnCallback notify the publisher of the failure. rabbbitTemplate.returnsCallback() default mode: discard message, not returning the returnedMessage method deals with messages in the exchange but not sent to the queues. Consumer Ack: acknowledge = none As soon as the consumer receives the message, it send the ack message (No service successfulness confirmation) acknowledge = “manual” Manually use channel.basicAck() to confirm and channel.basicNack() to let the publisher resend the message. auto Use republishMessageRecover to bind error message and its routing key. How to guarantee reliability？ Enbale publisher confirms and publisher returns to ensure message is sent to the exchange and queues Enable durability to ensure messages are durable Use consumer ack to ensure the consumer gets messages from queues Set acknowlwdge-mode to auto or simply use republishMessageRecoverer to bind error message queue and exchange. flow control ： use manual ack mode listener-container prefetch = 1000 to control the flow. (The consumer get 1000 messages every time, need manual confirmation to continue) TTL: time to live: before the message got discarded. x-message-tt queue ttl can be set in the confiogurations.(Delete all messages in the queue) ms message ttl need to implement messagePostProcessor to setExpiration in the message properties(When the expired message is on top of the queue, it will be deleted.) Setting queue ttl and message ttl at the same time, it will use the shorter one. DLX dead letter exchange When a message is dead, it can be sent to another exchange. It is called DLX. Dead letter queue summary: There is no difference between dead letter switches and dead letter queues and Xitong. When the message becomes a dead letter, if the queue is bound to a dead letter switch, the message will be rerouted to the dead letter team by the dead letter switch. Messages become dead letter: exceeded limit of the queue consumer refused(basicNack) and requeue = false queue expired/message expired parameters: x-dead-letter-exchange and x-dead-letter-routing-key Use plugin to create delayqueue. Install: Use: In the listener, declare delayed = true in th exchange setting. In the publisher, set message property: .setHeader(“x-delay”, 5000) The message will be delayed, but the publisher returns will get errors for not able to send the message directly to the exchange. In the message, the ReceivedDelay property will be the x-delay we set. So one possible solution would be using conditional statement to skip the publisher return if (message.getMessageProperties().getReceivedDelay() 0) { return; } Message stuck solutions: more consumers thread pool increase volume of the queue Lazy Queues Note that normal rabbitmq queues are stored in the memory. But the lazy queues directly store the messages in the disk(This will cause some performance issues). When the consumers are consuming messages, the messages are read from the disk into the memory.(This supports millions of messages storing.) To set one queue to be lazy queue, simply set x-queue-mode to “lazy” or @arguments = @Argument(name = “x-queue-mode”, value = “lazy”) Pro: higher upper limit(disk storage) no intermittent page-out more stable Con: longer delay Subject to the disk IO performance ","date":"2022-06-18","objectID":"/rabbitmq/:0:6","series":null,"tags":["RabbitMQ"],"title":"RabbitMQ","uri":"/rabbitmq/#use-plugin-to-create-delayqueue"},{"categories":["Tech"],"content":"Reliability publisher confirm: publisher-confirm-type = correlate Enabling publisher confirms calls back a confirm function to identify whether the process succeeds. (Producer to exchange) true: ack false: nack To define callback: rabbitTemplate.setConfirmCallback() confirm as an anonymous class method confirm(CorrelationData, ack, cause) ack: whether the exchange have received the message correlationData: info can be manually set (id) publisher-return: exchange to queue: if failed return “return Callback” publisher-returns: true template: mandatory: true When the message from the exchange did not get to the queue, the default mode is throwing the message away, but we can change the mode of returnCallback notify the publisher of the failure. rabbbitTemplate.returnsCallback() default mode: discard message, not returning the returnedMessage method deals with messages in the exchange but not sent to the queues. Consumer Ack: acknowledge = none As soon as the consumer receives the message, it send the ack message (No service successfulness confirmation) acknowledge = “manual” Manually use channel.basicAck() to confirm and channel.basicNack() to let the publisher resend the message. auto Use republishMessageRecover to bind error message and its routing key. How to guarantee reliability？ Enbale publisher confirms and publisher returns to ensure message is sent to the exchange and queues Enable durability to ensure messages are durable Use consumer ack to ensure the consumer gets messages from queues Set acknowlwdge-mode to auto or simply use republishMessageRecoverer to bind error message queue and exchange. flow control ： use manual ack mode listener-container prefetch = 1000 to control the flow. (The consumer get 1000 messages every time, need manual confirmation to continue) TTL: time to live: before the message got discarded. x-message-tt queue ttl can be set in the confiogurations.(Delete all messages in the queue) ms message ttl need to implement messagePostProcessor to setExpiration in the message properties(When the expired message is on top of the queue, it will be deleted.) Setting queue ttl and message ttl at the same time, it will use the shorter one. DLX dead letter exchange When a message is dead, it can be sent to another exchange. It is called DLX. Dead letter queue summary: There is no difference between dead letter switches and dead letter queues and Xitong. When the message becomes a dead letter, if the queue is bound to a dead letter switch, the message will be rerouted to the dead letter team by the dead letter switch. Messages become dead letter: exceeded limit of the queue consumer refused(basicNack) and requeue = false queue expired/message expired parameters: x-dead-letter-exchange and x-dead-letter-routing-key Use plugin to create delayqueue. Install: Use: In the listener, declare delayed = true in th exchange setting. In the publisher, set message property: .setHeader(“x-delay”, 5000) The message will be delayed, but the publisher returns will get errors for not able to send the message directly to the exchange. In the message, the ReceivedDelay property will be the x-delay we set. So one possible solution would be using conditional statement to skip the publisher return if (message.getMessageProperties().getReceivedDelay() 0) { return; } Message stuck solutions: more consumers thread pool increase volume of the queue Lazy Queues Note that normal rabbitmq queues are stored in the memory. But the lazy queues directly store the messages in the disk(This will cause some performance issues). When the consumers are consuming messages, the messages are read from the disk into the memory.(This supports millions of messages storing.) To set one queue to be lazy queue, simply set x-queue-mode to “lazy” or @arguments = @Argument(name = “x-queue-mode”, value = “lazy”) Pro: higher upper limit(disk storage) no intermittent page-out more stable Con: longer delay Subject to the disk IO performance ","date":"2022-06-18","objectID":"/rabbitmq/:0:6","series":null,"tags":["RabbitMQ"],"title":"RabbitMQ","uri":"/rabbitmq/#install"},{"categories":["Tech"],"content":"Reliability publisher confirm: publisher-confirm-type = correlate Enabling publisher confirms calls back a confirm function to identify whether the process succeeds. (Producer to exchange) true: ack false: nack To define callback: rabbitTemplate.setConfirmCallback() confirm as an anonymous class method confirm(CorrelationData, ack, cause) ack: whether the exchange have received the message correlationData: info can be manually set (id) publisher-return: exchange to queue: if failed return “return Callback” publisher-returns: true template: mandatory: true When the message from the exchange did not get to the queue, the default mode is throwing the message away, but we can change the mode of returnCallback notify the publisher of the failure. rabbbitTemplate.returnsCallback() default mode: discard message, not returning the returnedMessage method deals with messages in the exchange but not sent to the queues. Consumer Ack: acknowledge = none As soon as the consumer receives the message, it send the ack message (No service successfulness confirmation) acknowledge = “manual” Manually use channel.basicAck() to confirm and channel.basicNack() to let the publisher resend the message. auto Use republishMessageRecover to bind error message and its routing key. How to guarantee reliability？ Enbale publisher confirms and publisher returns to ensure message is sent to the exchange and queues Enable durability to ensure messages are durable Use consumer ack to ensure the consumer gets messages from queues Set acknowlwdge-mode to auto or simply use republishMessageRecoverer to bind error message queue and exchange. flow control ： use manual ack mode listener-container prefetch = 1000 to control the flow. (The consumer get 1000 messages every time, need manual confirmation to continue) TTL: time to live: before the message got discarded. x-message-tt queue ttl can be set in the confiogurations.(Delete all messages in the queue) ms message ttl need to implement messagePostProcessor to setExpiration in the message properties(When the expired message is on top of the queue, it will be deleted.) Setting queue ttl and message ttl at the same time, it will use the shorter one. DLX dead letter exchange When a message is dead, it can be sent to another exchange. It is called DLX. Dead letter queue summary: There is no difference between dead letter switches and dead letter queues and Xitong. When the message becomes a dead letter, if the queue is bound to a dead letter switch, the message will be rerouted to the dead letter team by the dead letter switch. Messages become dead letter: exceeded limit of the queue consumer refused(basicNack) and requeue = false queue expired/message expired parameters: x-dead-letter-exchange and x-dead-letter-routing-key Use plugin to create delayqueue. Install: Use: In the listener, declare delayed = true in th exchange setting. In the publisher, set message property: .setHeader(“x-delay”, 5000) The message will be delayed, but the publisher returns will get errors for not able to send the message directly to the exchange. In the message, the ReceivedDelay property will be the x-delay we set. So one possible solution would be using conditional statement to skip the publisher return if (message.getMessageProperties().getReceivedDelay() 0) { return; } Message stuck solutions: more consumers thread pool increase volume of the queue Lazy Queues Note that normal rabbitmq queues are stored in the memory. But the lazy queues directly store the messages in the disk(This will cause some performance issues). When the consumers are consuming messages, the messages are read from the disk into the memory.(This supports millions of messages storing.) To set one queue to be lazy queue, simply set x-queue-mode to “lazy” or @arguments = @Argument(name = “x-queue-mode”, value = “lazy”) Pro: higher upper limit(disk storage) no intermittent page-out more stable Con: longer delay Subject to the disk IO performance ","date":"2022-06-18","objectID":"/rabbitmq/:0:6","series":null,"tags":["RabbitMQ"],"title":"RabbitMQ","uri":"/rabbitmq/#use"},{"categories":["Tech"],"content":"Reliability publisher confirm: publisher-confirm-type = correlate Enabling publisher confirms calls back a confirm function to identify whether the process succeeds. (Producer to exchange) true: ack false: nack To define callback: rabbitTemplate.setConfirmCallback() confirm as an anonymous class method confirm(CorrelationData, ack, cause) ack: whether the exchange have received the message correlationData: info can be manually set (id) publisher-return: exchange to queue: if failed return “return Callback” publisher-returns: true template: mandatory: true When the message from the exchange did not get to the queue, the default mode is throwing the message away, but we can change the mode of returnCallback notify the publisher of the failure. rabbbitTemplate.returnsCallback() default mode: discard message, not returning the returnedMessage method deals with messages in the exchange but not sent to the queues. Consumer Ack: acknowledge = none As soon as the consumer receives the message, it send the ack message (No service successfulness confirmation) acknowledge = “manual” Manually use channel.basicAck() to confirm and channel.basicNack() to let the publisher resend the message. auto Use republishMessageRecover to bind error message and its routing key. How to guarantee reliability？ Enbale publisher confirms and publisher returns to ensure message is sent to the exchange and queues Enable durability to ensure messages are durable Use consumer ack to ensure the consumer gets messages from queues Set acknowlwdge-mode to auto or simply use republishMessageRecoverer to bind error message queue and exchange. flow control ： use manual ack mode listener-container prefetch = 1000 to control the flow. (The consumer get 1000 messages every time, need manual confirmation to continue) TTL: time to live: before the message got discarded. x-message-tt queue ttl can be set in the confiogurations.(Delete all messages in the queue) ms message ttl need to implement messagePostProcessor to setExpiration in the message properties(When the expired message is on top of the queue, it will be deleted.) Setting queue ttl and message ttl at the same time, it will use the shorter one. DLX dead letter exchange When a message is dead, it can be sent to another exchange. It is called DLX. Dead letter queue summary: There is no difference between dead letter switches and dead letter queues and Xitong. When the message becomes a dead letter, if the queue is bound to a dead letter switch, the message will be rerouted to the dead letter team by the dead letter switch. Messages become dead letter: exceeded limit of the queue consumer refused(basicNack) and requeue = false queue expired/message expired parameters: x-dead-letter-exchange and x-dead-letter-routing-key Use plugin to create delayqueue. Install: Use: In the listener, declare delayed = true in th exchange setting. In the publisher, set message property: .setHeader(“x-delay”, 5000) The message will be delayed, but the publisher returns will get errors for not able to send the message directly to the exchange. In the message, the ReceivedDelay property will be the x-delay we set. So one possible solution would be using conditional statement to skip the publisher return if (message.getMessageProperties().getReceivedDelay() 0) { return; } Message stuck solutions: more consumers thread pool increase volume of the queue Lazy Queues Note that normal rabbitmq queues are stored in the memory. But the lazy queues directly store the messages in the disk(This will cause some performance issues). When the consumers are consuming messages, the messages are read from the disk into the memory.(This supports millions of messages storing.) To set one queue to be lazy queue, simply set x-queue-mode to “lazy” or @arguments = @Argument(name = “x-queue-mode”, value = “lazy”) Pro: higher upper limit(disk storage) no intermittent page-out more stable Con: longer delay Subject to the disk IO performance ","date":"2022-06-18","objectID":"/rabbitmq/:0:6","series":null,"tags":["RabbitMQ"],"title":"RabbitMQ","uri":"/rabbitmq/#message-stuck"},{"categories":["Tech"],"content":"Reliability publisher confirm: publisher-confirm-type = correlate Enabling publisher confirms calls back a confirm function to identify whether the process succeeds. (Producer to exchange) true: ack false: nack To define callback: rabbitTemplate.setConfirmCallback() confirm as an anonymous class method confirm(CorrelationData, ack, cause) ack: whether the exchange have received the message correlationData: info can be manually set (id) publisher-return: exchange to queue: if failed return “return Callback” publisher-returns: true template: mandatory: true When the message from the exchange did not get to the queue, the default mode is throwing the message away, but we can change the mode of returnCallback notify the publisher of the failure. rabbbitTemplate.returnsCallback() default mode: discard message, not returning the returnedMessage method deals with messages in the exchange but not sent to the queues. Consumer Ack: acknowledge = none As soon as the consumer receives the message, it send the ack message (No service successfulness confirmation) acknowledge = “manual” Manually use channel.basicAck() to confirm and channel.basicNack() to let the publisher resend the message. auto Use republishMessageRecover to bind error message and its routing key. How to guarantee reliability？ Enbale publisher confirms and publisher returns to ensure message is sent to the exchange and queues Enable durability to ensure messages are durable Use consumer ack to ensure the consumer gets messages from queues Set acknowlwdge-mode to auto or simply use republishMessageRecoverer to bind error message queue and exchange. flow control ： use manual ack mode listener-container prefetch = 1000 to control the flow. (The consumer get 1000 messages every time, need manual confirmation to continue) TTL: time to live: before the message got discarded. x-message-tt queue ttl can be set in the confiogurations.(Delete all messages in the queue) ms message ttl need to implement messagePostProcessor to setExpiration in the message properties(When the expired message is on top of the queue, it will be deleted.) Setting queue ttl and message ttl at the same time, it will use the shorter one. DLX dead letter exchange When a message is dead, it can be sent to another exchange. It is called DLX. Dead letter queue summary: There is no difference between dead letter switches and dead letter queues and Xitong. When the message becomes a dead letter, if the queue is bound to a dead letter switch, the message will be rerouted to the dead letter team by the dead letter switch. Messages become dead letter: exceeded limit of the queue consumer refused(basicNack) and requeue = false queue expired/message expired parameters: x-dead-letter-exchange and x-dead-letter-routing-key Use plugin to create delayqueue. Install: Use: In the listener, declare delayed = true in th exchange setting. In the publisher, set message property: .setHeader(“x-delay”, 5000) The message will be delayed, but the publisher returns will get errors for not able to send the message directly to the exchange. In the message, the ReceivedDelay property will be the x-delay we set. So one possible solution would be using conditional statement to skip the publisher return if (message.getMessageProperties().getReceivedDelay() 0) { return; } Message stuck solutions: more consumers thread pool increase volume of the queue Lazy Queues Note that normal rabbitmq queues are stored in the memory. But the lazy queues directly store the messages in the disk(This will cause some performance issues). When the consumers are consuming messages, the messages are read from the disk into the memory.(This supports millions of messages storing.) To set one queue to be lazy queue, simply set x-queue-mode to “lazy” or @arguments = @Argument(name = “x-queue-mode”, value = “lazy”) Pro: higher upper limit(disk storage) no intermittent page-out more stable Con: longer delay Subject to the disk IO performance ","date":"2022-06-18","objectID":"/rabbitmq/:0:6","series":null,"tags":["RabbitMQ"],"title":"RabbitMQ","uri":"/rabbitmq/#lazy-queues"},{"categories":["Tech"],"content":"MQ cluster classic cluster Nodes in the cluster share some info like exchange queue but not messages in the queues. If one node is offline, the messages are lost. When the consumer are requesting info from one node but the visited node doesn’t have the queue. It will get the queue and its info from the node that has it. Image cluster The image queue structure is one master and multiple slave (slave is an image) all operations are performed on the primary node and then synchronized to the image node. After the master node is down, the image node is replaced by a new master node (if the master node is down before the master-slave synchronization is completed, data loss may occur) the server load balancer function is not available because all operations are completed by the master node (but the master node of different queues can be different, which can be used to improve throughput) higher reliability Quorum Queues Sane as image queues but easier to use.RabbitMQ部署指南.md ","date":"2022-06-18","objectID":"/rabbitmq/:0:7","series":null,"tags":["RabbitMQ"],"title":"RabbitMQ","uri":"/rabbitmq/#mq-cluster"},{"categories":["Tech"],"content":"MQ cluster classic cluster Nodes in the cluster share some info like exchange queue but not messages in the queues. If one node is offline, the messages are lost. When the consumer are requesting info from one node but the visited node doesn’t have the queue. It will get the queue and its info from the node that has it. Image cluster The image queue structure is one master and multiple slave (slave is an image) all operations are performed on the primary node and then synchronized to the image node. After the master node is down, the image node is replaced by a new master node (if the master node is down before the master-slave synchronization is completed, data loss may occur) the server load balancer function is not available because all operations are completed by the master node (but the master node of different queues can be different, which can be used to improve throughput) higher reliability Quorum Queues Sane as image queues but easier to use.RabbitMQ部署指南.md ","date":"2022-06-18","objectID":"/rabbitmq/:0:7","series":null,"tags":["RabbitMQ"],"title":"RabbitMQ","uri":"/rabbitmq/#classic-cluster"},{"categories":["Tech"],"content":"MQ cluster classic cluster Nodes in the cluster share some info like exchange queue but not messages in the queues. If one node is offline, the messages are lost. When the consumer are requesting info from one node but the visited node doesn’t have the queue. It will get the queue and its info from the node that has it. Image cluster The image queue structure is one master and multiple slave (slave is an image) all operations are performed on the primary node and then synchronized to the image node. After the master node is down, the image node is replaced by a new master node (if the master node is down before the master-slave synchronization is completed, data loss may occur) the server load balancer function is not available because all operations are completed by the master node (but the master node of different queues can be different, which can be used to improve throughput) higher reliability Quorum Queues Sane as image queues but easier to use.RabbitMQ部署指南.md ","date":"2022-06-18","objectID":"/rabbitmq/:0:7","series":null,"tags":["RabbitMQ"],"title":"RabbitMQ","uri":"/rabbitmq/#image-cluster"},{"categories":["Tech"],"content":"MQ cluster classic cluster Nodes in the cluster share some info like exchange queue but not messages in the queues. If one node is offline, the messages are lost. When the consumer are requesting info from one node but the visited node doesn’t have the queue. It will get the queue and its info from the node that has it. Image cluster The image queue structure is one master and multiple slave (slave is an image) all operations are performed on the primary node and then synchronized to the image node. After the master node is down, the image node is replaced by a new master node (if the master node is down before the master-slave synchronization is completed, data loss may occur) the server load balancer function is not available because all operations are completed by the master node (but the master node of different queues can be different, which can be used to improve throughput) higher reliability Quorum Queues Sane as image queues but easier to use.RabbitMQ部署指南.md ","date":"2022-06-18","objectID":"/rabbitmq/:0:7","series":null,"tags":["RabbitMQ"],"title":"RabbitMQ","uri":"/rabbitmq/#quorum-queues"},{"categories":["Tech"],"content":"Distributed lock Under highly concurrent environment, the distributed lock is used to ensure consistency in the database. Here’s an example of this. Using simple lock: stringRedisTemplate.opsForValue().setIfAbsent(\"lockKey\", \"ljy\"); // This is the simplest realization of lock in redis // But this lock will trigger deadlock when the service is down // adding delete won't help stringTemplate.delete(\"lockKey\"); // setting expiration time have problem as well stringRedisTemplate.opsForValue().setIfAbsent(\"lockKey\", \"ljy\", 10, TimeUnit.SECONDS); // we don't know when to expire the key (the request may haven't finished but the lock is expired) Let’s say we have a lock expires in 10 seconds, but the requests need 15 seconds to be processed. In that way the second thread will get the lock after 10 seconds and the get deleted after 20 seconds. This may trigger the distributed lock to be ineffect permanently. We can add an if sentence to ","date":"2022-05-18","objectID":"/redis-distributed-lock/:0:0","series":null,"tags":["Redis"],"title":"Redis distributed lock","uri":"/redis-distributed-lock/#distributed-lock"},{"categories":["Tech"],"content":"Redisson Redis Java client which has features of distributed locks. It considers all the problem above and get it done with simple command. Map\u003cString, List\u003cCatalog2Vo\u003e\u003e categoryMap=null; RLock lock = redissonClient.getLock(\"CatalogJson-Lock\"); lock.lock(10,TimeUnit.SECONDS); try { categoryMap = getCategoryMap(); } catch (InterruptedException e) { e.printStackTrace(); }finally { lock.unlock(); return categoryMap; } ","date":"2022-05-18","objectID":"/redis-distributed-lock/:1:0","series":null,"tags":["Redis"],"title":"Redis distributed lock","uri":"/redis-distributed-lock/#redisson"},{"categories":["Leetcode"],"content":"Redis ","date":"2022-05-17","objectID":"/best-time-to-buy-and-sell-stock/:0:0","series":null,"tags":["java"],"title":"Best Time to Buy and Sell Stock","uri":"/best-time-to-buy-and-sell-stock/#redis"},{"categories":["Leetcode"],"content":"Redis with pyrhon Simple example of how to use redis as Cache in python: If we want to store some info in cache(logo, name), we first can convert them into JSON format and assign a key, and store the info in cache. logo = get_logo() #maybe get logo from api json.dumps(logo) #convert to JSON logo_key = f\"{symbol}_logo\" #naming convention When we need to use the info, we can first get it from the cache. redis_client.get(logo_key) If it returns null, that means the key is not in cache. We need to store it first. redis_client.set(logo_key, json.dumps(logo)) The standard procedure would be: logo_key = f\"{symbol}_logo\" logo = redis_client.get(logo_key) if logo is None: logo = get_logo() redis_client.set(logo_key, json.dumps(logo)) else: redis_client.get(logo_key) ","date":"2022-05-17","objectID":"/best-time-to-buy-and-sell-stock/:1:0","series":null,"tags":["java"],"title":"Best Time to Buy and Sell Stock","uri":"/best-time-to-buy-and-sell-stock/#redis-with-pyrhon"},{"categories":["Tech"],"content":"Redis ","date":"2022-05-16","objectID":"/redis/:0:0","series":null,"tags":["Redis"],"title":"Redis","uri":"/redis/#redis"},{"categories":["Tech"],"content":"Redis with python Simple example of how to use redis as Cache in python: If we want to store some info in cache(logo, name), we first can convert them into JSON format and assign a key, and store the info in cache. logo = get_logo() #maybe get logo from api json.dumps(logo) #convert to JSON logo_key = f\"{symbol}_logo\" #naming convention When we need to use the info, we can first get it from the cache. redis_client.get(logo_key) If it returns null, that means the key is not in cache. We need to store it first. redis_client.set(logo_key, json.dumps(logo)) The standard procedure would be: logo_key = f\"{symbol}_logo\" logo = redis_client.get(logo_key) if logo is None: logo = get_logo() redis_client.set(logo_key, json.dumps(logo)) else: redis_client.get(logo_key) ","date":"2022-05-16","objectID":"/redis/:1:0","series":null,"tags":["Redis"],"title":"Redis","uri":"/redis/#redis-with-python"},{"categories":["Leetcode"],"content":"Leetcode 93 https://leetcode-cn.com/problems/restore-ip-addresses/ A valid IP address consists of exactly four integers separated by single dots. Each integer is between 0 and 255 (inclusive) and cannot have leading zeros. For example, “0.1.2.201” and “192.168.1.1” are valid IP addresses, but “0.011.255.245”, “192.168.1.312” and “192.168@1.1” are invalid IP addresses. Given a string s containing only digits, return all possible valid IP addresses that can be formed by inserting dots into s. You are not allowed to reorder or remove any digits in s. You may return the valid IP addresses in any order. Example 1: Input: s = “25525511135” Output: [“255.255.11.135”,“255.255.111.35”] Example 2: Input: s = “0000” Output: [“0.0.0.0”] Example 3: Input: s = “101023” Output: [“1.0.10.23”,“1.0.102.3”,“10.1.0.23”,“10.10.2.3”,“101.0.2.3”] Constraints: 1 \u003c= s.length \u003c= 20 s consists of digits only. class Solution { List\u003cString\u003e result = new ArrayList\u003c\u003e(); public List\u003cString\u003e restoreIpAddresses(String s) { if (s.length() \u003e 12) return result; // 算是剪枝了 backTrack(s, 0, 0); return result; } // startIndex: 搜索的起始位置， pointNum:添加逗点的数量 private void backTrack(String s, int startIndex, int pointNum) { if (pointNum == 3) {// 逗点数量为3时，分隔结束 // 判断第四段⼦字符串是否合法，如果合法就放进result中 if (isValid(s,startIndex,s.length()-1)) { result.add(s); } return; } for (int i = startIndex; i \u003c s.length() \u0026\u0026 i \u003c index + 3 ; i++) { if (isValid(s, startIndex, i)) { s = s.substring(0, i + 1) + \".\" + s.substring(i + 1); //在str的后⾯插⼊⼀个逗点 pointNum++; backTrack(s, i + 2, pointNum);// 插⼊逗点之后下⼀个⼦串的起始位置为i+2 pointNum--;// 回溯 s = s.substring(0, i + 1) + s.substring(i + 2);// 回溯删掉逗点 } else { break; } } } // 判断字符串s在左闭⼜闭区间[start, end]所组成的数字是否合法 private Boolean isValid(String s, int start, int end) { if (start \u003e end) { return false; } if (s.charAt(start) == '0' \u0026\u0026 start != end) { // 0开头的数字不合法 return false; } int num = 0; for (int i = start; i \u003c= end; i++) { if (s.charAt(i) \u003e '9' || s.charAt(i) \u003c '0') { // 遇到⾮数字字符不合法 return false; } num = num * 10 + (s.charAt(i) - '0'); if (num \u003e 255) { // 如果⼤于255了不合法 return false; } } return true; } } //Still room for improvement ","date":"2022-05-07","objectID":"/restore-ip-address/:0:0","series":null,"tags":["java"],"title":"Restore IP address","uri":"/restore-ip-address/#"},{"categories":["Leetcode"],"content":"回溯法，一般可以解决如下几种问题： 组合问题：N个数里面按一定规则找出k个数的集合 切割问题：一个字符串按一定规则有几种切割方式 子集问题：一个N个数的集合里有多少符合条件的子集 排列问题：N个数按一定规则全排列，有几种排列方式 棋盘问题：N皇后，解数独等等 模板： void backtracking(参数) { if (终止条件) { 存放结果; return; } for (选择：本层集合中元素（树中节点孩子的数量就是集合的大小）) { 处理节点; backtracking(路径，选择列表); // 递归 回溯，撤销处理结果 } } //for循环横向遍历 //递归纵向遍历 for循环横向遍历，递归纵向遍历，回溯不断调整结果集 ","date":"2022-05-06","objectID":"/%E5%9B%9E%E6%BA%AF%E6%80%BB%E7%BB%93/:0:0","series":null,"tags":["java"],"title":"backtracking summary","uri":"/%E5%9B%9E%E6%BA%AF%E6%80%BB%E7%BB%93/#"},{"categories":["Leetcode"],"content":"组合 组合中使用 startindex 控制遍历顺序 回溯算法：求组合问题！(opens new window)剪枝精髓是：for循环在寻找起点的时候要有一个范围，如果这个起点到集合终止之间的元素已经不够题目要求的k个元素了，就没有必要搜索了。 关键：去重! 在candidates[i] == candidates[i - 1]相同的情况下： used[i - 1] == true，说明同一树枝candidates[i - 1]使用过 used[i - 1] == false，说明同一树层candidates[i - 1]使用过 利用used[i - 1] == false 效率更高。 ","date":"2022-05-06","objectID":"/%E5%9B%9E%E6%BA%AF%E6%80%BB%E7%BB%93/:0:1","series":null,"tags":["java"],"title":"backtracking summary","uri":"/%E5%9B%9E%E6%BA%AF%E6%80%BB%E7%BB%93/#组合"},{"categories":["Leetcode"],"content":"子集问题 注意：子集问题中数组要排序 递增子序列中不能直接按照默认升序进行解答，需要used数组进行去重 回溯算法：求子集问题（二）(opens new window)也可以使用set针对同一父节点本层去重，但子集问题一定要排序，为什么呢？ 我用没有排序的集合{2,1,2,2}来举个例子画一个图，如下： ","date":"2022-05-06","objectID":"/%E5%9B%9E%E6%BA%AF%E6%80%BB%E7%BB%93/:0:2","series":null,"tags":["java"],"title":"backtracking summary","uri":"/%E5%9B%9E%E6%BA%AF%E6%80%BB%E7%BB%93/#子集问题"},{"categories":["Leetcode"],"content":"排列问题 排列问题的不同： 每层都是从0开始搜索而不是startIndex 需要used数组记录path里都放了哪些元素了 去重时使用used数组，效率较高，优于使用set ","date":"2022-05-06","objectID":"/%E5%9B%9E%E6%BA%AF%E6%80%BB%E7%BB%93/:0:3","series":null,"tags":["java"],"title":"backtracking summary","uri":"/%E5%9B%9E%E6%BA%AF%E6%80%BB%E7%BB%93/#排列问题"},{"categories":["Leetcode"],"content":"棋盘问题 N皇后 ","date":"2022-05-06","objectID":"/%E5%9B%9E%E6%BA%AF%E6%80%BB%E7%BB%93/:0:4","series":null,"tags":["java"],"title":"backtracking summary","uri":"/%E5%9B%9E%E6%BA%AF%E6%80%BB%E7%BB%93/#棋盘问题"},{"categories":["Leetcode"],"content":" 棋盘的宽度就是for循环的长度，递归的深度就是棋盘的高度 数独 二维递归： 返回值为bool，在找到正确答案时直接递归返回true。 ","date":"2022-05-06","objectID":"/%E5%9B%9E%E6%BA%AF%E6%80%BB%E7%BB%93/:1:0","series":null,"tags":["java"],"title":"backtracking summary","uri":"/%E5%9B%9E%E6%BA%AF%E6%80%BB%E7%BB%93/#20201118225433127pnghttpscdnnlarkcomyuque02022png266796611651916827858-63619790-956a-4e52-be7b-fe93eddd10e7pngclientidu2146c5ac-cbaf-4crop0crop0crop1crop1fromdropidu2a0659d4margin5bobject20object5dname20201118225433127pngoriginheight1092originwidth1274originaltypebinaryratio1rotation0showtitlefalsesize205235statusdonestylenonetaskidu2e1ef327-8321-4fd0-8283-49529937822title"},{"categories":["Leetcode"],"content":"性能分析 子集问题分析： 时间复杂度：O(2^n)，因为每一个元素的状态无外乎取与不取，所以时间复杂度为O(2^n) 空间复杂度：O(n)，递归深度为n，所以系统栈所用空间为O(n)，每一层递归所用的空间都是常数级别，注意代码里的result和path都是全局变量，就算是放在参数里，传的也是引用，并不会新申请内存空间，最终空间复杂度为O(n) 排列问题分析： 时间复杂度：O(n!)，这个可以从排列的树形图中很明显发现，每一层节点为n，第二层每一个分支都延伸了n-1个分支，再往下又是n-2个分支，所以一直到叶子节点一共就是 n * n-1 * n-2 * ….. 1 = n!。 空间复杂度：O(n)，和子集问题同理。 组合问题分析： 时间复杂度：O(2^n)，组合问题其实就是一种子集的问题，所以组合问题最坏的情况，也不会超过子集问题的时间复杂度。 空间复杂度：O(n)，和子集问题同理。 N皇后问题分析： 时间复杂度：O(n!) ，其实如果看树形图的话，直觉上是O(n^n)，但皇后之间不能见面所以在搜索的过程中是有剪枝的，最差也就是O（n!），n!表示n * (n-1) * …. * 1。 空间复杂度：O(n)，和子集问题同理。 解数独问题分析： 时间复杂度：O(9^m) , m是’.‘的数目。 空间复杂度：O(n^2)，递归的深度是n^2 ","date":"2022-05-06","objectID":"/%E5%9B%9E%E6%BA%AF%E6%80%BB%E7%BB%93/:1:1","series":null,"tags":["java"],"title":"backtracking summary","uri":"/%E5%9B%9E%E6%BA%AF%E6%80%BB%E7%BB%93/#性能分析"},{"categories":["backTracking"],"content":"Leetcode 491 https://leetcode-cn.com/problems/increasing-subsequences/ Given an integer array nums, return all the different possible increasing subsequences of the given array with at least two elements. You may return the answer in any order. The given array may contain duplicates, and two equal integers should also be considered a special case of increasing sequence. Example 1: Input: nums = [4,6,7,7] Output: [[4,6],[4,6,7],[4,6,7,7],[4,7],[4,7,7],[6,7],[6,7,7],[7,7]] Example 2: Input: nums = [4,4,3,2,1] Output: [[4,4]] Constraints: 1 \u003c= nums.length \u003c= 15 -100 \u003c= nums[i] \u003c= 100 class Solution { List\u003cList\u003cInteger\u003e\u003e result = new ArrayList\u003c\u003e(); LinkedList\u003cInteger\u003e list = new LinkedList\u003c\u003e(); public List\u003cList\u003cInteger\u003e\u003e findSubsequences(int[] nums) { backTracking(nums, 0); return result; } public void backTracking(int nums[], int index){ if(list.size() \u003e 1){ result.add(new ArrayList\u003c\u003e(list)); } HashMap\u003cInteger, Integer\u003e map = new HashMap\u003c\u003e(); //使用map进行去重 for(int i = index; i \u003c nums.length; i++){ if(list.size() \u003e 0 \u0026\u0026 nums[i] \u003c list.getLast()){ continue; } //change from nums[i] i-1 to map if(map.getOrDefault(nums[i], 0) \u003e= 1){ continue; } map.put(nums[i], map.getOrDefault(nums[i], 0) + 1); list.add(nums[i]); backTracking(nums, i + 1); list.removeLast(); } } } ","date":"2022-05-06","objectID":"/increasing-subsequences/:0:0","series":null,"tags":["java"],"title":"Increasing Subsequence","uri":"/increasing-subsequences/#"},{"categories":["backTracking"],"content":"Leetcode 17 https://leetcode-cn.com/problems/letter-combinations-of-a-phone-number/ Given a string containing digits from 2-9 inclusive, return all possible letter combinations that the number could represent. Return the answer in any order. A mapping of digit to letters (just like on the telephone buttons) is given below. Note that 1 does not map to any letters. Example 1: Input: digits = “23” Output: [“ad”,“ae”,“af”,“bd”,“be”,“bf”,“cd”,“ce”,“cf”] Example 2: Input: digits = \"\" Output: [] Example 3: Input: digits = “2” Output: [“a”,“b”,“c”] Constraints: 0 \u003c= digits.length \u003c= 4 digits[i] is a digit in the range [‘2’, ‘9’]. class Solution { List\u003cString\u003e result = new LinkedList\u003c\u003e(); HashMap\u003cInteger, String\u003e map = new HashMap\u003c\u003e(); public List\u003cString\u003e letterCombinations(String digits) { if(digits.length() == 0){ return result; } map.put(2, \"abc\"); map.put(3, \"def\"); map.put(4, \"ghi\"); map.put(5, \"jkl\"); map.put(6, \"mno\"); map.put(7, \"pqrs\"); map.put(8, \"tuv\"); map.put(9, \"wxyz\"); letterCombinationHelper(digits, new StringBuilder(), 0); return result; } public void letterCombinationHelper(String digits, StringBuilder sb, int index){ if(sb.toString().length() == digits.length()){ result.add(sb.toString()); return; } if(index \u003e= digits.length()){ return; } int startIndex = digits.charAt(index) - '0'; // for loop(horizontal:a b c) for(int i = 0; i \u003c map.get(startIndex).length(); i++){ sb.append(map.get(startIndex).charAt(i)); letterCombinationHelper(digits, sb, index + 1); // vertical 2 3 sb.deleteCharAt(sb.length() - 1); } } } ","date":"2022-05-06","objectID":"/letter-combinations-of-a-phone-number/:0:0","series":null,"tags":["backTracking"],"title":"Letter combination of a phone number","uri":"/letter-combinations-of-a-phone-number/#"},{"categories":["Leetcode"],"content":"Leetcode 47 https://leetcode-cn.com/problems/permutations-ii/ Given a collection of numbers, nums, that might contain duplicates, return all possible unique permutations in any order. Example 1: Input: nums = [1,1,2] Output: [[1,1,2], [1,2,1], [2,1,1]] Example 2: Input: nums = [1,2,3] Output: [[1,2,3],[1,3,2],[2,1,3],[2,3,1],[3,1,2],[3,2,1]] Constraints: 1 \u003c= nums.length \u003c= 8 -10 \u003c= nums[i] \u003c= 10 class Solution { List\u003cList\u003cInteger\u003e\u003e result = new ArrayList\u003c\u003e(); LinkedList\u003cInteger\u003e list = new LinkedList\u003c\u003e(); public List\u003cList\u003cInteger\u003e\u003e permuteUnique(int[] nums) { Arrays.sort(nums); int [] used = new int[nums.length]; backTracking(nums, used); return result; } public void backTracking(int nums[], int[] used){ if(list.size() == nums.length){ result.add(new ArrayList\u003c\u003e(list)); return; } for(int i = 0; i \u003c nums.length; i++){ if(i \u003e 0 \u0026\u0026 nums[i] == nums[i - 1] \u0026\u0026 used[i - 1] == 0){ // cut continue; } if(used[i] == 0){ list.add(nums[i]); used[i] = 1; backTracking(nums, used); list.removeLast(); used[i] = 0; } } } } 假设当前到达了元素 b ，那么前一个元素 a 一定是已经处理过了的，它的 used[a]==true 理应成立。但如果 used[a]==false 说明什么？？说明 nums[a] 已经被从当前组合集合中撤销掉了！！ 既然现在 nums[a] 被撤销，而 nums[b] 被选择，也就是说：现在 nums[b] 被顶替在了之前 nums[a] 的位置上。如果 nums[a]==nums[b] ，那么当前组合结果和之前不撤销nums[a]的那个组合有什么区别？是没有区别的，这样就导致两个组合是重复的！这也就是我们需要剪枝去重的情况！ ","date":"2022-05-06","objectID":"/permutations-ii/:0:0","series":null,"tags":["java"],"title":"Permutations II","uri":"/permutations-ii/#"},{"categories":["Leetcode"],"content":"Leetcode 98 https://leetcode-cn.com/problems/validate-binary-search-tree/ Given the root of a binary tree, determine if it is a valid binary search tree (BST). A valid BST is defined as follows: The left subtree of a node contains only nodes with keys less than the node’s key. The right subtree of a node contains only nodes with keys greater than the node’s key. Both the left and right subtrees must also be binary search trees. Example 1: Input: root = [2,1,3] Output: true Example 2: Input: root = [5,1,4,null,null,3,6] Output: false Explanation: The root node’s value is 5 but its right child’s value is 4. Constraints: The number of nodes in the tree is in the range [1, 104]. -231 \u003c= Node.val \u003c= 231 - 1 class Solution { // 递归 TreeNode max; public boolean isValidBST(TreeNode root) { if (root == null) { return true; } // 左 boolean left = isValidBST(root.left); if (!left) { return false; } // 中 if (max != null \u0026\u0026 root.val \u003c= max.val) { return false; } max = root; // 右 boolean right = isValidBST(root.right); return right; } } class Solution { // 迭代 public boolean isValidBST(TreeNode root) { if (root == null) { return true; } Stack\u003cTreeNode\u003e stack = new Stack\u003c\u003e(); TreeNode pre = null; while (root != null || !stack.isEmpty()) { while (root != null) { stack.push(root); root = root.left;// 左 } // 中，处理 TreeNode pop = stack.pop(); if (pre != null \u0026\u0026 pop.val \u003c= pre.val) { return false; } pre = pop; root = pop.right;// 右 } return true; } } ","date":"2022-05-03","objectID":"/validate-binary-search-tree/:0:0","series":null,"tags":["java"],"title":"Validate Binary Search Tree","uri":"/validate-binary-search-tree/#"},{"categories":["Leetcode"],"content":"Leetcode 106 105 https://leetcode-cn.com/problems/construct-binary-tree-from-inorder-and-postorder-traversal/ Example 1: Input: inorder = [9,3,15,20,7], postorder = [9,15,7,20,3] Output: [3,9,20,null,null,15,7] Example 2: Input: inorder = [-1], postorder = [-1] Output: [-1] //Recursion 106 class Solution { public TreeNode buildTree(int[] inorder, int[] postorder) { return Traverse(inorder, 0, inorder.length, postorder, 0, postorder.length); } public TreeNode Traverse(int[] inorder, int inLeft, int inRight, int[] postorder, int postLeft, int postRight){ if(inRight - inLeft \u003c 1){ return null; } if(inRight - inLeft == 1){ return new TreeNode(inorder[inLeft]); } //postorder last node as root int rootVal = postorder[postRight - 1]; TreeNode root = new TreeNode(rootVal); int rootIndex = 0; for(int i = inLeft; i \u003c inRight; i++){ if(inorder[i] == rootVal){ rootIndex = i; break; } } root.left = Traverse(inorder, inLeft, rootIndex, postorder, postLeft, postLeft + rootIndex - inLeft); root.right = Traverse(inorder, rootIndex + 1, inRight, postorder, postLeft + rootIndex - inLeft, postRight - 1); return root; } } //105 class Solution { public TreeNode buildTree(int[] preorder, int[] inorder) { return helper(preorder, 0, preorder.length - 1, inorder, 0, inorder.length - 1); } public TreeNode helper(int[] preorder, int preLeft, int preRight, int[] inorder, int inLeft, int inRight) { // 递归终止条件 if (inLeft \u003e inRight || preLeft \u003e preRight) return null; // val 为前序遍历第一个的值，也即是根节点的值 // idx 为根据根节点的值来找中序遍历的下标 int idx = inLeft, val = preorder[preLeft]; TreeNode root = new TreeNode(val); for (int i = inLeft; i \u003c= inRight; i++) { if (inorder[i] == val) { idx = i; break; } } // 根据 idx 来递归找左右子树 root.left = helper(preorder, preLeft + 1, preLeft + (idx - inLeft), inorder, inLeft, idx - 1); root.right = helper(preorder, preLeft + (idx - inLeft) + 1, preRight, inorder, idx + 1, inRight); return root; } } ","date":"2022-05-02","objectID":"/construct-tree/:0:0","series":null,"tags":["java"],"title":"Construct Tree","uri":"/construct-tree/#"},{"categories":["Leetcode"],"content":"https://leetcode-cn.com/problems/invert-binary-tree/ Given the root of a binary tree, invert the tree, and return its root. Example 1: Input: root = [4,2,7,1,3,6,9] Output: [4,7,2,9,6,3,1] Example 2: Input: root = [2,1,3] Output: [2,3,1] Example 3: Input: root = [] Output: [] Constraints: The number of nodes in the tree is in the range [0, 100]. -100 \u003c= Node.val \u003c= 100 //Iteration /** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode() {} * TreeNode(int val) { this.val = val; } * TreeNode(int val, TreeNode left, TreeNode right) { * this.val = val; * this.left = left; * this.right = right; * } * } */ class Solution { public TreeNode invertTree(TreeNode root) { Deque\u003cTreeNode\u003e stack = new LinkedList\u003c\u003e(); if(root != null){ stack.offerLast(root); } TreeNode node = root; while(!stack.isEmpty() ){ node = stack.pollLast(); TreeNode temp = node.left; node.left = node.right; node.right = temp; if(node.left != null){ stack.offerLast(node.left); } if(node.right != null){ stack.offerLast(node.right); } } return root; } } //Recursion class Solution { public TreeNode invertTree(TreeNode root) { InvertNode(root); return root; } public void InvertNode(TreeNode root){ if(root == null){ return; } TreeNode temp = root.left; root.left = root.right; root.right = temp; InvertNode(root.left); InvertNode(root.right); } } ","date":"2022-05-02","objectID":"/invert-binary-tree/:0:0","series":null,"tags":["java"],"title":"Invert Binary Tree","uri":"/invert-binary-tree/#"},{"categories":null,"content":" Second-hand Trading system A simple but comprehensive trading system. Read more... Distributed Backend cart system Implementation of cart system under distributed environment. Read more... Personal Blog System Discover my personal blogs. Read more... ","date":"2022-05-02","objectID":"/projects/:0:0","series":null,"tags":null,"title":"Projects","uri":"/projects/#"},{"categories":["Leetcode"],"content":"Leetcode 102 https://leetcode-cn.com/problems/binary-tree-level-order-traversal/ Given the root of a binary tree, return the level order traversal of its nodes’ values. (i.e., from left to right, level by level). Example 1: Input: root = [3,9,20,null,null,15,7] Output: [[3],[9,20],[15,7]] Example 2: Input: root = [1] Output: [[1]] Example 3: Input: root = [] Output: [] Constraints: The number of nodes in the tree is in the range [0, 2000]. -1000 \u003c= Node.val \u003c= 1000 //DFS method /** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode() {} * TreeNode(int val) { this.val = val; } * TreeNode(int val, TreeNode left, TreeNode right) { * this.val = val; * this.left = left; * this.right = right; * } * } */ class Solution { List\u003cList\u003cInteger\u003e\u003e result = new LinkedList\u003c\u003e(); public List\u003cList\u003cInteger\u003e\u003e levelOrder(TreeNode root) { int depth = 0; DFS(root,0); return result; } public void DFS(TreeNode node, int depth){ if(node == null){ return; } depth++; if(depth \u003e result.size()){ List\u003cInteger\u003e layer = new LinkedList\u003c\u003e(); result.add(layer); } result.get(depth - 1).add(node.val); DFS(node.left, depth); DFS(node.right, depth); } } // Queue class Solution { public List\u003cList\u003cInteger\u003e\u003e levelOrder(TreeNode root) { Deque\u003cTreeNode\u003e queue = new LinkedList\u003c\u003e(); if(root != null){ queue.add(root); } List\u003cList\u003cInteger\u003e\u003e result = new LinkedList\u003c\u003e(); while(queue.size() != 0){ int size = queue.size(); List\u003cInteger\u003e layers = new LinkedList\u003c\u003e(); for(int i = 0; i \u003c size; i++){ TreeNode node = queue.pollFirst(); layers.add(node.val); if(node.left != null){ queue.offerLast(node.left); } if(node.right != null){ queue.offerLast(node.right); } } result.add(layers); } return result; } } Similar problems: Leetcode 107 class Solution { public List\u003cList\u003cInteger\u003e\u003e levelOrderBottom(TreeNode root) { LinkedList\u003cList\u003cInteger\u003e\u003e result = new LinkedList\u003c\u003e(); Deque\u003cTreeNode\u003e queue = new LinkedList\u003c\u003e(); if(root != null){ queue.offerLast(root); } while(!queue.isEmpty()){ int size = queue.size(); List\u003cInteger\u003e list = new LinkedList\u003c\u003e(); for(int i =0; i \u003c size; i++){ TreeNode node = queue.pollFirst(); if(node.left != null){ queue.offerLast(node.left); } if(node.right != null){ queue.offerLast(node.right); } list.add(node.val); } result.addFirst(list); } return result; } } //DFS class Solution { LinkedList\u003cList\u003cInteger\u003e\u003e result = new LinkedList\u003c\u003e(); public List\u003cList\u003cInteger\u003e\u003e levelOrderBottom(TreeNode root) { level(root,0); Collections.reverse(result); return result; } public void level(TreeNode node, int depth){ if(node == null){ return; } depth++; if(result.size() \u003c depth){ List\u003cInteger\u003e list = new LinkedList\u003c\u003e(); result.add(list); } result.get(depth-1).add(node.val); level(node.left, depth); level(node.right, depth); } } Leetcode 199 //DFS class Solution { List\u003cInteger\u003e result = new LinkedList\u003c\u003e(); public List\u003cInteger\u003e rightSideView(TreeNode root) { level(root,0); return result; } public void level(TreeNode node, int depth){ if(node == null){ return; } depth++; if(result.size() \u003c depth){ result.add(0); } result.set(depth-1, node.val); level(node.left, depth); level(node.right, depth); } } //BFS class Solution { public List\u003cInteger\u003e rightSideView(TreeNode root) { Deque\u003cTreeNode\u003e queue = new LinkedList\u003c\u003e(); List\u003cInteger\u003e list = new LinkedList\u003c\u003e(); if(root != null){ queue.offerLast(root); } while(!queue.isEmpty()){ int size = queue.size(); for(int i =0; i \u003c size; i++){ TreeNode node = queue.pollFirst(); if(node.left != null){ queue.offerLast(node.left); } if(node.right != null){ queue.offerLast(node.right); } if(i == size - 1){ list.add(node.val); } } } return list; } } ","date":"2022-05-01","objectID":"/bfs/:0:0","series":null,"tags":["java"],"title":"BFS","uri":"/bfs/#"},{"categories":["Leetcode"],"content":"leetcode 100 https://leetcode-cn.com/problems/same-tree/ //DFS /** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode() {} * TreeNode(int val) { this.val = val; } * TreeNode(int val, TreeNode left, TreeNode right) { * this.val = val; * this.left = left; * this.right = right; * } * } */ class Solution { public boolean isSameTree(TreeNode p, TreeNode q) { return compare(p, q); } public boolean compare(TreeNode p, TreeNode q){ if(p == null \u0026\u0026 q != null){ return false; } else if(p != null \u0026\u0026 q == null){ return false; } else if(p == null \u0026\u0026 q == null){ return true; } else if(p.val != q.val){ return false; } boolean left = compare(p.left, q.left); boolean right = compare(p.right, q.right); return left \u0026 right; } } ","date":"2022-05-01","objectID":"/compare-tree/:0:0","series":null,"tags":["java"],"title":"Compare Tree","uri":"/compare-tree/#"},{"categories":["SpringBoot"],"content":"SpringBoot ","date":"2021-12-16","objectID":"/springboot/:0:0","series":null,"tags":["java"],"title":"SpringBoot","uri":"/springboot/#springboot"},{"categories":["SpringBoot"],"content":"@Configuration ","date":"2021-12-16","objectID":"/springboot/:1:0","series":null,"tags":["java"],"title":"SpringBoot","uri":"/springboot/#configuration"},{"categories":["SpringBoot"],"content":"组件添加 源码重点： postProcessBeanDefinitionRegistry 每个registryId唯一 调用processConfigBeanDefinitions将registry中Bean进行加载 processConfigBeanDefinitions中 ConfigurationClassUtils判断full或者lite 有@configuration则为完整的配置类 对于代理Bean的方法中 分为Full和Lite Full: @configuration(procyBeanMethods=true) 保证每个@Bean方法被调用返回的组件是单实例的(默认方式) Lite: @configuration(procyBeanMethods=false) 每个@Bean方法调用的组件都是新创建的 Lite模式难以声明Bean之间的依赖 重要： 如果存在不同@Bean之间的组件依赖 必须使用Full模式 其他使用Lite模式启动更加迅速 总结： @Configuration的注解类标识这个类可以使用Spring IoC容器作为bean定义的来源。 @Bean注解告诉Spring，一个带有@Bean的注解方法将返回一个对象，该对象应该被注册为在Spring应用程序上下文中的bean。 @ComponentScan的功能其实就是自动扫描并加载符合条件的组件（比如@Component和@Repository等）或者bean定义，最终将这些bean定义加载到IoC容器中。 @EnableAutoConfiguration会根据类路径中的jar依赖为项目进行自动配置，如：添加了spring-boot-starter-web依赖，会自动添加Tomcat和Spring MVC的依赖，Spring Boot会对Tomcat和Spring MVC进行自动配置。 @ImportResource(classpath:) 若存在较老的组件或使用xml配置的组件，使用此方式引入依赖 ","date":"2021-12-16","objectID":"/springboot/:1:1","series":null,"tags":["java"],"title":"SpringBoot","uri":"/springboot/#组件添加"},{"categories":["SpringBoot"],"content":"组件添加 源码重点： postProcessBeanDefinitionRegistry 每个registryId唯一 调用processConfigBeanDefinitions将registry中Bean进行加载 processConfigBeanDefinitions中 ConfigurationClassUtils判断full或者lite 有@configuration则为完整的配置类 对于代理Bean的方法中 分为Full和Lite Full: @configuration(procyBeanMethods=true) 保证每个@Bean方法被调用返回的组件是单实例的(默认方式) Lite: @configuration(procyBeanMethods=false) 每个@Bean方法调用的组件都是新创建的 Lite模式难以声明Bean之间的依赖 重要： 如果存在不同@Bean之间的组件依赖 必须使用Full模式 其他使用Lite模式启动更加迅速 总结： @Configuration的注解类标识这个类可以使用Spring IoC容器作为bean定义的来源。 @Bean注解告诉Spring，一个带有@Bean的注解方法将返回一个对象，该对象应该被注册为在Spring应用程序上下文中的bean。 @ComponentScan的功能其实就是自动扫描并加载符合条件的组件（比如@Component和@Repository等）或者bean定义，最终将这些bean定义加载到IoC容器中。 @EnableAutoConfiguration会根据类路径中的jar依赖为项目进行自动配置，如：添加了spring-boot-starter-web依赖，会自动添加Tomcat和Spring MVC的依赖，Spring Boot会对Tomcat和Spring MVC进行自动配置。 @ImportResource(classpath:) 若存在较老的组件或使用xml配置的组件，使用此方式引入依赖 ","date":"2021-12-16","objectID":"/springboot/:1:1","series":null,"tags":["java"],"title":"SpringBoot","uri":"/springboot/#重要"},{"categories":["SpringBoot"],"content":"组件添加 源码重点： postProcessBeanDefinitionRegistry 每个registryId唯一 调用processConfigBeanDefinitions将registry中Bean进行加载 processConfigBeanDefinitions中 ConfigurationClassUtils判断full或者lite 有@configuration则为完整的配置类 对于代理Bean的方法中 分为Full和Lite Full: @configuration(procyBeanMethods=true) 保证每个@Bean方法被调用返回的组件是单实例的(默认方式) Lite: @configuration(procyBeanMethods=false) 每个@Bean方法调用的组件都是新创建的 Lite模式难以声明Bean之间的依赖 重要： 如果存在不同@Bean之间的组件依赖 必须使用Full模式 其他使用Lite模式启动更加迅速 总结： @Configuration的注解类标识这个类可以使用Spring IoC容器作为bean定义的来源。 @Bean注解告诉Spring，一个带有@Bean的注解方法将返回一个对象，该对象应该被注册为在Spring应用程序上下文中的bean。 @ComponentScan的功能其实就是自动扫描并加载符合条件的组件（比如@Component和@Repository等）或者bean定义，最终将这些bean定义加载到IoC容器中。 @EnableAutoConfiguration会根据类路径中的jar依赖为项目进行自动配置，如：添加了spring-boot-starter-web依赖，会自动添加Tomcat和Spring MVC的依赖，Spring Boot会对Tomcat和Spring MVC进行自动配置。 @ImportResource(classpath:) 若存在较老的组件或使用xml配置的组件，使用此方式引入依赖 ","date":"2021-12-16","objectID":"/springboot/:1:1","series":null,"tags":["java"],"title":"SpringBoot","uri":"/springboot/#总结"},{"categories":["SpringBoot"],"content":"组件添加 源码重点： postProcessBeanDefinitionRegistry 每个registryId唯一 调用processConfigBeanDefinitions将registry中Bean进行加载 processConfigBeanDefinitions中 ConfigurationClassUtils判断full或者lite 有@configuration则为完整的配置类 对于代理Bean的方法中 分为Full和Lite Full: @configuration(procyBeanMethods=true) 保证每个@Bean方法被调用返回的组件是单实例的(默认方式) Lite: @configuration(procyBeanMethods=false) 每个@Bean方法调用的组件都是新创建的 Lite模式难以声明Bean之间的依赖 重要： 如果存在不同@Bean之间的组件依赖 必须使用Full模式 其他使用Lite模式启动更加迅速 总结： @Configuration的注解类标识这个类可以使用Spring IoC容器作为bean定义的来源。 @Bean注解告诉Spring，一个带有@Bean的注解方法将返回一个对象，该对象应该被注册为在Spring应用程序上下文中的bean。 @ComponentScan的功能其实就是自动扫描并加载符合条件的组件（比如@Component和@Repository等）或者bean定义，最终将这些bean定义加载到IoC容器中。 @EnableAutoConfiguration会根据类路径中的jar依赖为项目进行自动配置，如：添加了spring-boot-starter-web依赖，会自动添加Tomcat和Spring MVC的依赖，Spring Boot会对Tomcat和Spring MVC进行自动配置。 @ImportResource(classpath:) 若存在较老的组件或使用xml配置的组件，使用此方式引入依赖 ","date":"2021-12-16","objectID":"/springboot/:1:1","series":null,"tags":["java"],"title":"SpringBoot","uri":"/springboot/#importresourceclasspath"},{"categories":["SpringBoot"],"content":"配置绑定 当需要将properties文件中的配置（例：数据库）解析至Bean中 使用以下注解 @Component+@ConfigurationProperties(prefix=\"\") 只有在容器中的组件才有需要用的功能 记得利用component注解将其加入容器中 @EnableConfigurationProperties(xx.class) 开启xx的配置绑定功能 并且将xx组件注册到容器中 重点： @EnableAutoConfiguration @AutoConfigurationPackage 利用register 将一个包中的所有组件注册（默认MainApplication所在的包） Import(AutoConfigurationEntry) 每次springboot启动时会导入所有场景的自动配置文件 xxxAutoConfiguration 基础配置文件总共127个 但并不是每次导入的配置都会生效，源码内部配合@Conditional注解进行限制加载（按需加载） ","date":"2021-12-16","objectID":"/springboot/:2:0","series":null,"tags":["java"],"title":"SpringBoot","uri":"/springboot/#配置绑定"},{"categories":["SpringBoot"],"content":"配置绑定 当需要将properties文件中的配置（例：数据库）解析至Bean中 使用以下注解 @Component+@ConfigurationProperties(prefix=\"\") 只有在容器中的组件才有需要用的功能 记得利用component注解将其加入容器中 @EnableConfigurationProperties(xx.class) 开启xx的配置绑定功能 并且将xx组件注册到容器中 重点： @EnableAutoConfiguration @AutoConfigurationPackage 利用register 将一个包中的所有组件注册（默认MainApplication所在的包） Import(AutoConfigurationEntry) 每次springboot启动时会导入所有场景的自动配置文件 xxxAutoConfiguration 基础配置文件总共127个 但并不是每次导入的配置都会生效，源码内部配合@Conditional注解进行限制加载（按需加载） ","date":"2021-12-16","objectID":"/springboot/:2:0","series":null,"tags":["java"],"title":"SpringBoot","uri":"/springboot/#enableautoconfiguration"},{"categories":["SpringBoot"],"content":"配置绑定 当需要将properties文件中的配置（例：数据库）解析至Bean中 使用以下注解 @Component+@ConfigurationProperties(prefix=\"\") 只有在容器中的组件才有需要用的功能 记得利用component注解将其加入容器中 @EnableConfigurationProperties(xx.class) 开启xx的配置绑定功能 并且将xx组件注册到容器中 重点： @EnableAutoConfiguration @AutoConfigurationPackage 利用register 将一个包中的所有组件注册（默认MainApplication所在的包） Import(AutoConfigurationEntry) 每次springboot启动时会导入所有场景的自动配置文件 xxxAutoConfiguration 基础配置文件总共127个 但并不是每次导入的配置都会生效，源码内部配合@Conditional注解进行限制加载（按需加载） ","date":"2021-12-16","objectID":"/springboot/:2:0","series":null,"tags":["java"],"title":"SpringBoot","uri":"/springboot/#autoconfigurationpackage"},{"categories":["SpringBoot"],"content":"配置绑定 当需要将properties文件中的配置（例：数据库）解析至Bean中 使用以下注解 @Component+@ConfigurationProperties(prefix=\"\") 只有在容器中的组件才有需要用的功能 记得利用component注解将其加入容器中 @EnableConfigurationProperties(xx.class) 开启xx的配置绑定功能 并且将xx组件注册到容器中 重点： @EnableAutoConfiguration @AutoConfigurationPackage 利用register 将一个包中的所有组件注册（默认MainApplication所在的包） Import(AutoConfigurationEntry) 每次springboot启动时会导入所有场景的自动配置文件 xxxAutoConfiguration 基础配置文件总共127个 但并不是每次导入的配置都会生效，源码内部配合@Conditional注解进行限制加载（按需加载） ","date":"2021-12-16","objectID":"/springboot/:2:0","series":null,"tags":["java"],"title":"SpringBoot","uri":"/springboot/#importautoconfigurationentry"},{"categories":["SpringBoot"],"content":"默认配置 springboot首先加载所有自动配置类 自动配置类按照条件生效 默认配置类都绑定了文件的值，xxxProperties里面拿 和配置文件进行了绑定 生效的配置类给容器装配置组件 因而实现功能 xxxAuto Configuration –\u003e 组件 –\u003e xxxProperties中拿值 –\u003e applicatio.properties 在默认配置中，使用了大量@conditional 以用户配置优先，用户没有进行配置则使用springboot装配的组件 用户想要定制化进行配置的方法主要有两种：1用户直接自己@Bean替换底层组件 2用户去看这个组件货去的配置文件值并且进行修改 ","date":"2021-12-16","objectID":"/springboot/:3:0","series":null,"tags":["java"],"title":"SpringBoot","uri":"/springboot/#默认配置"},{"categories":["SpringBoot"],"content":"Web开发 静态资源的访问首先找Controller内部能否处理，如果不能则交给静态资源处理器（在静态资源文件夹下寻找） ","date":"2021-12-16","objectID":"/springboot/:4:0","series":null,"tags":["java"],"title":"SpringBoot","uri":"/springboot/#web开发"},{"categories":["Tech"],"content":"Using spring cloud gateway can quickly build an API gateway, but before that, let’s introduce some special concepts involved in using spring cloud gateway framework, so as to deepen the understanding of spring cloud gateway and facilitate the later use. Routing: it is a basic component in spring cloud gateway. It is usually composed of an ID, a target URI, and a series of predicates and filters. Predicate: it is the predicate object of Java 8 function library. The specific type isPredicateIt is used to match data information on HTTP request, such as request header information and request body information. If the assertion for a request is true, then the route it is associated with is successful, and then the request is processed by the route. Filter: a component used to modify the request or response of a route. In spring cloud gateway, the gatewayfilter interface should be implemented, and it should be constructed by a specific implementation class based on gatewayfilterfactory. The filters of spring cloud gateway are executed in an orderly manner, and the execution order is determined by the size of the order value. The smaller the value is, the higher the priority is, and the earlier the execution is. Order number of Gateway filter and default filter are default numbers starting from 1. When numbers of different gateway are the same, the order will be Defaultfilter-\u003egateway filter-\u003eglobal filter ","date":"2021-04-06","objectID":"/spring-gateway/:0:0","series":null,"tags":["Spring Cloud, java"],"title":"Spring Cloud Gateway","uri":"/spring-gateway/#"},{"categories":["Tech"],"content":"Docker and MySQL 开源应用容器引擎： 将软件编译成镜像，做好配置后发布，保证启动快速。 启动快 占用资源少 体积小 docker主机(Host)：安装了Docker程序的机器（Docker直接安装在操作系统之上）； docker客户端(Client)：连接docker主机进行操作； docker仓库(Registry)：用来保存各种打包好的软件镜像； docker镜像(Images)：软件打包好的镜像；放在docker仓库中； docker容器(Container)：镜像启动后的实例称为一个容器；容器是独立运行的一个或一组应用 ","date":"2021-02-06","objectID":"/docker/:0:0","series":null,"tags":["Docker"],"title":"Docker","uri":"/docker/#docker-and-mysql"},{"categories":["Tech"],"content":"命令总结： docker pull 下载 `Docker run =docker create+start 创建容器后立即运行 例：docker run -p 3306:3306 --name mysql01 -e MYSQL_ROOT_PASSWORD=mysql -d mysql` -d：后台运行 -p: 将主机的端口映射到容器的一个端口 主机端口:容器内部的端口 docker ps 查看现有所有容器 docker rm containerID ","date":"2021-02-06","objectID":"/docker/:1:0","series":null,"tags":["Docker"],"title":"Docker","uri":"/docker/#命令总结"},{"categories":["Tech"],"content":"docker connect to MySQL docker exec -it mysql01 bash #进入mysql命令行 mysql01为container名字 mysql -h localhost -u root -p mysql#连接 ","date":"2021-02-06","objectID":"/docker/:2:0","series":null,"tags":["Docker"],"title":"Docker","uri":"/docker/#docker-connect-to-mysql"},{"categories":["Tech"],"content":"MySQL常用命令总结： showdatabses;#显示数据库列表usexxx;#切换至xxx数据库showtables;#显示数据库表describetablename;#显示数据表结构createdatabasesdatabasename;createtabletablename(字段列表);#例：createtablemyTable(idint(5)notnullprimarykeyauto_increment,namechar(20)notnull,sexint(4)notnulldefault'0');dropdatabasedatabasename;droptabletablename;select*fromtablename;select*fromtablenamelimit0,2;#仅查询前几行insertintotablenamevalues();renametabletablenametonewtablename;#表名更改updatetablenamesetstrname=newcoontent;#更新updatemysql.usersetpassword=PASSWORD('xxxxx')whereuser='root';flushprivileges;#修改root密码selectuser();#显示当前用户grantselect,insert...ondatabasename.(*ortablename)newuusername@localhostidentifiedby\"password\";#新增用户并限制其登登录权限和地址deletefromuserwhereuser='username'andhost='localhost';flushprivileges;#删除用户 ","date":"2021-02-06","objectID":"/docker/:0:0","series":null,"tags":["Docker"],"title":"Docker","uri":"/docker/#mysql常用命令总结"},{"categories":null,"content":"Hi this is the first post of mine. From now on, my personal working and studying process will be posted here. ","date":"2021-02-01","objectID":"/first_post/:0:0","series":null,"tags":null,"title":"First_post","uri":"/first_post/#"}]